{"./":{"url":"./","title":"Introduction","keywords":"","body":"git文档计划总结路线:操作系统计算机网络篇数据结构篇JVM篇多线程篇设计模式篇涵盖 计算机/java/大数据 基础知识的仓库 git github 文档 http://zisuu.gitee.io/hy_computerandjava_basic 计划总结路线: 基础: 这条知识链层层相扣,缺一不可 操作系统->计算机网络->java jvm虚拟机-> java 多线程,高并发 ->设计模式 -> spring源码 提升 高并发架构(消息队列,搜索引擎,缓存,数据库高级)->分布式系统->springcloud微服务 大数据 hadoop(大数据基础)->hive(超大型数据库)->flume(日志收集)->spark,flink(流式计算) 这是一条基础知识链,层层相扣,缺一不可 操作系统 1.操作系统基本概述 2.进程管理之进程与线程 3.进程管理之处理机调度 4.进程管理之进程同步 5.内存管理之基本概念 6.内存管理之虚拟内存 7.IO设备管理之基本概念 8.面试题总结一 计算机网络篇 1.计算机网络入门文章(必读) 2.计算机网络通信入门文章(必读) 3.数据链路层 4.网络层(一) 5.网络层(二) 6.传输层 7.应用层(⭐) 8.HTTP相关及发展 9.websocket 10.面试题总结一 数据结构篇 1.图论 2.图论例题 3.散列表 4.排序算法 5.排序算法例题 6.树论 7.树论例题.md 数据结构模板 JVM篇 1.jvm内存结构 2.HotSpot虚拟机对象探秘.md 3.对象存亡与垃圾收集算法.md 4.HotSpot的算法细节与垃圾收集器.md 5.内存分配与回收策略.md 6.类文件结构.md 7.类加载机制.md 8.逃逸分析技术 多线程篇 1.Java内存模型与线程.md 2.并行程序基础.md 3.同步控制工具.md 4.线程池及底层原理 5.synchronized底层原理 6.hanppen-before与指令重排 7.volatile与final底层原理 8.乐观锁CAS与悲观锁 9.JUC之AQS 设计模式篇 "},"操作系统/1.基本概述.html":{"url":"操作系统/1.基本概述.html","title":"1.操作系统基本概述","keywords":"","body":"0 思维导图一 操作系统基本概念1.1 概念1.2 特征1.3 目标和功能二 操作系统的发展和分类三 操作系统的运行环境1.运行机制2.内核定义3.中断和异常4.系统调用四 操作系统体系结构1.大内核和微内核0 思维导图 一 操作系统基本概念 1.1 概念 概念 是系统最基本最核心的软件，属于系统软件 控制和管理整个计算机的硬件和软件资源 合理的组织、调度计算机的工作与资源的分配 为用户和其它软件提供方便的接口和环境 层次结构 1.2 特征 1.并发 关于并发和并行的解释: 并发：两个或多个事件在同一时间间隔内发生，这些事件在宏观上是同时发生的，在微观上是交替发生的， 操作系统的并发性指系统中同时存在着多个运行的程序 并行：两个或多个事件在同一时刻发生 2.共享 指系统中的资源可供内存中多个并发执行的进程共同调用,资源共享分为互斥共享和同时访问两种 互斥共享 计算机中的某个资源在一段时间内只能允许一个进程访问，别的进程没有使用权 临界资源(独占资源)：在一段时间内只允许一个进程访问的资源，计算机中大多数物理设备及某些软件中的栈、变量和表格都属于临界资源，它们被要求互斥共享 举个例子：打印机在某一个时刻只能打印一份资料,打印机就属于临界资源 同时共享 计算机中的某个资源在在一段时间内可以同时允许多个进程访问 同时共享通常要求一个请求分为几个时间片段间隔的完成，即交替进行，“分时共享” 这里的同时指在宏观上是同时的，在微观上是交替进行访问的，只是cpu处理速度很快，我们感觉不到，在宏观上感觉是在同时进 同时共享通常将请求分为几个时间片完成,但效果与连续完成几乎相同 举个例子：比如QQ在发送文件A，微信在发送文件B，宏观上两个进程A和B都在访问磁盘，在我们看来是同时进行的，但是在微观上两个进程A和B是交替进行访问磁盘的，只是时间太短，cpu处理速度太快，我们感觉不到。 3.虚拟 虚拟指将一个物理上的实体变为若干逻辑上的对应物.前者是实体,后者是用户感觉到的虚体 虚拟技术：用于实现虚拟的技术 虚拟处理器（CPU）：通过多道程序设计技术，采用让多道程序并发执行的方法，分时来使用一个CPU，实际物理上只有一个CPU，但是用户感觉到有多个CPU 虚拟存储器：从逻辑上扩充存储器容量，用户感觉到但实际不存在的存储器 虚拟设备：将一台物理设备虚拟为逻辑上的多台设备，使多个用户在同一时间段内访问同一台设备，即同时共享，用户宏观上感觉是同时的，但实际上是微观交替访问同一台设备的 操作系统的虚拟技术科归纳为： 时分复用技术：如处理器的分时共享 空间复用技术：如虚拟存储器 4.异步 在多道程序环境下，允许多个程序并发执行，但由于资源优先，进程的执行不是一贯到底的，而是走走停停，以不可预知的速度向前推进，这就是进程的异步性。 比如进程A请求某一个IO操作后,便会让出CPU的占用权给B,等IO结束后,发现B正在占用CPU,A便开始等待 也即,CPU不会因为IO操作而阻塞,而是可以转而忙别的事情.待IO准备就绪之后,再回来处理 这就是异步.关于异步有很多种 如NIO,BIO,AIO等...可自行查阅 1.3 目标和功能 思维导图: 1.计算机系统资源的管理者 接下去几章将围绕操作系统作为软硬件资源管理者,调度资源和工作展开 1.处理机管理(cpu) 在多道程序环境下，cpu的分配和运行都以进程（或线程）为基本单位，因此对cpu的管理可理解为对进程的管理。进程管理的主要功能包括 进程管理 进程同步 进程通信 死锁处理 处理机调度 2.存储器管理(内存) 存储器管理为了给多道应用程序提供良好的运行环境,方便用户使用及提高内存的利用率. 包括: 内存分配与回收 地址映射 内存保护与共享 内存扩容 3.文件管理(文件系统) 包括: 存储空间管理 文件目录管理 文件读写和保护 4.设备管理(io设备) 设备管理的主要内容是完成用户的I/O请求,方便用户使用各种设备 主要包括: 设备分配和处理 缓冲分配 虚拟设备 2.为用户和其他应用程序提供方便接口 为了让用户方便、快捷、可靠的操作计算机硬件并执行自己的程序，操作系统提供了用户接口 操作系统提供的接口分为两类：命令接口和程序接口 命令接口：用户可以直接使用的，利用这些操作命令来组织和控制作业的执行 程序接口：用户通过程序间接使用的，编程人员可以使用它们来请求操作系统服务 3.用作扩充机器 没有任何软件支持的计算机称为裸机 覆盖了软件的机器称为扩充机器或虚拟机 裸机构成计算机系统的物质基础,而实际呈现在用户面前 的是经过若干层软件改造的计算机.裸机在最里层,操作系统在其外层,提供各种遍历服务.因此可视为扩充机器 二 操作系统的发展和分类 三 操作系统的运行环境 1.运行机制 CPU通常执行两种不同性质的程序:内核程序和用户应用程序 特权指令:指计算机中不允许用户直接使用的指令,如I/O指令和置中断指令. 在具体的实现上,将cpu分为两个状态:核心态和用户态 也即,cpu在核心态才能执行特权指令,运行内核程序 分层架构 现代操作系统通常是层次式的结构,一些与硬件关联较为紧密的模块,如时钟管理,中断处理,及执行原语等,处于最底层,其次是调用较为频繁的程序,如进程管理,设备管理,存储器管理等.这两部分构成了操作系统的内核.这部分内容运行在核心态. 2.内核定义 大多数的操作系统内核可定义为四个方面的内容: 1.时钟管理 向用户提供标准的时间 通过时钟中断管理,实现进程的切换机制.如分时操作系统(采用时间片轮换调度机制),实时操作系统(按截止时间控制运行)等 2.中断管理 进程调度 系统调用 设备驱动 文件访问 可以说,现代操作系统是靠中断驱动的软件 3.原语 最接近操作系统底层 操作具有原子性,一气呵成 运行时间短,操作频繁 最直接的方法就是关中断,在原语运行结束后,再开中断 4.系统控制的数据结构及处理 系统中用来登记状态信息的数据结构很多,如进程控制块(PCB),作业控制块,消息队列,缓冲区等.为了实现有效的管理 ,系统需要一些基本的操作.常见的有: 进程管理 存储器管理 设备管理 5.总结 从上可看出,核心态指令实际上包括系统调用指令和一些对时钟,中断及原语的指令 3.中断和异常 操作系统内核运行在核心态,用户程序运行在用户态.用户程序不允许直接实现核心态的功能,但又必须使用这些功能.因此需要在核心态建立一些门,使用户态程序能通过这些们进入核心态.而中断和异常就是唯二的两种途径 中断的分类 中断的执行过程 4.系统调用 作用 执行过程 四 操作系统体系结构 1.大内核和微内核 操作系统在核心态为应用程序提供公共的服务，那么操作系统在核心态应该提供什么服务、怎样提供服务？有关这个问题的回答形成了两种主要的体系结构：大内核和微内核。 大内核系统将操作系统的主要功能模块都作为一个紧密联系的整体运行在核心态，从而为应用提供高性能的系统服务。因为各管理模块之间共享信息，能有效利用相互之间的有效特性，所以具有无可比拟的性能优势。 但随着体系结构和应用需求的不断发展，需要操作系统提供的服务越来越多，而且接口形式越来越复杂，操作系统的设计规模也急剧增长，操作系统也面临着“软件危机”困境。为此，操作系统设计人员试图按照复杂性、时间常数、抽象级别等因素，将操作系统内核分成基本进程管理、虚存、I/O与设备管理、IPC、文件系统等几个层次，继而定义层次之间的服务结构，提高操作系统内核设计上的模块化。但是由于层次之间的交互关系错综复杂，定义清晰的层次间接口非常困难，复杂的交互关系也使得层次之间的界限极其模糊。 为解决操作系统的内核代码难以维护的问题，于是提出了微内核的体系结构。它将内核中最基本的功能（如进程管理等）保留在内核，而将那些不需要在核心态执行的功能移到用户态执行，从而降低了内核的设计复杂性。而那些移出内核的操作系统代码根据分层的原则被划分成若干服务程序，它们的执行相互独立，交互则都借助于微内核进行通信。 微内核结构有效地分离了内核与服务、服务与服务，使得它们之间的接口更加清晰，维护的代价大大降低，各部分可以独立地优化和演进，从而保证了操作系统的可靠性。 微内核结构的最大问题是性能问题，因为需要频繁地在核心态和用户态之间进行切换，操作系统的执行开销偏大。因此有的操作系统将那些频繁使用的系统服务又移回内核，从而保证系统性能。但是有相当多的实验数据表明，体系结构不是引起性能下降的主要因素，体系结构带来的性能提升足以弥补切换开销带来的缺陷。为减少切换开销，也有人提出将系统服务作为运行库链接到用户程序的一种解决方案，这样的体系结构称为库操作系统。 "},"操作系统/2.进程管理之进程与线程.html":{"url":"操作系统/2.进程管理之进程与线程.html","title":"2.进程管理之进程与线程","keywords":"","body":"0 思维导图一 进程与线程1.1 进程的定义1.2 进程的组成与组织1.3 进程的特征1.4 进程的状态与转化1.5 原语实现进程控制1.6 进程通信1.7 线程与多线程模型0 思维导图 一 进程与线程 1.1 进程的定义 为了方便操作系统管理,完成各程序并发执行,引入了进程等相关概念 程序段,数据段,PCB三部分构成了进程实体,一般情况下,我们把进程实体称为进程 PCB是进程存在的唯一标识.因此,所谓创建进程就是创建进程中的PCB,撤销进程就是 撤销进程中的PCB 引入进程实体的概念后,进程可定义未: 进程是进程实体的运行过程,是系统进行资源分配和调度的基本单位 1.2 进程的组成与组织 (一) 组成 进程是由程序段,数据段和PCB三部分组成 而其中最重要的就是进程控制块PCB（Process Control Block） PCB简介： PCB中记录了操作系统所需的，用于描述进程的当前情况以及控制进程运行的全部信息。 PCB的作用是使一个在多道程序环境下不能独立运行的程序（含数据），成为一个能独立运行的基本单位，一个能与其他进程并发执行的进程。 或者说，`OS是根据PCB来对并发执行的进程进行控制和管理的。` 例如，当OS要`调度`某进程执行时，要从该进程的PCB中查处其现行状态及优先级；在调度到某进程后，要根据其PCB中所保存的处理机状态信息，设置该进程恢复运行的现场，并根据其PCB中的程序和数据的内存始址，找到其程序和数据； 进程在`执行`过程中，当需要和与之合作的进程实现同步，通信或者访问文件时，也都需要访问PCB； 当进程由于某种原因而`暂停`执行时，又须将断点的处理机环境保存在PCB中。 可见，在进程的整个生命期中，系统总是通过PCB对进程进行控制的，即系统是根据进程的PCB而不是任何别的什么而感知到该进程的存在的。 所以说，PCB是进程存在的唯一标志。 PCB通常包含的内容： (二) 组织 （1）链接方式 （2）索引方式 1.3 进程的特征 1.4 进程的状态与转化 (一) 进程状态 运行态:已占有CPU处理机,并正在运行 就绪态: 以具备运行的条件(即获取了除了CPU外的一切资源),但因为还未获取CPU而在就绪队列上等待 阻塞态:因为等待某一件事件而让出了CPU,暂停运行.如等待操作系统分配打印机,等待磁盘读写操作等 创建态:进程正在被创建.进程创建需要许多步骤: 首先创建一个PCB 向PCB中填写一些控制和管理进程的信息 系统为进程分配运行时所必须的资源 结束态:进程运行结束.操作系统回收相关资源 (二) 状态转化 1.5 原语实现进程控制 (一) 进程控制及原语 进程控制的主要功能是对系统中的所有进程施行有效的管理,主要具有: 创建进程 删除进程 切换进程状态等 在操作系统中,将进程控制所用到的程序段成为原语,原语的特点就是执行期间不中断,只能一气呵成 原语采用关中断和开中断的指令实现: (二) 五种进程控制原语 进程创建原语 进程终止 进程唤醒和阻塞 这是一对成双出现的原语,阻塞就必须伴随着唤醒 阻塞原语 是进程进行了系统调用 唤醒原语是 系统调用等相关操作结束后唤醒被阻塞的线程 进程切换 (三) 进程控制总结 进程控制,只是变化PCB的位置,程序本身还是在内存中的这也体现了,os是根据PCB来对进程进行管理和调度的 进程控制相同点: 1.6 进程通信 首先,进程是系统资源分配的基本单位,每个进程都有自己独立的内存空间,与其他进程相互隔离 为了保护进程安全,进程不能直接访问另一个进程的内存空间 但是,各个进程间数据的交互又是必须的 因此,操作系统提供了三种让进程通信的方式: (一) 共享存储 共享一块到家都能访问的空间,但为互斥共享,进每次只能有一个线程进行读写操作 (二) 消息传递 在消息传递系统中,进程的数据交换是以格式化的数据为单位的. 直接通信方式:A进程直接发送消息到B进程的消息缓冲队列上 间接通信方式:A进程将消息发送到某个中间件上(如消息队列),B再从该中间件上取消息 (三)管道通信 管道通信相当于开辟了一个内存缓冲区,以实现读写进程的连接和通信,类似于java nio 1.7 线程与多线程模型 (一) 线程的概念 为什么要引入线程的概念呢? 因为有的进程可能需要同时做很多事情(很多程序),如QQ(一个大进程),需要同时处理视频,聊天,传输文件等 而传统的进程只能串行的执行一系列的程序,所以为了提高系统的并发度,引入了线程的概念 (二) 引入线程的变化与好处 资源分配和调度: 传统进程是资源分配和调度的基本单位 引入线程后,进程只作为资源分配的独立单位,而线程才是CPU调度的基本单位 并发性 原先只能实现进程的并发 引入线程后,实现了进程间的线程的并发执行,提供了并发成都 系统开销 传统的进程间并发,需要切换进程的运行环境,系统开销大,浪费了大量时间用户进程切换 引入线程后,同一进程内的线程可并发执行,而不需要切换进程环,大大减小系统开销 (三) 线程类型 用户级线程 有关线程的管理由用户完成,内核感知不到线程的存在. 内核级线程 线程的管理由内核完成 (四) 多线程模型 用户级和内核级线程的交叉组合,会产生不一样的模型 多对一模型 一对一模型 也即每个用户级线程对应一个内核级线程, 优点:并发度高,一个线程的阻塞不会导致别的线程阻塞 缺点:每个线程创建一个内核线程与之对应,开销过大 多对多模型 多对多模型是对以上两种模型的这种,有n各用户级线程和m各内核级线程,要求(m 即解决了多对一模型并发度不高的缺点,又解决了一对一模型开销大的缺点: "},"操作系统/3.进程管理之处理机调度.html":{"url":"操作系统/3.进程管理之处理机调度.html","title":"3.进程管理之处理机调度","keywords":"","body":"0 思维导图一 处理机调度的概念0 思维导图 一 处理机调度的概念 (一) 定义 当有一堆任务需要处理,但由于系统的资源有限,因此进程争夺处理机的情况在所难免 如上述,处理机调度是指按照一定的算法,从就绪队列中选择一个进程并将处理机分配给他使用,以实现进程 的并发执行过程 类比排队: (二) 层次 1.作业调度 又称高级调度,是指从外存队列中选择一个或多个作业,为他们分配内存和输入输出的必要资源,为其建立进程(也即建立PCB),并调度到内存中,等到获取CPU; 也即,作业调度是外存与内存之间的调度 2.中级调度 又称内存调度,主要作用是为了提高内存的利用效率 因为内存资源有限,不可能放置所有的作业.因此可先将那些还无法运行的作业(可能现有的资源还不够满足其运行条件),调至外村中进行等待,将其进程状态设置为挂起态.待内存资源空闲时,再右中级调度决定将哪些满足条件的进程,调回至内存中. 3.进程调度 又称低级调度.主要任务是按照某种算法和策略,从就绪队列中选择一个进程,并将空闲的CPU分配给其使用 4.三级调度的联系 (三)调度时机 1.何时能调度 当前进程主动放弃处理机 进程终止 进程异常 进行系统调用 当前进程被动放弃处理机 时间片用完 有更高级的进程进入队列中 2.何时不能调度 在处理中断的过程中,一般中断过程复杂,且与硬件紧密联系,所以很难在中断过程中进行进程切换 进程处于操作系统内核程序临界区中 操作系统内核程序临界区: 一般用来访问某种内核数据结构的,如进程的就绪队列 在进行原子操作的过程 3.调度的方式 非剥夺式,即只允许进程主动的放弃CPU 剥夺式,也即当有更高级的进程进入队列时,会剥夺CPU当前的使用权给该进程使用,类似于VIP客户插队.. 4.调度与进程切换的区别 侠义进程调度:只从就绪队列中按照一个算法跳出一个进程,分配CPU 进程切换:一个进程让出处理机,由另一个进程使用 广义进程调度:包括选择一个进程和进程切换两个步骤 (四) 调度准则 1.CPU利用率 利用率=(CPU的运行时间)/总时间 2.系统吞吐量 吞吐量=单位时间内CPU完成作业的数目 3.周转时间 周转时间是指作业从提交到结束所用的时间 周转时间=等待+就绪队列排队+CPU处理+输入输出时间的总和 4.等待时间 等待时间=作业等待的时间之和(不包括服务的时间) 也即,作业在外存队列和就绪队列上的等待时间,但不包括IO时 间(因为IO相当于已经被服务了) 5.响应时间 指用户从提交作业到首次响应所用的时间. (五) ⭐调度算法 1.先到先来 --FCFS 顾名思义,就是哪个进程先到达就绪队列,就将空闲的CPU分配给该进程 优点:算法简单 缺点:效率低,对于短作业不公平 2.短作业优先 -- SJF 短作业优先原则,即从就绪队列中选一个评估运行时间最短的进程,分配CPU 优点:对短作业有利 缺点: 对长作业不利 由于进程的评估运行时间是用户提供的,而用户可能不能准确提供运行时间 因此,或许无法做到真正的短作业优先调度 无法让更急迫的作业率先运行 3.高相应比优先 -- HRRN 高相应比优先算法主要用于作业调度,是对FCFS调度算法和SJF调度算法的一种综合平衡,同时考了了每个作业的等待时间和估计的运行时间.在每次进行作业调度时,先计算就绪队列中每个进程的响应比,选择最高的投入使用 相应比=(等待时长+服务时长)/服务时长 基于上述公式: 当服务时长相同时,等待时长越长,相应比越高,也即(FCFS)算法的思想 当等待时长相同时,服务时长越短,相应比越高,也即(SJF)算法的思想 4.时间片轮换算法 --RR (Round-robin) 也即分时操作系统所用的算法,每次给一个进程一个固定的时间片,若该时间内未执行完, 则剥夺该进程的CPU使用权,将该进程放置到就绪队列尾部,将CPU分配给其他进程 对于RR算法,最重要的就是确定时间片的大小 若时间片太大,则相当于退化到了FCFS算法 若太小,则频繁的切换进程,导致系统开销过大 5.优先级算法 类似于优先队列的思想,每次从对立中选择优先级最高的进程分配CPU 通常,优先级的设计如下: 系统进程>用户进程 前台进程>后台进程 IO型进程(频繁使用io设备)>计算型(更多的利用CPU,较少使用IO) 6.多级反馈队列算法 首先关于上述的各个算法的优点: 那么,能否找到一个算法,能同时具备上述的优点呢? 也即:多级反馈队列算法: 特点如下: 设置多个队列,每个队列的优先级从高到低 每个队列的时间片不同,从低到高 新进程进入内存后,先放入第一级队列,按FCFS算法等待调度,若在第一级队列的时间片内未完成,则剥夺其CPU使用权,放入第二级队列尾部,再按FCFS算法调度 当且仅当前N-1级队列都无任务后,才能调度第N级的队列 优势如下: 短作业优先 即使是长作业,也能通过前几个队列得到部分的运行,而不会长期得不到处理 "},"操作系统/4.进程管理之进程同步.html":{"url":"操作系统/4.进程管理之进程同步.html","title":"4.进程管理之进程同步","keywords":"","body":"0 思维导图1.基本概念(一) 临界资源(二) 互斥(三) 同步机制原则2.临界区互斥的软件层面(一) 单标志法(二) 双标志先检查法(三) 双标志后检查法(四)Peterson算法3.临界区互斥的硬件层面(一) 中断屏蔽(二) TestAndSet方法4.信号量机制(一) 定义(二) 整型信号量(三) 记录型信号量(四)利用信号量实现同步(五) 利用信号量实现互斥0 思维导图 1.基本概念 在多道程序环境下，进程是并发执行的，不同进程之间存在着不同的相互制约关系。为了协调进程之间的相互制约关系,如等待、传递信息等，引入了进程同步的概念。进程同步是为了解决进程的异步问题。 一个简单的例子来理解这个概念。 例如，让系统计算1 + 2x3，假设系统产生两个进程: 一个是加法进程，一个是乘法进程。要让计算结果是正确的，一定要让加法进程发生在乘法进程之后,但实际上操作系统具有异步性,若不加以制约，加法进程发生在乘法进程之前是绝对有可能的，因此要制定一定的机制去约束加法进程，让它在乘法进程完成之后才发生。 (一) 临界资源 系统中的许多资源虽然可以被进程共享使用,但是也存在一些一次只能被一个进程使用的资源,如打印机 将系统中一次只能被一个进程使用的资源称为临界资源 而在程序中对临界资源进行访问的那段代码,成为临界区 (二) 互斥 互斥，亦称间接制约关系。进程互斥指当一个进程访问某临界资源时，另一个想要访问该临界资源的进程必须等待。当前访问临界资源的进程访问结束，释放该资源之后，另一个进程才能去访问临界资源。 (三) 同步机制原则 为了禁止两个进程同时出现在临界区,应满足下列准则: 空闲让进 : 当临界区空闲时,可以让请求的进程直接进入使用 忙则等待 : 当临界区已被占用,则请求的进程必须等待 有限等待 : 对请求访问临界区的进程,应保证在有限时间内进入临界区 让权等待 : 当进程不能进入临界区时,应该让出处理器,防止阻塞发送 2.临界区互斥的软件层面 (一) 单标志法 思想:两个进程在访问完临界区资源后,会把临界区的权限交给另一个进程 漏洞:若现在turn=0,也即p0可以进入临界区,但是p0因为特殊原因迟迟不进入,导致p1也无法进入, 这就违背了'空闲让进原则' (二) 双标志先检查法 设置一个布尔型数组flag[],flag[i]为true就代表进程i想要进入临界区.每个进程在进入临界区之前,先检查有没有别的进程要进入,没有则将flag[i]设置为true: 漏洞:若p0,p1同时进入,同时将flag设置为true,就违背了'忙则等待的原则' 原因:检测和上锁不是一气呵成的,导致在上锁前,可能有进程切换,导致两个进程一起进入了临界区 (三) 双标志后检查法 设置一个布尔型数组flag[],flag[i]为true就代表进程i想要进入临界区.每个进程在进入临界区之前,先将flag[i]设置为true,再检测有没有别的进程想进入 ​ 漏洞:显而易见,若同时设为true,就会互相谦让,导致谁也进不了临界区 (四)Peterson算法 该算法可谓对双标志后检查法的改进,思想:若两个进程争夺进入,进程p0发现p1也想进入,就谦让给p1 ​ 也即'孔融让梨'的思想 3.临界区互斥的硬件层面 (一) 中断屏蔽 因为只有在发生中断的时候,cpu才会进行进程切换 因此进程互斥最简单的方法就是屏蔽中断,进而保证互斥的实现 (二) TestAndSet方法 执行TSL指令时，它的内部运转逻辑： 假设lock现在为false，代表临界资源A空闲，那么我就可以访问这个资源，同时将lock=true，提醒别的进程，这个临界资源A我正在使用，让他们等等 假设lock为true，代表临界资源正在有人使用，所以我必须等待，并且将lock=true，并不影响什么，所以没关系，只是为了让lock为false时可以上锁，将上锁与检查在一个TSL指令完成。 4.信号量机制 (一) 定义 为什么会出现信号量机制呢?回顾之前的软硬件互斥方法,软件方面上锁和加锁操作无法一气和成,硬件操作不法实现'让全等待'.信号量机制就是为了更高效的实现进程的同步以及互斥 所谓信号量,顾名思义就是用一个变量来代表系统资源的数量,如打印机,则信号量就为1(代表每次只有一个进程能使用打印机). 信号量利用一对原语wait()和singal()实现,通常简称为p,v操作 进程互斥软件层面上锁和加锁操作无法一气和成,而如果能把这些操作都利用原语实现,而原语又是由关中断/开中断实现的,因此,就能实现进程互斥. (二) 整型信号量 用一个整数型的变量作为信号量,用来表示系统中的某个临界资源 int s=1; void wait(int s){ while(s(三) 记录型信号量 记录型信号量是一个不会出现忙等现象的进程同步机制,除了一个整形外,还要有一个等待队列来保存相关进程 struct{ int value; struct process *L; //等待队列 } wait(semphare s){ s.value--; if(s.value(四)利用信号量实现同步 设进程P2的语句y需要进程P1的语句X的执行结果作为前驱条件: int semphare =0 p1{ .... X; v(semphare) } p2{ ---- p(semphare) Y } (五) 利用信号量实现互斥 设临界资源每次只匀速一个进程访问: int semphare =1 p1{ .... p(semphare) X; V(semphare) } p2{ .... p(semphare) Y; V(semphare) } "},"操作系统/5.内存管理之基本概念.html":{"url":"操作系统/5.内存管理之基本概念.html","title":"5.内存管理之基本概念","keywords":"","body":"0 思维导图一 基本原理和要求二 内存管理的功能(一) 内存保护(二 )内存扩充(三) 内存的分配与回收3. 连续内存分配(一)单一连续分配(二) 固定分区分配(三) 动态分区分配(可变分配)(四) 四种动态分区分配算法四 非连续内存分配(一) 分页管理(二) 分段管理(三).段页式存储管理方式0 思维导图 一 基本原理和要求 (一) 内存管理的概念 操作系统对内存的划分,动态分配和保护,就是内存管理的基本概念,其主要功能有: 内存的分配与回收 地址转化 内存扩充 内存保护 (二) 程序运行的基本原理 逻辑地址和物理地址 从写程序到运行程序 在进行具体的内存管理之前,还需了解程序运行的基本原理 编译:编译程序将用户源代码变成成若干目标模块 链接:链接程序将一组目标模块及所需的库函数链接在一起,形成一个完整的装入模块 装入:由装入程序将装入模块装入内存运行 链接的方式 静态链接: 运行之前完成目标模块和库函数的链接,之后不再分开 装入时动态链接: 在装入内存时,再将目标模块边装入变链接 运行时动态链接: 在程序运行中,需要某个目标模块时,在进行目标模块和库函数的链接.有利于目标模块的共享 装入的方式 绝对装入 在编译时,若指导程序将驻留在内存中的某个位置,则编译程序将产生绝对地址的目标代码.装入程序按照装入模块的地址,将代码放入内存中,且无需修改地址 缺点:只适合单道程序环境. 可重定位装入 在多道环境下,多个模块模块的其实地址通常从0开始,程序中的其他地址都是相对于起始地址的. 此时应采用可重定位装入,寻找内存中的合适起始位置,将装入模块的逻辑地址+该起始位置,修改 成新的物理地址,该修改过程称为重定位 特点: 在一个作业要进入内存时,必须为其分配所有的空间,且分配后,不能再移动 动态运行时装入 装入时不会立即修改逻辑地址为物理地址,而是等到程序运行时再进行修改 此方法需要借助一个重定位寄存器的支持,来存储内存中该程序的起始物理地址 特点:允许程序在内存中进行移动 二 内存管理的功能 (一) 内存保护 多道环境下用户的进程不能受其他进程的影响,也即其他进程不能直接访问当前进程所占有的内存 因此,需要采取一定策略防止 内存越界的现象发生,以保证进程之间互补干扰 方法一 方法二 (二 )内存扩充 覆盖和交换技术是多道程序环境下内存扩充的两种基本方式 覆盖技术 思想: 由于程序 运行时并不会访问程序及数据的每个部分,因此可将程序划分成多个部分,将用户空间也划分成一个 固定去和若干个覆盖区.将活跃部分的程序放在固定区, 其余部分,若即将访问,就调入覆盖区,其他的放在外村中. 特点: 不必将程序一次性调入内存 交换技术 思想: 调出:将处于等待状态的进程移出到辅存中,把内存空间腾出来 调入:将准备好禁止CPU运行的程序从辅存移到内存中 CPU调度的中级调度算法采用的就是交换技术 总结 交换技术主要在不同进程之间进行,而覆盖则用于同一个进程中 显然,交换技术更适合多道程序环境 (三) 内存的分配与回收 内存的分配分为: 连续内存分配 非连续内存分配 下面我们来总结这两大类的分配方式 3. 连续内存分配 连续分配是指为用户进程分配一个连续的空间 (一)单一连续分配 将内存划分为系统区和用户区,每次只装入一道程序 (二) 固定分区分配 为了能在内存中装入多道程序,出现了固定分区分配,即将用户空间划分成大小固定的若干内存区,每块 内存区装入一道程序 划分方式为: 分区大小相等 分区大小不相等 分区大小相等: 缺乏灵活性,但适合用于控制多个相同对象的场景 分区大小不相等:增加了灵活性,但是可能产生内存碎片(也即一个程序可能只有3m,但是却占据了5M的内存区) 分区表:操作系统通过建立一张分区表来说明分区, 需要内存分配时,通过申请的内存大小检索该表,获取未分配的区域 (三) 动态分区分配(可变分配) 这种方式不会事先划分内存区域,而是在程序装入时动态划分.并使分区的大小刚好适应 程序的大小,因此,这种方式下分区的大小和数目是可变的 (四) 四种动态分区分配算法 首次适应算法（First Fit）： 算法思想：将空闲分区链以地址递增的顺序连接；在进行内存分配时，从链首开始顺序查找，直到找到一块分区的大小可以满足需求 时，按照该作业的大小，从该分区中分配出内存，将剩下的空闲分区仍然链在空闲分区链中。 ​ ​ ​ 优点：高址部分的大的空闲分区得到保留，为大作业的内存分配创造了条件； ​ 缺点：（1）每次都是优先利用低址部分的空闲分区，造成低址部分产生大量的外碎片。 ​ （2）每次都是从低址部分查找，使得查找空闲分区的开销增大； 循环首次适应算法（Next Fit） 算法思想：分配内存时不是从链首进行查找可以分配 内存的空闲分区，而是从上一次分配内存的空闲分区的下一个分区开始查找，直到 找到可以为该进程分配内存的空闲分区。 ​ ​ 优点：（1）使得空闲分区分布更加均匀； ​ （2）空闲分区的查找开销小； 缺点：高址部分的大空闲分区被分小，使得大作业进入无法分配内存； 最佳适应算法（Best Fit） 算法思想：将空闲分区链中的空闲分区按照空闲分区由小到大的顺序排序，从而形成空闲分区链。每次从链首进行查找合适的空闲分区 为作业分配内存，这样每次找到的空闲分区是和作业大小最接近的，所谓“最佳”. ​ ​ ​ 优点：第一次找到的空闲分区是大小最接近待分配内存作业大小的； ​ 缺点:产生大量难以利用的外部碎片。 最坏适应算法（Worst Fit） 算法思想：与最佳适应算法刚好相反，将空闲分区链的分区按照从大到小的顺序排序形成空闲分区链，每次查找时只要看第一个空闲分区是否满足即可。 ​ ​ 优点：效率高，分区查找方便； 缺点：当小作业把大空闲分区分小了，那么，大作业就找不到合适的空闲分区。 四 非连续内存分配 非连续指可将作业分散到内存中的多个位置 (一) 分页管理 连续分配内存方式会形成许多“碎片”，通过紧凑的方式将碎片拼接成一块大的空间，但是拼接过程系统开销太大。如果允许将一个进程直接分散地装入到许多不相邻的分区中，那么就不需要再进行“紧凑”。基于这一思想而产生了离散分配方式。如果离散分配的基本单位是页，则称为分页存储管理方式；如果离散分配的基本单位是段，则称为分段存储管理方式。 在分页管理方式中，如果不具备页面对换功能（将处于阻塞状态且优先级低的进程对换到外存），则称为基本的分页存储管理方式，或称为纯分页存储管理方式，它不具有支持实现虚拟存储器的功能，它要求把每个作业全部装入内存后才能运行。 1、页面与页表 1.1、页面 a)、页面和物理块 分页存储管理是将一个进程的逻辑地址空间分成若干个大小相等的片，称为页面或者页，并为各页加以编号0、1、2...。相应地，把内存空间分成与页面相同大小的若干个存储块，称为（物理）块或页框，也加以编号0、1、2...。在为进程分配内存时，以块为单位将进程中的若干个页面分别装入到多个可以不相邻接的物理块中。由于进程的最后一页经常装不满一块而形成了不可利用的碎片——“页内碎片”。 b)、页面大小 在分页系统中的页面其大小适应中。页面若太小，一方面虽然可使内存碎片减小，从而减小了内存碎片的总空间，有利于提高内存利用率，但是另一方面也会使每个进程占用较多的页面，导致进程页表过长，占用大量内存，反而降低页面换进换出的效率。然而如果选择的页面较大，虽然可以减小页表的长度，提供页面换进换出的速度，但却又会使页内碎片增大。因此，页面的大小应选择适中，且页面大小应该是2的幂，通常为512B~8KB。 1.2、地址结构 分页地址中的地址结构如下： 0~11位为页内地址，即每页的大小为2^12=4KB；12~31位为页号，地址空间最多允许有2^20=1M页。 对于某特定机器，其地址结构是一定的。若给定一个逻辑地址空间中的地址为A，每页的大小为L，则页号P=A/L和页内地址d=A%L。 1.3、页表 在分页系统中，允许将进程的各个页离散地存储在内存不同的物理块中，但系统应该保证进程的正确运行，即能在内存中找到每个页面所对应的物理块。系统又为每个进程建立了一张页表，其记录着相应页在内存中对应的物理块号。进程在执行时，通过查找页表找到内存中对应的物理块号。可见，页表的作用的实现从页号到物理块号的地址映射。 2、地址变换机构 为了能将用户地址空间中的逻辑地址 变换为内存空间的物理地址，在系统中必须设置地址变换机构。其基本任务是实现从逻辑地址到物理地址的转换。即将逻辑地址中的页号转换为内存中的物理块号，借助页表来实现。 2.1、基本的地址变换机构 页表的功能可以由一组专门的寄存器来实现。一个页表项用一个寄存器。由于寄存器成本高，数量少，因此页表大多驻留在内存中。在系统中只设置一个页表寄存器PTR(Page-Table Register)，在其中存放页表在内存的起始地址和页表的长度。平时，进程未执行时，页表起始地址和页表长度是存放在本进程的PCB中，当调度程序调度该进程时，才将其放入页表寄存器中。因此，在单处理机环境下，虽然系统中可以运行多个进程，但只需一个页表寄存器。 具体如何转化呢? 以一道题为例子: 3.快表 块表是对基本地址转化的改进版本 1.局部性原理 2.快表 又称联想寄存器(TLB),是一种访问速度比内存快很多的高速缓冲存储器,用来存放当前经常访问的 若干页表项,以加快地址变换的过程.与此对应,内存中的页表常称为慢表 3.地址变化逻辑 (二) 分段管理 引入分段存储管理方式的目的：满足程序员在编程和使用上多方面的要求。这种存储管理方式已经成为当今所有存储管理方式的基础。 1、分段存储管理方式的引入 主要满足用户和程序员以下需求： 1)、方便编程 用户把自己的作业按照逻辑管理划分为若干段，每个段都是从0开始编址，并有自己的名字和长度。因此，希望要访问的逻辑地址是由段名（段号）和段内偏移量（段内地址）决定的。 LOAD1，[A] | ;//将分段A中D单元内的值读入寄存器1 STORE1，[B] | ;//将寄存器1的内容存入B分段的C单元中 2)、信息共享 在实现对程序和数据的共享时，是以信息的逻辑单位为基础的。。比如共享某个例程和函数。分页系统中的页只是存放信息的物理单位（块），并无完整的意义，不便于实现共享；然而段却是信息的逻辑单位。由此可知，为了实现段的共享，希望存储管理能与用户程序分段的组织方式相适应。 3)、信息保护 4)、动态增长 有些段，会随着程序的使用不断增长。而事先又无法确切地知道数据段会增长到多大。 5)、动态链接 动态链接是指在作业运行前，并不把几个目标程序段链接起来。要运行时，先将主程序所对应的目标程序装入内存并启动运行，当运行过程中有需要调用某段时，才将该段调入内存并进行链接。可见动态链接也要求以段作为管理的单位。 2、分段系统的基本原理 2.1、分段 在分段存储管理方式中，作业的地址空间被划分为若干个段，每个段定义了一组逻辑信息。例如，有主程序段MAIN、子程序段X、数据段D及栈段S等。 在该地址结构中，允许一个作业最长有64K个段，每个段的最大长度为64KB。 分段方式已得到许多编译程序的支持，编译程序能自动地根据源程序的情况而产生若各个段。 2.1、段表 在动态分配方式中，系统为整个进程分配一个连续的内存空间。而在分段式存储管理系统中，则是为每个分段分配一个连续的分区，而进程中的各个段可以离散地移入内存不同的分区中。为了使程序能正常运行，也能从物理内存中找出每个逻辑段所对应的位置，应像分页那样，在系统中为每个进程建立一段映射表，简称“段表”。每个段在表中占有一个表项。其中记录了该段在内存中的起始地址（基址）和段的长度。段表可以存放在一组寄存器中，以提高访问速度，但更常见的是将段表放在内存中。 在配置了段表后，执行中的进程可通过查找段表找到每个段所对应的内存区。可见段表是用于实现从逻辑段到物理内存区的映射： 2.3、地址变换机构 2.4、分页和分段的主要区别 a)、页是信息的物理单位，分页是为实现离散分配方式，以消减内存的外零头，提高内存的利用率；段则是信息的逻辑单位，它含有一组其意义相对完整的信息，分段的目的是为了能更好地满足用户的需要。 b)、页的大小固定且由系统决定，由系统把逻辑地址划分为页号和页内地址两部分，是由机器硬件实现的，因而在系统中只能有一种大小的页面；而段的长度却不固定，决定于用户所编写的程序，通常由编译程序在对源程序进行编译时，根据信息的性质来划分。 c)、分页的作业地址空间是一维的，即单一的线性地址空间，程序员只需利用一个记忆符，即可表示一个地址；而分段的作业地址空间则是二维的，程序员在标识一个地址是，即需给出段名，又需给出段内地址。 3、信息共享 在分段系统的允许若干个进程共享一个或多个分段，容易实现对段的保存和共享。 可重入代码又称为“纯代码”，是一种允许多个进程同时访问的代码。为使各个进程所执行的代码完全相同，绝对不允许可重入代码在执行中有任何的改变。在每个进程中，都必须配以局部数据区，把在执行中可能改变的部分拷贝到该数据区，这样程序在执行时，只需对数据区中的内容进行修改，并不去改变共享的代码，这时的可共享代码级成为可重入码。 (三).段页式存储管理方式 3.0 分段,分页的优缺点 3.1、段页式基本原理 先分段，在段内进行分页，为每一个段赋予一个段名。以下展示出了一个作业地址空间的结构。该作业有三个段，页面大小4KB。在段页式系统中，其地址结构由段号、段内页号及页内地址三部分所组成。 3.2、地址变换过程 为了方便段页式系统中地址变换的实现，需配置一个段表寄存器，其中存放段表起始地址和段表长TL。比较段号与TL是否越界，从段表寄存器中获取段表始址找到段表，根据段表内的页表始址找到对应的页表，在根据页表的存储块找到内存中的物理块，从而获取物理地址。 段页式系统中，为了获得一条指令或数据，须三次访问内存： ①　访问内存中的段表，从中取得页表始址 ②　访问内存中的页表，从中取出该页所在的物理块号，并与页内地址形成物理地址 ③　访问真正从第二次访问所得的地址中，取出指令或者数据 多次访问内存，执行速度降低，因此在地址变换机构中增设一个高速缓冲寄存器。每次访问它时，都须同时利用段号和页号去检索高速缓存，若找到匹配的表项，便可以从中得到相应页的物理块号，用来与页内地址一起形成物理地址；若未找到匹配表项，则仍需要再三次访问内存。 "},"操作系统/6.内存管理之虚拟内存.html":{"url":"操作系统/6.内存管理之虚拟内存.html","title":"6.内存管理之虚拟内存","keywords":"","body":"0 思维导图一 基本概念(一) 传统存储管理的特征及缺点(二) 局部性原理(三)计算机中存储器的层次结构(四) 虚拟内存的定义(五) 虚拟内存的实现技术二 请求分页管理(一) 页表(二) 缺页中断(三) 地址转化三 页面置换算法(一) 最佳置换算法(OPT)(二) 先进先出(FIFO)(三) 最近最久未使用(LRU)(四). 时钟(CLOCK)四 页面分配策略(一) 驻留集(二) 页面分配、置换策略(三)固定分配局部置换(四)可变分配全局置换(五)可变分配局部置换(六)策略比较五 调入页面时机(一)预调页策略(二)请求调页策略六 从何处调页(一) 磁盘对换区、文件区1、系统拥有足够的对换区空间2、系统缺少足够的对换区空间3、UNIX方式七 抖动现象0 思维导图 一 基本概念 (一) 传统存储管理的特征及缺点 之前讨论的内存管理方式都是为了多道运行环境,将多道程序同时装入内存中,但他们都有共同缺陷: 1.一次性 . 作业必须一次性全部装入内存,才能开始运行,这就导致了: 作业太大,装不下 作业太多,塞不下 2.驻留性. 作业一旦被装入内存,就会一直存在,直到运行结束,这就导致了: 若当前作业发生io操作,即使阻塞了,也会一直存在内存中 从以上分析可知,内存中由许多作业暂时无法运行,而一些准备就绪的作业却无法进入内存,浪费了大量的空间 (二) 局部性原理 1.时间局部性: 执行了程序的某条指令,在不久后很可能会再次执行这条指令 2.空间局部性 执行了某个存储单元,不久后,其附近的存储单元可能也会被访问 如这段代码: int i =0; int a[100]; while(i 在极短的时间内,a[i]=i,i++被访问100次,因此便可利用缓存技术将这段常用的指令进行高速缓存 (三)计算机中存储器的层次结构 时间局部性是将近来经常访问的数据存储到高速缓存寄存器中 空间局部性适用的是较大的高速缓存 虚拟内存技术实际上是建立了\"内存-外层\"的两级存储器结构 (四) 虚拟内存的定义 利用局部性原理的定义,在程序装入时,可以将常用的部分装入到内存,其余部分留在外存,并在必要时刻进行: 若当前所访问的信息不存在,则由操作系统将所需的页调入内存(--也即请求调换的功能) 若内存空间不够,则利用一定的算法将某些不常用的部分调出到外存(--也即置换功能) 这样,在用户看来,就好似拥有了一个比实际大的多的内存,因此称为'虚拟内存' (五) 虚拟内存的实现技术 若采用 连续分配方式,将使一部分连续的内存很长一段时间处在'暂停使用'或'空闲'的状态 因此,虚拟内存应基于离散的内存分配管理 主要存在以下三种方式: 请求分页式管理l 请求分段式管理 请求段页式管理 但无论哪一种,都需要以下几方面的支持 内外存 缺页中断机制 页表 地址转化 二 请求分页管理 请求分页与基本分页管理的区别在于,基本分页管理是一次性将程序导入内存,而请求分页是按需导入 (一) 页表 对比基本分页管理的页表,新增了四个存储字段 (二) 缺页中断 在请求分页系统中,若被访问的页面不存在内存中,进程便发起缺页中断,请求操作系统掉页进入内存.此时,发起中断的进程将被阻塞,放入阻塞队列,当调换结束后,再将其放入就绪队列 调换时,若内存中有空闲块,则系统为其分配内存 若不存在空闲块,则通过一定的置换算法,淘汰某些内存中的页(若该页在调入内存的过程中被修改过,则需要先将其写回外存) (三) 地址转化 同样在基本分页管理的基础上,新增了一些步骤: 若在快表中找到要访问的页面,则修改页表项中的访问段,然后利用页表项中的内存块号和地址中的偏移量获得物理地址 若未找到,则需到内存中查找页表,再对比页表项中的状态位P,看该页是否已经调入内存. 若未调入内存,则发出缺页中断,请求调入 三 页面置换算法 选择调出页面的算法就称为页面置换算法。好的页面置换算法应有较低的页面更换频率，也就是说，应将以后不会再访问或者以后较长时间内不会再访问的页面先调出。 常见的置换算法有以下四种（以下来自操作系统课本）。 (一) 最佳置换算法(OPT) 最佳(Optimal, OPT)置换算法所选择的被淘汰页面将是以后永不使用的，或者是在最长时间内不再被访问的页面,这样可以保证获得最低的缺页率。但由于人们目前无法预知进程在内存下的若千页面中哪个是未来最长时间内不再被访问的，因而该算法无法实现。 最佳置换算法可以用来评价其他算法。假定系统为某进程分配了三个物理块，并考虑有以下页面号引用串： 7, 0, 1, 2, 0, 3, 0, 4, 2, 3, 0, 3, 2, 1, 2, 0, 1, 7, 0, 1 进程运行时，先将7, 0, 1三个页面依次装入内存。进程要访问页面2时，产生缺页中断，根据最佳置换算法，选择第18次访问才需调入的页面7予以淘汰。然后，访问页面0时，因为已在内存中所以不必产生缺页中断。访问页面3时又会根据最佳置换算法将页面1淘汰……依此类推，如图3-26所示。从图中可以看出釆用最佳置换算法时的情况。 可以看到，发生缺页中断的次数为9，页面置换的次数为6。 访问页面 7 0 1 2 0 3 0 4 2 3 0 3 2 1 2 0 1 7 0 1 物理块1 7 7 7 2 2 2 2 2 7 物理块2 0 0 0 0 4 0 0 0 物理块3 1 1 3 3 3 1 1 缺页否 √ √ √ √ √ √ √ √ (二) 先进先出(FIFO) 优先淘汰最早进入内存的页面，亦即在内存中驻留时间最久的页面。该算法实现简单，只需把调入内存的页面根据先后次序链接成队列，设置一个指针总指向最早的页面。但该算法与进程实际运行时的规律不适应，因为在进程中，有的页面经常被访问。 访问页面 7 0 1 2 0 3 0 4 2 3 0 3 2 1 2 0 1 7 0 1 物理块1 7 7 7 2 2 2 4 4 4 0 0 0 7 7 7 物理块2 0 0 0 3 3 3 2 2 2 1 1 1 0 0 物理块3 1 1 1 0 0 0 3 3 3 2 2 2 1 缺页否 √ √ √ √ √ √ √ √ √ √ √ √ √ √ √ 　　　 这里仍用上面的实例，釆用FIFO算法进行页面置换。进程访问页面2时，把最早进入内存的页面7换出。然后访问页面3时，再把2, 0, 1中最先进入内存的页换出。由图 3-27可以看出，利用FIFO算法时进行了 12次页面置换，比最佳置换算法正好多一倍。 FIFO算法还会产生当所分配的物理块数增大而页故障数不减反增的异常现象，这是由 Belady于1969年发现，故称为Belady异常，如图3-28所示。只有FIFO算法可能出现Belady 异常，而LRU和OPT算法永远不会出现Belady异常。 访问页面 1 2 3 4 1 2 5 1 2 3 4 5 物理块1 1 1 1 4 4 4 5 5 5 物理块2 2 2 2 1 1 1 3 3 物理块3 3 3 3 2 2 2 4 缺页否 √ √ √ √ √ √ √ √ √ 1 1 1 5 5 5 5 4 4 物理块2* 2 2 2 2 1 1 1 1 5 物理块3* 3 3 3 3 2 2 2 2 物理块4* 4 4 4 4 3 3 3 缺页否 √ √ √ √ √ √ √ √ √ 　　　　图 3-28 Belady 异常 (三) 最近最久未使用(LRU) 选择最近最长时间未访问过的页面予以淘汰，它认为过去一段时间内未访问过的页面，在最近的将来可能也不会被访问。该算法为每个页面设置一个访问字段，来记录页面自上次被访问以来所经历的时间，淘汰页面时选择现有页面中值最大的予以淘汰。 再对上面的实例釆用LRU算法进行页面置换，如图3-29所示。进程第一次对页面2访问时，将最近最久未被访问的页面7置换出去。然后访问页面3时，将最近最久未使用的页面1换出。 访问页面 7 0 1 2 0 3 0 4 2 3 0 3 2 1 2 0 1 7 0 1 物理块1 7 7 7 2 2 4 4 4 0 1 1 1 物理块2 0 0 0 0 0 0 3 3 3 0 0 物理块3 1 1 3 3 2 2 2 2 2 7 缺页否 √ √ √ √ √ √ √ √ √ √ √ √ 　 在图中，前5次置换的情况与最佳置换算法相同，但两种算法并无必然联系。实际上，LRU算法根据各页以前的情况，是“向前看”的，而最佳置换算法则根据各页以后的使用情况，是“向后看”的。 LRU性能较好，但需要寄存器和栈的硬件支持。LRU是堆栈类的算法。理论上可以证明，堆栈类算法不可能出现Belady异常。FIFO算法基于队列实现，不是堆栈类算法。 (四). 时钟(CLOCK) LRU算法的性能接近于OPT,但是实现起来比较困难，且开销大；FIFO算法实现简单，但性能差。所以操作系统的设计者尝试了很多算法，试图用比较小的开销接近LRU的性能，这类算法都是CLOCK算法的变体。 简单的CLOCK算法是给每一帧关联一个附加位，称为使用位。当某一页首次装入主存时，该帧的使用位设置为1;当该页随后再被访问到时，它的使用位也被置为1。对于页替换算法，用于替换的候选帧集合看做一个循环缓冲区，并且有一个指针与之相关联。当某一页被替换时，该指针被设置成指向缓冲区中的下一帧。当需要替换一页时，操作系统扫描缓冲区，以查找使用位被置为0的一帧。每当遇到一个使用位为1的帧时，操作系统就将该位重新置为0；如果在这个过程开始时，缓冲区中所有帧的使用位均为0，则选择遇到的第一个帧替换；如果所有帧的使用位均为1,则指针在缓冲区中完整地循环一周，把所有使用位都置为0，并且停留在最初的位置上，替换该帧中的页。由于该算法循环地检查各页面的情况，故称为CLOCK算法，又称为最近未用(Not Recently Used, NRU)算法。 CLOCK算法的性能比较接近LRU，而通过增加使用的位数目，可以使得CLOCK算法更加高效。在使用位的基础上再增加一个修改位，则得到改进型的CLOCK置换算法。这样，每一帧都处于以下四种情况之一： 最近未被访问，也未被修改(u=0, m=0)。 最近被访问，但未被修改(u=1, m=0)。 最近未被访问，但被修改(u=0, m=1)。 最近被访问，被修改(u=1, m=1)。 算法执行如下操作步骤： 从指针的当前位置开始，扫描帧缓冲区。在这次扫描过程中，对使用位不做任何修改。选择遇到的第一个帧(u=0, m=0)用于替换。 如果第1)步失败，则重新扫描，查找(u=0, m=1)的帧。选择遇到的第一个这样的帧用于替换。在这个扫描过程中，对每个跳过的帧，把它的使用位设置成0。 如果第2)步失败，指针将回到它的最初位置，并且集合中所有帧的使用位均为0。重复第1步，并且如果有必要，重复第2步。这样将可以找到供替换的帧。 改进型的CLOCK算法优于简单CLOCK算法之处在于替换时首选没有变化的页。由于修改过的页在被替换之前必须写回，因而这样做会节省时间。 四 页面分配策略 (一) 驻留集 指请求分页存储管理中给进程分配的物理块的集合。 在采用了虚拟存储技术的系统中，驻留集大小一般小于进程的总大小。 应该选择一个合适的驻留集大小 若驻留集太小，会导致缺页频繁，系统要花大量的时间来处理缺页，实际用于进程推进的时间很少； 驻留集太大，会导致多道程序并发度下降，资源利用率降低。 极端情况： 若某进程共有100个页面，则该进程的驻留集大小为100时进程可以全部放入内存，运行期间不可能再发生缺页。 若驻留集大小为1，则进程运行期间必定会极频繁地缺页 (二) 页面分配、置换策略 固定分配： 操作系统为每个进程分配一组固定数目的物理块，在进程运行期间不再改变。 即，驻留集大小不变 可变分配： 先为每个进程分配一定数目的物理块，在进程运行期间，可根据情况做适当的增加或减少。 即，驻留集大小可变 局部置换：发生缺页时只能选进程自己的物理块进行置换。 全局置换：可以将操作系统保留的空闲物理块分配给缺页进程，也可以将别的进程持有的物理块置换到外存，再分配给缺页进程。 (三)固定分配局部置换 系统为每个进程分配一定数量的物理块，在整个运行期间都不改变。 若进程在运行中发生缺页，则只能从该进程在内存中的页面中选出一页换出，然后聘调入需要的页面。 缺点： 很难在刚开始就确定应为每个进程分配多少个物理块才算合理。 采用这种策略的系统可以根据进程大小、优先级、或是根据程序员给出的参数来确定为一个进程分配的内存块数 (四)可变分配全局置换 刚开始会为每个进程分配一定数量的物理块。 操作系统会保持一个空闲物理块队列。 当某进程发生缺页时，从空闲物理块中取出一块分配给该进程； 若已无空闲物理块，则可选择一个未锁定的页面换出外存，再将该物理块分配给缺页的进程。 未锁定：系统会锁定一些页面，这些页面中的内容不能置换出外存 如：重要的内核数据可以设为“锁定” 采用这种策略时，只要某进程发生缺页，都将获得新的物理块，仅当空闲物理块用完时，系统才选择一个未锁定的页面调出。被选择调出的页可能是系统中任何一个进程中的页，因此这个被选中的进程拥有的物理块会减少，缺页率会增加。 (五)可变分配局部置换 刚开始会为每个进程分配一定数量的物理块。 当某进程发生缺页时，只允许从该进程自己的物理块中选出一个进行换出外存。 如果进程在运行中频繁地缺页，系统会为该进程多分配几个物理块，直至该进程缺页率趋势适当程度； 反之，如果进程在运行中缺页率特别低，则可适当减少分配给该进程的物理块。 (六)策略比较 可变分配全局置换：只要缺页就给分配新物理块 可变分配局部置换：要根据发生缺页的频率来动态地增加或减少进程的物理块 五 调入页面时机 (一)预调页策略 根据局部性原理，一次调入若干个相邻的页面可能比一次调入一个页面更高效。但如果提前调入的频面中大多数都没被访问过，则又是低效的。 主要根据空间局部性，即：如果当前访问了某个内存单元，在之后很有可能会接着访问与其相邻的那些内存单元。 因此可以预测不久之后可能访问到的页面，将它们预先调入内存，但目前预测成功率只有50%左右。故这种策略主要用于进程的首次调入，由程序员指出应该先调入哪些部分。 (二)请求调页策略 运行时调入 进程在运行期间发现缺页时才将所缺页面调入内存 由这种策略调入的页面一定会被访问到，但由于每次只能调入一页，而每次调页都要磁盘I/O操作，因此I/O开销较大。 六 从何处调页 (一) 磁盘对换区、文件区 磁盘分为两个部分 对换区 读/写速度更快，采用连续分配方式 文件区 读/写速度更慢，采用离散分配方式 1、系统拥有足够的对换区空间 页面的调入、调出都是在内存与对换区之间进行 在进程运行前，需将进程相关的数据从文件区复制到对换区，再调入内存。 若内存空间不够，则将内存中的某些页面调出到对换区。 这样可以保证页面的调入、调出速度很快。 2、系统缺少足够的对换区空间 凡是不会被修改的数据都直接从文件区调入，由于这些页面不会被修改，因此换出时不必写回磁盘，下次需要时再从文件区调入即可。 对于可能被修改的部分，换出时需写回磁盘对换区，下次需要时再从对换区调入。 3、UNIX方式 运行之前进程有关的数据全部放在文件区，故未使用过的页面，都可从文件区调入。 若被使用过的页面需要换出,则写回对换区，下次需要时从对换区调入。 七 抖动现象 刚刚换出的页面马上又要换入内存，刚刚换入的页面马上又要换出外存，这种频繁的页面调度行为称为抖动（或颠簸）。 产生抖动的主要原因：进程频繁访问的页面数目高于可用的物理块数（分配给进程的物理块不够） 为进程分配的物理块太少，会使进程发生抖动现象。 为进程分配的物理块太多，又会降低系统整体的并发度，降低某些资源的利用率 "},"操作系统/7.IO设备管理之基本概念.html":{"url":"操作系统/7.IO设备管理之基本概念.html","title":"7.IO设备管理之基本概念","keywords":"","body":"0 思维导图一 IO设备定义(一) 定义(二) 分类二 I/O 控制器I/O设备的组成:I/O控制器的功能:I/O控制器的组成:三 OS 控制 I/O 设备的方式(一) 程序直接控制方式(二) 中断驱动方式(三)DMA方式(四) 通道控制方式四 I/O 子系统的层次结构(一) 用户软件层(二) 设备独立性软件(三) 设备驱动程序(四) 中断处理程序0 思维导图 一 IO设备定义 (一) 定义 I/O 就是输入输出 I/O设备就是可以将数据输入到计算机,或者接收计算机输出数据的外部设备,属于计算机的硬件 (二) 分类 1.按使用特性分类: 人机交互类外部设备：用于同计算机用户之间交互的设备，如打印机、显示器、鼠标、键盘等。这类设备数据交换速度相对较慢，通常是以字节为单位进行数据交换。 存储设备：用于存储程序和数据的设备，如磁盘、磁带、光盘等。这类设备用于数据交换，速度较快，通常以多字节组成的块为单位进行数据交换。 网络通信设备：用于与远程设备通信的设备，如各种网络接口、调制解调器等。其速度介于前两类设备之间。网络通信设备在使用和管理上与前两类设备也有很大不同。 2 . 按传输速率分类： 低速设备：传输速率仅为每秒几个到数百个字节的一类设备，如键盘、鼠标等。 中速设备：传输速率在每秒数千个字节至数万个字节的一类设备，如行式打印机、 激光打印机等。 高速设备：传输速率在数百个千字节至千兆字节的一类设备，如磁带机、磁盘机、 光盘机等。 按信息交换的单位分类： 块设备：由于信息的存取总是以数据块为单位，所以存储信息的设备称为块设备。它属于有结构设备，如磁盘等。磁盘设备的基本特征是传输速率较高，以及可寻址，即对它可随机地读/写任一块。 字符设备：用于数据输入/输出的设备为字符设备，因为其传输的基本单位是字符。它属于无结构类型，如交互式终端机、打印机等。它们的基本特征是传输速率低、不可寻址，并且在输入/输出时常釆用中断驱动方式。 二 I/O 控制器 I/O设备的组成: 机械部件 (主要执行具体的IO操作,如鼠标/键盘) 电子部件 (作为CPU与设备之间的中介) 因为CPU无法直接操作I/O设备,所以需要一个电子部件作为两者之间的中介,CPU通过操作电子部件间接操控IO设备 这个电子部件就称作 I/O控制器 I/O控制器的功能: I/O控制器的组成: 三 OS 控制 I/O 设备的方式 I/O 控制方式 指的是用什么样的方式来控制I/O设备的数据读/写 可用以下几个方式进行对比 读写流程 CPU 干预的频率 数据传输单位 数据的流向 优缺点 (一) 程序直接控制方式 1.读写流程: 2.CPU 干预的频率: I/O开始前后需要CPU介入,且在等待I/O过程中CPU需要不断的轮询检查状态 3.数据传输单位 一个字节 4.数据的流向 读操作: I/O设备->CPU寄存器->内存 写操作:内存->CPU寄存器->IO设备 5.优缺点 优点: 简单高效 缺点: 因为CPU高速和IO设备的低速,导致CPU一直在轮询,处于忙等状态,CPU利用率低 (二) 中断驱动方式 1.读写流程: 由于I/O设备速度很慢,因此在CPU发出I/O命令后,可将当前进程阻塞,加入该IO设备的等待队列中,让出CPU的使用权.当I/O完成后,向CPU发送中断信号,CPU检测到该信号后,进行中断处理 处理中断过程中,CPU从I/O寄存器读取数据到CPU寄存器,再将数据写入主存. 最后,恢复被阻塞的继承,继续执行程序 2.CPU 干预的频率: I/O开始前后需要CPU介入,等待过程可切到换别的进程 3.数据传输单位 一个字节 4.数据的流向 读操作: I/O设备->CPU寄存器->内存 写操作:内存->CPU寄存器->IO设备 5.优缺点 优点: CPU不再忙等,提高了CPU的利用率 缺点: 数据的传输仍然需要通过CPU这个中介,频繁的中断处理会消耗较多的CPU时间 (三)DMA方式 1.与中断驱动方式相比,DMA(direct momery access ,直接存储器存储),主要有以下的改进: 数据传输单位改为'块' 数据无需通过CPU,直接从设备到内存 CPU只在传输一个或多个块的开始或结束时才需干预 2.DMA控制器 3.缺点: CPU一条指令,只能读写 一个或多个连续的数据块, 若需要离散存储的数据库,则需要发出多条CPU指令 (四) 通道控制方式 通道控制方式是为了解决DMA方式连续存储的问题 1.读写流程: 也即,CPU给出一些I/O指令, 保存到任务清单中,通道一个个完成 2.CPU干预频率:极低,只需给出任务,接收中断请求 3.单位:一组数据块 4.可以把通道理解为'CPU的代理/小型CPU' 四 I/O 子系统的层次结构 为了使复杂的I/O软件具有清晰的,可移植的结构,在I/O软件中普遍采用了层次式的结构 上层的结构只是调用下层的结构,具体的实现只由下层实现 (一) 用户软件层 (二) 设备独立性软件 (三) 设备驱动程序 (四) 中断处理程序 "},"操作系统/8.面试题总结一.html":{"url":"操作系统/8.面试题总结一.html","title":"8.面试题总结一","keywords":"","body":"一 操作系统基础1.1 什么是操作系统？1.2 系统调用二 进程和线程2.1 进程和线程的区别2.2 进程有哪几种状态?2.3 进程间的通信方式2.4 线程间的同步的方式2.5 进程的调度算法三 操作系统内存管理基础3.1 内存管理介绍3.2 常见的几种内存管理机制3.3 快表和多级页表3.4 分页机制和分段机制的共同点和区别3.5 逻辑(虚拟)地址和物理地址3.6 CPU 寻址了解吗?为什么需要虚拟地址空间?四 虚拟内存4.1 什么是虚拟内存(Virtual Memory)?4.2 局部性原理4.3 虚拟存储器4.4 虚拟内存的技术实现4.5 页面置换算法一 操作系统基础 1.1 什么是操作系统？ &#x1F468;‍&#x1F4BB;面试官 ： 先来个简单问题吧！什么是操作系统？ &#x1F64B; 我 ：我通过以下四点向您介绍一下什么是操作系统吧！ 操作系统（Operating System，简称 OS）是管理计算机硬件与软件资源的程序，是计算机的基石。 操作系统本质上是一个运行在计算机上的软件程序 ，用于管理计算机硬件和软件资源。 举例：运行在你电脑上的所有应用程序都通过操作系统来调用系统内存以及磁盘等等硬件。 操作系统存在屏蔽了硬件层的复杂性。 操作系统就像是硬件使用的负责人，统筹着各种相关事项。 操作系统的内核（Kernel）是操作系统的核心部分，它负责系统的内存管理，硬件设备的管理，文件系统的管理以及应用程序的管理。 内核是连接应用程序和硬件的桥梁，决定着系统的性能和稳定性。 1.2 系统调用 &#x1F468;‍&#x1F4BB;面试官 ：什么是系统调用呢？ 能不能详细介绍一下。 &#x1F64B; 我 ：介绍系统调用之前，我们先来了解一下用户态和系统态。 根据进程访问资源的特点，我们可以把进程在系统上的运行分为两个级别： 用户态(user mode) : 用户态运行的进程或可以直接读取用户程序的数据。 系统态(kernel mode):可以简单的理解系统态运行的进程或程序几乎可以访问计算机的任何资源，不受限制。 说了用户态和系统态之后，那么什么是系统调用呢？ 我们运行的程序基本都是运行在用户态，如果我们调用操作系统提供的系统态级别的子功能咋办呢？那就需要系统调用了！ 也就是说在我们运行的用户程序中，凡是与系统态级别的资源有关的操作（如文件管理、进程控制、内存管理等)，都必须通过系统调用方式向操作系统提出服务请求，并由操作系统代为完成。 这些系统调用按功能大致可分为如下几类： 设备管理。完成设备的请求或释放，以及设备启动等功能。 文件管理。完成文件的读、写、创建及删除等功能。 进程控制。完成进程的创建、撤销、阻塞及唤醒等功能。 进程通信。完成进程之间的消息传递或信号传递等功能。 内存管理。完成内存的分配、回收以及获取作业占用内存区大小及地址等功能。 二 进程和线程 2.1 进程和线程的区别 &#x1F468;‍&#x1F4BB;面试官: 好的！我明白了！那你再说一下： 进程和线程的区别。 &#x1F64B; 我： 好的！ 下图是 Java 内存区域，我们从 JVM 的角度来说一下线程和进程之间的关系吧！ 如果你对 Java 内存区域 (运行时数据区) 这部分知识不太了解的话可以阅读一下这篇文章：《可能是把 Java 内存区域讲的最清楚的一篇文章》 从上图可以看出：一个进程中可以有多个线程，多个线程共享进程的堆和方法区 (JDK1.8 之后的元空间)资源，但是每个线程有自己的程序计数器、虚拟机栈 和 本地方法栈。 总结： 线程是进程划分成的更小的运行单位,一个进程在其执行的过程中可以产生多个线程。线程和进程最大的不同在于基本上各进程是独立的，而各线程则不一定，因为同一进程中的线程极有可能会相互影响。线程执行开销小，但不利于资源的管理和保护；而进程正相反。 2.2 进程有哪几种状态? &#x1F468;‍&#x1F4BB;面试官 ： 那你再说说进程有哪几种状态? &#x1F64B; 我 ：我们一般把进程大致分为 5 种状态，这一点和线程很像！ 创建状态(new) ：进程正在被创建，尚未到就绪状态。 就绪状态(ready) ：进程已处于准备运行状态，即进程获得了除了处理器之外的一切所需资源，一旦得到处理器资源(处理器分配的时间片)即可运行。 运行状态(running) ：进程正在处理器上上运行(单核 CPU 下任意时刻只有一个进程处于运行状态)。 阻塞状态(waiting) ：又称为等待状态，进程正在等待某一事件而暂停运行如等待某资源为可用或等待 IO 操作完成。即使处理器空闲，该进程也不能运行。 结束状态(terminated) ：进程正在从系统中消失。可能是进程正常结束或其他原因中断退出运行。 订正：下图中 running 状态被 interrupt 向 ready 状态转换的箭头方向反了。 2.3 进程间的通信方式 &#x1F468;‍&#x1F4BB;面试官 ：进程间的通信常见的的有哪几种方式呢? &#x1F64B; 我 ：大概有 7 种常见的进程间的通信方式。 下面这部分总结参考了:《进程间通信 IPC (InterProcess Communication)》 这篇文章，推荐阅读，总结的非常不错。 管道/匿名管道(Pipes) ：用于具有亲缘关系的父子进程间或者兄弟进程之间的通信。 有名管道(Names Pipes) : 匿名管道由于没有名字，只能用于亲缘关系的进程间通信。为了克服这个缺点，提出了有名管道。有名管道严格遵循先进先出(first in first out)。有名管道以磁盘文件的方式存在，可以实现本机任意两个进程通信。 信号(Signal) ：信号是一种比较复杂的通信方式，用于通知接收进程某个事件已经发生； 消息队列(Message Queuing) ：消息队列是消息的链表,具有特定的格式,存放在内存中并由消息队列标识符标识。管道和消息队列的通信数据都是先进先出的原则。与管道（无名管道：只存在于内存中的文件；命名管道：存在于实际的磁盘介质或者文件系统）不同的是消息队列存放在内核中，只有在内核重启(即，操作系统重启)或者显示地删除一个消息队列时，该消息队列才会被真正的删除。消息队列可以实现消息的随机查询,消息不一定要以先进先出的次序读取,也可以按消息的类型读取.比 FIFO 更有优势。消息队列克服了信号承载信息量少，管道只能承载无格式字 节流以及缓冲区大小受限等缺。 信号量(Semaphores) ：信号量是一个计数器，用于多进程对共享数据的访问，信号量的意图在于进程间同步。这种通信方式主要用于解决与同步相关的问题并避免竞争条件。 共享内存(Shared memory) ：使得多个进程可以访问同一块内存空间，不同进程可以及时看到对方进程中对共享内存中数据的更新。这种方式需要依靠某种同步操作，如互斥锁和信号量等。可以说这是最有用的进程间通信方式。 套接字(Sockets) : 此方法主要用于在客户端和服务器之间通过网络进行通信。套接字是支持 TCP/IP 的网络通信的基本操作单元，可以看做是不同主机之间的进程进行双向通信的端点，简单的说就是通信的两方的一种约定，用套接字中的相关函数来完成通信过程。 2.4 线程间的同步的方式 &#x1F468;‍&#x1F4BB;面试官 ：那线程间的同步的方式有哪些呢? &#x1F64B; 我 ：线程同步是两个或多个共享关键资源的线程的并发执行。应该同步线程以避免关键的资源使用冲突。操作系统一般有下面三种线程同步的方式： 互斥量(Mutex)：采用互斥对象机制，只有拥有互斥对象的线程才有访问公共资源的权限。因为互斥对象只有一个，所以可以保证公共资源不会被多个线程同时访问。比如 Java 中的 synchronized 关键词和各种 Lock 都是这种机制。 信号量(Semphares) ：它允许同一时刻多个线程访问同一资源，但是需要控制同一时刻访问此资源的最大线程数量 事件(Event) :Wait/Notify：通过通知操作的方式来保持多线程同步，还可以方便的实现多线程优先级的比较操 2.5 进程的调度算法 &#x1F468;‍&#x1F4BB;面试官 ：你知道操作系统中进程的调度算法有哪些吗? &#x1F64B; 我 ：嗯嗯！这个我们大学的时候学过，是一个很重要的知识点！ 为了确定首先执行哪个进程以及最后执行哪个进程以实现最大 CPU 利用率，计算机科学家已经定义了一些算法，它们是： 先到先服务(FCFS)调度算法 : 从就绪队列中选择一个最先进入该队列的进程为之分配资源，使它立即执行并一直执行到完成或发生某事件而被阻塞放弃占用 CPU 时再重新调度。 短作业优先(SJF)的调度算法 : 从就绪队列中选出一个估计运行时间最短的进程为之分配资源，使它立即执行并一直执行到完成或发生某事件而被阻塞放弃占用 CPU 时再重新调度。 时间片轮转调度算法 : 时间片轮转调度是一种最古老，最简单，最公平且使用最广的算法，又称 RR(Round robin)调度。每个进程被分配一个时间段，称作它的时间片，即该进程允许运行的时间。 多级反馈队列调度算法 ：前面介绍的几种进程调度的算法都有一定的局限性。如短进程优先的调度算法，仅照顾了短进程而忽略了长进程 。多级反馈队列调度算法既能使高优先级的作业得到响应又能使短作业（进程）迅速完成。，因而它是目前被公认的一种较好的进程调度算法，UNIX 操作系统采取的便是这种调度算法。 优先级调度 ： 为每个流程分配优先级，首先执行具有最高优先级的进程，依此类推。具有相同优先级的进程以 FCFS 方式执行。可以根据内存要求，时间要求或任何其他资源要求来确定优先级。 三 操作系统内存管理基础 3.1 内存管理介绍 &#x1F468;‍&#x1F4BB; 面试官: 操作系统的内存管理主要是做什么？ &#x1F64B; 我： 操作系统的内存管理主要负责内存的分配与回收（malloc 函数：申请内存，free 函数：释放内存），另外地址转换也就是将逻辑地址转换成相应的物理地址等功能也是操作系统内存管理做的事情。 3.2 常见的几种内存管理机制 &#x1F468;‍&#x1F4BB; 面试官: 操作系统的内存管理机制了解吗？内存管理有哪几种方式? &#x1F64B; 我： 这个在学习操作系统的时候有了解过。 简单分为连续分配管理方式和非连续分配管理方式这两种。连续分配管理方式是指为一个用户程序分配一个连续的内存空间，常见的如 块式管理 。同样地，非连续分配管理方式允许一个程序使用的内存分布在离散或者说不相邻的内存中，常见的如页式管理 和 段式管理。 块式管理 ： 远古时代的计算机操系统的内存管理方式。将内存分为几个固定大小的块，每个块中只包含一个进程。如果程序运行需要内存的话，操作系统就分配给它一块，如果程序运行只需要很小的空间的话，分配的这块内存很大一部分几乎被浪费了。这些在每个块中未被利用的空间，我们称之为碎片。 页式管理 ：把主存分为大小相等且固定的一页一页的形式，页较小，相对相比于块式管理的划分力度更大，提高了内存利用率，减少了碎片。页式管理通过页表对应逻辑地址和物理地址。 段式管理 ： 页式管理虽然提高了内存利用率，但是页式管理其中的页实际并无任何实际意义。 段式管理把主存分为一段段的，每一段的空间又要比一页的空间小很多 。但是，最重要的是段是有实际意义的，每个段定义了一组逻辑信息，例如,有主程序段 MAIN、子程序段 X、数据段 D 及栈段 S 等。 段式管理通过段表对应逻辑地址和物理地址。 &#x1F468;‍&#x1F4BB;面试官 ： 回答的还不错！不过漏掉了一个很重要的 段页式管理机制 。段页式管理机制结合了段式管理和页式管理的优点。简单来说段页式管理机制就是把主存先分成若干段，每个段又分成若干页，也就是说 段页式管理机制 中段与段之间以及段的内部的都是离散的。 &#x1F64B; 我 ：谢谢面试官！刚刚把这个给忘记了～ 3.3 快表和多级页表 &#x1F468;‍&#x1F4BB;面试官 ： 页表管理机制中有两个很重要的概念：快表和多级页表，这两个东西分别解决了页表管理中很重要的两个问题。你给我简单介绍一下吧！ &#x1F64B; 我 ：在分页内存管理中，很重要的两点是： 虚拟地址到物理地址的转换要快。 解决虚拟地址空间大，页表也会很大的问题。 快表 为了解决虚拟地址到物理地址的转换速度，操作系统在 页表方案 基础之上引入了 快表 来加速虚拟地址到物理地址的转换。我们可以把快表理解为一种特殊的高速缓冲存储器（Cache），其中的内容是页表的一部分或者全部内容。作为页表的 Cache，它的作用与页表相似，但是提高了访问速率。由于采用页表做地址转换，读写内存数据时 CPU 要访问两次主存。有了快表，有时只要访问一次高速缓冲存储器，一次主存，这样可加速查找并提高指令执行速度。 使用快表之后的地址转换流程是这样的： 根据虚拟地址中的页号查快表； 如果该页在快表中，直接从快表中读取相应的物理地址； 如果该页不在快表中，就访问内存中的页表，再从页表中得到物理地址，同时将页表中的该映射表项添加到快表中； 当快表填满后，又要登记新页时，就按照一定的淘汰策略淘汰掉快表中的一个页。 看完了之后你会发现快表和我们平时经常在我们开发的系统使用的缓存（比如 Redis）很像，的确是这样的，操作系统中的很多思想、很多经典的算法，你都可以在我们日常开发使用的各种工具或者框架中找到它们的影子。 多级页表 引入多级页表的主要目的是为了避免把全部页表一直放在内存中占用过多空间，特别是那些根本就不需要的页表就不需要保留在内存中。多级页表属于时间换空间的典型场景，具体可以查看下面这篇文章 多级页表如何节约内存：https://www.polarxiong.com/archives/多级页表如何节约内存.html 总结 为了提高内存的空间性能，提出了多级页表的概念；但是提到空间性能是以浪费时间性能为基础的，因此为了补充损失的时间性能，提出了快表（即 TLB）的概念。 不论是快表还是多级页表实际上都利用到了程序的局部性原理，局部性原理在后面的虚拟内存这部分会介绍到。 3.4 分页机制和分段机制的共同点和区别 &#x1F468;‍&#x1F4BB;面试官 ： 分页机制和分段机制有哪些共同点和区别呢？ &#x1F64B; 我 ： 共同点 ： 分页机制和分段机制都是为了提高内存利用率，较少内存碎片。 页和段都是离散存储的，所以两者都是离散分配内存的方式。但是，每个页和段中的内存是连续的。 区别 ： 页的大小是固定的，由操作系统决定；而段的大小不固定，取决于我们当前运行的程序。 分页仅仅是为了满足操作系统内存管理的需求，而段是逻辑信息的单位，在程序中可以体现为代码段，数据段，能够更好满足用户的需要。 3.5 逻辑(虚拟)地址和物理地址 &#x1F468;‍&#x1F4BB;面试官 ：你刚刚还提到了逻辑地址和物理地址这两个概念，我不太清楚，你能为我解释一下不？ &#x1F64B; 我： em...好的嘛！我们编程一般只有可能和逻辑地址打交道，比如在 C 语言中，指针里面存储的数值就可以理解成为内存里的一个地址，这个地址也就是我们说的逻辑地址，逻辑地址由操作系统决定。物理地址指的是真实物理内存中地址，更具体一点来说就是内存地址寄存器中的地址。物理地址是内存单元真正的地址。 3.6 CPU 寻址了解吗?为什么需要虚拟地址空间? &#x1F468;‍&#x1F4BB;面试官 ：CPU 寻址了解吗?为什么需要虚拟地址空间? &#x1F64B; 我 ：这部分我真不清楚！ 这部分内容参考了 Microsoft 官网的介绍，地址：https://msdn.microsoft.com/zh-cn/library/windows/hardware/hh439648(v=vs.85).aspx 现代处理器使用的是一种称为 虚拟寻址(Virtual Addressing) 的寻址方式。使用虚拟寻址，CPU 需要将虚拟地址翻译成物理地址，这样才能访问到真实的物理内存。 实际上完成虚拟地址转换为物理地址转换的硬件是 CPU 中含有一个被称为 内存管理单元（Memory Management Unit, MMU） 的硬件。如下图所示： 为什么要有虚拟地址空间呢？ 先从没有虚拟地址空间的时候说起吧！没有虚拟地址空间的时候，程序都是直接访问和操作的都是物理内存 。但是这样有什么问题呢？ 用户程序可以访问任意内存，寻址内存的每个字节，这样就很容易（有意或者无意）破坏操作系统，造成操作系统崩溃。 想要同时运行多个程序特别困难，比如你想同时运行一个微信和一个 QQ 音乐都不行。为什么呢？举个简单的例子：微信在运行的时候给内存地址 1xxx 赋值后，QQ 音乐也同样给内存地址 1xxx 赋值，那么 QQ 音乐对内存的赋值就会覆盖微信之前所赋的值，这就造成了微信这个程序就会崩溃。 总结来说：如果直接把物理地址暴露出来的话会带来严重问题，比如可能对操作系统造成伤害以及给同时运行多个程序造成困难。 通过虚拟地址访问内存有以下优势： 程序可以使用一系列相邻的虚拟地址来访问物理内存中不相邻的大内存缓冲区。 程序可以使用一系列虚拟地址来访问大于可用物理内存的内存缓冲区。当物理内存的供应量变小时，内存管理器会将物理内存页（通常大小为 4 KB）保存到磁盘文件。数据或代码页会根据需要在物理内存与磁盘之间移动。 不同进程使用的虚拟地址彼此隔离。一个进程中的代码无法更改正在由另一进程或操作系统使用的物理内存。 四 虚拟内存 4.1 什么是虚拟内存(Virtual Memory)? &#x1F468;‍&#x1F4BB;面试官 ：再问你一个常识性的问题！什么是虚拟内存(Virtual Memory)? &#x1F64B; 我 ：这个在我们平时使用电脑特别是 Windows 系统的时候太常见了。很多时候我们使用点开了很多占内存的软件，这些软件占用的内存可能已经远远超出了我们电脑本身具有的物理内存。为什么可以这样呢？ 正是因为 虚拟内存 的存在，通过 虚拟内存 可以让程序可以拥有超过系统物理内存大小的可用内存空间。另外，虚拟内存为每个进程提供了一个一致的、私有的地址空间，它让每个进程产生了一种自己在独享主存的错觉（每个进程拥有一片连续完整的内存空间）。这样会更加有效地管理内存并减少出错。 虚拟内存是计算机系统内存管理的一种技术，我们可以手动设置自己电脑的虚拟内存。不要单纯认为虚拟内存只是“使用硬盘空间来扩展内存“的技术。虚拟内存的重要意义是它定义了一个连续的虚拟地址空间，并且 把内存扩展到硬盘空间。推荐阅读：《虚拟内存的那点事儿》 维基百科中有几句话是这样介绍虚拟内存的。 虚拟内存 使得应用程序认为它拥有连续的可用的内存（一个连续完整的地址空间），而实际上，它通常是被分隔成多个物理内存碎片，还有部分暂时存储在外部磁盘存储器上，在需要时进行数据交换。与没有使用虚拟内存技术的系统相比，使用这种技术的系统使得大型程序的编写变得更容易，对真正的物理内存（例如 RAM）的使用也更有效率。目前，大多数操作系统都使用了虚拟内存，如 Windows 家族的“虚拟内存”；Linux 的“交换空间”等。From:https://zh.wikipedia.org/wiki/虚拟内存 4.2 局部性原理 &#x1F468;‍&#x1F4BB;面试官 ：要想更好地理解虚拟内存技术，必须要知道计算机中著名的局部性原理。另外，局部性原理既适用于程序结构，也适用于数据结构，是非常重要的一个概念。 &#x1F64B; 我 ：局部性原理是虚拟内存技术的基础，正是因为程序运行具有局部性原理，才可以只装入部分程序到内存就开始运行。 以下内容摘自《计算机操作系统教程》 第 4 章存储器管理。 早在 1968 年的时候，就有人指出我们的程序在执行的时候往往呈现局部性规律，也就是说在某个较短的时间段内，程序执行局限于某一小部分，程序访问的存储空间也局限于某个区域。 局部性原理表现在以下两个方面： 时间局部性 ：如果程序中的某条指令一旦执行，不久以后该指令可能再次执行；如果某数据被访问过，不久以后该数据可能再次被访问。产生时间局部性的典型原因，是由于在程序中存在着大量的循环操作。 空间局部性 ：一旦程序访问了某个存储单元，在不久之后，其附近的存储单元也将被访问，即程序在一段时间内所访问的地址，可能集中在一定的范围之内，这是因为指令通常是顺序存放、顺序执行的，数据也一般是以向量、数组、表等形式簇聚存储的。 时间局部性是通过将近来使用的指令和数据保存到高速缓存存储器中，并使用高速缓存的层次结构实现。空间局部性通常是使用较大的高速缓存，并将预取机制集成到高速缓存控制逻辑中实现。虚拟内存技术实际上就是建立了 “内存一外存”的两级存储器的结构，利用局部性原理实现髙速缓存。 4.3 虚拟存储器 &#x1F468;‍&#x1F4BB;面试官 ：都说了虚拟内存了。你再讲讲虚拟存储器把！ &#x1F64B; 我 ： 这部分内容来自：王道考研操作系统知识点整理。 基于局部性原理，在程序装入时，可以将程序的一部分装入内存，而将其他部分留在外存，就可以启动程序执行。由于外存往往比内存大很多，所以我们运行的软件的内存大小实际上是可以比计算机系统实际的内存大小大的。在程序执行过程中，当所访问的信息不在内存时，由操作系统将所需要的部分调入内存，然后继续执行程序。另一方面，操作系统将内存中暂时不使用的内容换到外存上，从而腾出空间存放将要调入内存的信息。这样，计算机好像为用户提供了一个比实际内存大的多的存储器——虚拟存储器。 实际上，我觉得虚拟内存同样是一种时间换空间的策略，你用 CPU 的计算时间，页的调入调出花费的时间，换来了一个虚拟的更大的空间来支持程序的运行。不得不感叹，程序世界几乎不是时间换空间就是空间换时间。 4.4 虚拟内存的技术实现 &#x1F468;‍&#x1F4BB;面试官 ：虚拟内存技术的实现呢？ &#x1F64B; 我 ：虚拟内存的实现需要建立在离散分配的内存管理方式的基础上。 虚拟内存的实现有以下三种方式： 请求分页存储管理 ：建立在分页管理之上，为了支持虚拟存储器功能而增加了请求调页功能和页面置换功能。请求分页是目前最常用的一种实现虚拟存储器的方法。请求分页存储管理系统中，在作业开始运行之前，仅装入当前要执行的部分段即可运行。假如在作业运行的过程中发现要访问的页面不在内存，则由处理器通知操作系统按照对应的页面置换算法将相应的页面调入到主存，同时操作系统也可以将暂时不用的页面置换到外存中。 请求分段存储管理 ：建立在分段存储管理之上，增加了请求调段功能、分段置换功能。请求分段储存管理方式就如同请求分页储存管理方式一样，在作业开始运行之前，仅装入当前要执行的部分段即可运行；在执行过程中，可使用请求调入中断动态装入要访问但又不在内存的程序段；当内存空间已满，而又需要装入新的段时，根据置换功能适当调出某个段，以便腾出空间而装入新的段。 请求段页式存储管理 这里多说一下？很多人容易搞混请求分页与分页存储管理，两者有何不同呢？ 请求分页存储管理建立在分页管理之上。他们的根本区别是是否将程序全部所需的全部地址空间都装入主存，这也是请求分页存储管理可以提供虚拟内存的原因，我们在上面已经分析过了。 它们之间的根本区别在于是否将一作业的全部地址空间同时装入主存。请求分页存储管理不要求将作业全部地址空间同时装入主存。基于这一点，请求分页存储管理可以提供虚存，而分页存储管理却不能提供虚存。 不管是上面那种实现方式，我们一般都需要： 一定容量的内存和外存：在载入程序的时候，只需要将程序的一部分装入内存，而将其他部分留在外存，然后程序就可以执行了； 缺页中断：如果需执行的指令或访问的数据尚未在内存（称为缺页或缺段），则由处理器通知操作系统将相应的页面或段调入到内存，然后继续执行程序； 虚拟地址空间 ：逻辑地址到物理地址的变换。 4.5 页面置换算法 &#x1F468;‍&#x1F4BB;面试官 ：虚拟内存管理很重要的一个概念就是页面置换算法。那你说一下 页面置换算法的作用?常见的页面置换算法有哪些? &#x1F64B; 我 ： 这个题目经常作为笔试题出现，网上已经给出了很不错的回答，我这里只是总结整理了一下。 地址映射过程中，若在页面中发现所要访问的页面不在内存中，则发生缺页中断 。 缺页中断 就是要访问的页不在主存，需要操作系统将其调入主存后再进行访问。 在这个时候，被内存映射的文件实际上成了一个分页交换文件。 当发生缺页中断时，如果当前内存中并没有空闲的页面，操作系统就必须在内存选择一个页面将其移出内存，以便为即将调入的页面让出空间。用来选择淘汰哪一页的规则叫做页面置换算法，我们可以把页面置换算法看成是淘汰页面的规则。 OPT 页面置换算法（最佳页面置换算法） ：最佳(Optimal, OPT)置换算法所选择的被淘汰页面将是以后永不使用的，或者是在最长时间内不再被访问的页面,这样可以保证获得最低的缺页率。但由于人们目前无法预知进程在内存下的若千页面中哪个是未来最长时间内不再被访问的，因而该算法无法实现。一般作为衡量其他置换算法的方法。 FIFO（First In First Out） 页面置换算法（先进先出页面置换算法） : 总是淘汰最先进入内存的页面，即选择在内存中驻留时间最久的页面进行淘汰。 LRU （Least Currently Used）页面置换算法（最近最久未使用页面置换算法） ：LRU算法赋予每个页面一个访问字段，用来记录一个页面自上次被访问以来所经历的时间 T，当须淘汰一个页面时，选择现有页面中其 T 值最大的，即最近最久未使用的页面予以淘汰。 LFU （Least Frequently Used）页面置换算法（最少使用页面置换算法） : 该置换算法选择在之前时期使用最少的页面作为淘汰页。 "},"计算机网络/1.计算机网络入门基础概念.html":{"url":"计算机网络/1.计算机网络入门基础概念.html","title":"1.计算机网络入门文章(必读)","keywords":"","body":"1.计算机网络基础1 osi各层机构和功能1.1 应用层1.2 运输层1.3 网络层1.4 数据链路层1.5 物理层2.tcp/ip协议栈1.什么是TCP/IP2.数据包1.应用层2.传输层3.网络层4.网络接口层（更多）1.计算机网络基础 1 osi各层机构和功能 学习计算机网络时我们一般采用折中的办法，也就是中和 OSI 和 TCP/IP 的优点，采用一种只有五层协议的体系结构，这样既简洁又能将概念阐述清楚。 1.1 应用层 应用层(application-layer）的任务是通过应用进程间的交互来完成特定网络应用。应用层协议定义的是应用进程（进程：主机中正在运行的程序）间的通信和交互的规则。对于不同的网络应用需要不同的应用层协议。在互联网中应用层协议很多，如域名系统DNS，支持万维网应用的 HTTP协议，支持电子邮件的 SMTP协议等等。我们把应用层交互的数据单元称为报文。 域名系统 域名系统(Domain Name System缩写 DNS，Domain Name被译为域名)是因特网的一项核心服务，它作为可以将域名和IP地址相互映射的一个分布式数据库，能够使人更方便的访问互联网，而不用去记住能够被机器直接读取的IP数串。（百度百科）例如：一个公司的 Web 网站可看作是它在网上的门户，而域名就相当于其门牌地址，通常域名都使用该公司的名称或简称。例如上面提到的微软公司的域名，类似的还有：IBM 公司的域名是 www.ibm.com、Oracle 公司的域名是 www.oracle.com、Cisco公司的域名是 www.cisco.com 等。 HTTP协议 超文本传输协议（HTTP，HyperText Transfer Protocol)是互联网上应用最为广泛的一种网络协议。所有的 WWW（万维网） 文件都必须遵守这个标准。设计 HTTP 最初的目的是为了提供一种发布和接收 HTML 页面的方法。（百度百科） 1.2 运输层 运输层(transport layer)的主要任务就是负责向两台主机进程之间的通信提供通用的数据传输服务。应用进程利用该服务传送应用层报文。“通用的”是指并不针对某一个特定的网络应用，而是多种应用可以使用同一个运输层服务。由于一台主机可同时运行多个线程，因此运输层有复用和分用的功能。所谓复用就是指多个应用层进程可同时使用下面运输层的服务，分用和复用相反，是运输层把收到的信息分别交付上面应用层中的相应进程。 运输层主要使用以下两种协议: 传输控制协议 TCP（Transmission Control Protocol）--提供面向连接的，可靠的数据传输服务。 用户数据协议 UDP（User Datagram Protocol）--提供无连接的，尽最大努力的数据传输服务（不保证数据传输的可靠性）。 1.3 网络层 在 计算机网络中进行通信的两个计算机之间可能会经过很多个数据链路，也可能还要经过很多通信子网。网络层的任务就是选择合适的网间路由和交换结点， 确保数据及时传送。 在发送数据时，网络层把运输层产生的报文段或用户数据报封装成分组和包进行传送。在 TCP/IP 体系结构中，由于网络层使用 IP 协议，因此分组也叫 IP 数据报 ，简称 数据报。 这里要注意：不要把运输层的“用户数据报 UDP ”和网络层的“ IP 数据报”弄混。另外，无论是哪一层的数据单元，都可笼统地用“分组”来表示。 这里强调指出，网络层中的“网络”二字已经不是我们通常谈到的具体网络，而是指计算机网络体系结构模型中第三层的名称. 互联网是由大量的异构（heterogeneous）网络通过路由器（router）相互连接起来的。互联网使用的网络层协议是无连接的网际协议（Intert Protocol）和许多路由选择协议，因此互联网的网络层也叫做网际层或IP层。 1.4 数据链路层 数据链路层(data link layer)通常简称为链路层。两台主机之间的数据传输，总是在一段一段的链路上传送的，这就需要使用专门的链路层的协议。 在两个相邻节点之间传送数据时，数据链路层将网络层交下来的 IP 数据报组装成帧，在两个相邻节点间的链路上传送帧。每一帧包括数据和必要的控制信息（如同步信息，地址信息，差错控制等）。 在接收数据时，控制信息使接收端能够知道一个帧从哪个比特开始和到哪个比特结束。这样，数据链路层在收到一个帧后，就可从中提出数据部分，上交给网络层。 控制信息还使接收端能够检测到所收到的帧中有误差错。如果发现差错，数据链路层就简单地丢弃这个出了差错的帧，以避免继续在网络中传送下去白白浪费网络资源。如果需要改正数据在链路层传输时出现差错（这就是说，数据链路层不仅要检错，而且还要纠错），那么就要采用可靠性传输协议来纠正出现的差错。这种方法会使链路层的协议复杂些。 1.5 物理层 在物理层上所传送的数据单位是比特。 物理层(physical layer)的作用是实现相邻计算机节点之间比特流的透明传送，尽可能屏蔽掉具体传输介质和物理设备的差异。 使其上面的数据链路层不必考虑网络的具体传输介质是什么。“透明传送比特流”表示经实际电路传送后的比特流没有发生变化，对传送的比特流来说，这个电路好像是看不见的。 在互联网使用的各种协中最重要和最著名的就是 TCP/IP 两个协议。现在人们经常提到的TCP/IP并不一定单指TCP和IP这两个具体的协议，而往往表示互联网所使用的整个TCP/IP协议族。 2.tcp/ip协议栈 1.什么是TCP/IP TCP/IP是一套用于网络通信的协议集合或者系统。TCP/IP协议模型就有OSI模型分为7层。但其实一般我们所谈到的都是四层的TCP/IP协议栈。 网络接口层：主要是指一些物理层层次的接口，比如电缆等 网络层：提供了独立于硬件的逻辑寻址，实现物理地址和逻辑地址的转换。网络层协议包括IP协议（网际协议），ICMP协议（互联网控制报文协议），IGMP协议(Internet组协议管理) 传输层：为网络提供了流量控制，错误控制和确认服务。传输层有两个互不相同的传输协议：TCP（传输控制协议）、UDP（用户数据报协议） 应用层：为文件传输，网络排错和Internet操作提供具体的程序应用 2.数据包 在TCP/IP协议中数据由上至下将数据封装成包，然后再由下至上的拆包。那么数据又是怎么打包的呢？ 在装包的时候，每一层都会增加一些信息用于传输，这部分信息叫做报头。当上层数据到达本层的时候，会将数据加上报头打包在一起形成新的数据包继续往下一层传递。拆包的时候就是反着来了，就像俄罗斯套娃一样，拆完最外面一层得到需要的报头，向上传递。 1.应用层 应用层作为TCP/IP协议的最上层，其实是我们接触最多的。 由于在传输层的传输协议大致分成了TCP和UDP，所以在应用层对应的协议也就分成了两部分。 运行在TCP协议上的协议： HTTP（Hypertext Transfer Protocol，超文本传输协议），主要用于普通浏览。 HTTPS（Hypertext Transfer Protocol over Secure Socket Layer, or HTTP over SSL，安全超文本传输协议）,HTTP协议的安全版本。 FTP（File Transfer Protocol，文件传输协议），由名知义，用于文件传输。 POP3（Post Office Protocol, version 3，邮局协议），收邮件用。 SMTP（Simple Mail Transfer Protocol，简单邮件传输协议），用来发送电子邮件。 TELNET（Teletype over the Network，网络电传），通过一个终端（terminal）登陆到网络。 SSH（Secure Shell，用于替代安全性差的TELNET），用于加密安全登陆用。 运行在UDP协议上的协议： BOOTP（Boot Protocol，启动协议），应用于无盘设备。 NTP（Network Time Protocol，网络时间协议），用于网络同步。 DHCP（Dynamic Host Configuration Protocol，动态主机配置协议），动态配置IP地址。 2.传输层 http封装请求数据包以后传给传输层，tcp协议部分开始运作。这里将数据包和TCP报头生成TCP报文，打包成新的数据包。 TCP报文结构（点击查看详情）： 3.网络层 TCP数据包到了这一层，再加上IP报文生成新的IP数据包 前面有提到网络层主要负责物理地址（mac）和逻辑地址（ip）的转换。 ICMP（Internet Control Message Protocol：互联网控制消息协议）：主要负责网络层和传输层的数据交换，是为了更有效地转发IP数据报文和提高数据报文交付成功的机会，是介于传输层和网络层之间的协议。 ARP（Address Resolution Protocol：地址解析协议）：主要是将IP地址解析成MAC地址的协议。 RARP（Reverse Address Resolution Protocol：逆地址解析协议）**：正好相反，是将MAC地址解析成IP地址的协议。** IP协议（Internet Protocol：网际协议）：是TCP/IP协议族中最为核心的协议。它提供不可靠、无连接的服务，也即依赖其他层的协议进行差错控制。 报文结构格式（了解更多）： 4.网络接口层（更多） 这一层主要涉及到一些物理传输，比如以太网，无线局域网，电缆等 IP数据包到了这层就不一样了啊！数据链路会在IP数据报的首尾加上首部和尾部代表数据包的结束，封装成帧。首部和尾部都是8位2进制表示，可以一样也可以不一样。 　　链路：一条点到点的物理线路段，中间没有任何其他的交换结点，通俗的将，就是一根线，其中不经过任何东西，这样的就是链路，一条链路只是一条通路的一个组成部分 　　数据链路：除了物理线路外，还必须有通信协议来控制这些数据的传输。若把实现这些协议的硬件和软件加到链路上，就构成了数据链路。 通俗讲，就是经过了一些交换机呀，什么的。 　　　　　　　最终到达目的地，所有路段就是数据链路，而数据链路中就包含了多段链路。 　　适配器：也就是网卡，就是用来实现数据链路上一些协议。 ​ 帧：数据链路层上传送的就是帧 如果再往下到物理层呢？就成为比特流传输了 至此，一个请求就完成了由应用层到物理层的传递。在各种交换机中找到最后的服务器地址。然后再把数据封装反着来一遍。再将请求一步步封装传出去，同样的方式由客户端拿到数据，Http协议解析读取显示。 "},"计算机网络/2.计算机网络通信基础.html":{"url":"计算机网络/2.计算机网络通信基础.html","title":"2.计算机网络通信入门文章(必读)","keywords":"","body":"网络通信基础0.同步,异步 ,阻塞/非阻塞0.1 进程通信上下文的同步/异步， 阻塞/非阻塞0.2 system call 层面0.3 总结1 socket入门1.1 socket简介1.2 socket基本操作1.3 socket和tcp的关系2 五大io 模型1.阻塞IO模型2.非阻塞IO模型3.多路复用IO模型4.信号驱动IO模型5.异步IO模型3 多路复用io模型详解3.0 基础基础3.1 select3.2 EPOLL3.3 poll网络通信基础 网络通信偏向于操作系统层面的知识,但是也是计算机网络的基础,是理解好计算机网络的的前提 0.同步,异步 ,阻塞/非阻塞 0.1 进程通信上下文的同步/异步， 阻塞/非阻塞 首先强调一点， 网络上的很多博文关于同步/异步， 阻塞非阻塞区别的解释其实都很经不起推敲。 例如怎样理解阻塞非阻塞与同步异步的区别 这一高赞回答中 ， 有如下解释（不准确）： 同步/异步关注的是消息通信机制 (synchronous communication/ asynchronous communication) 。 所谓同步，就是在发出一个调用时，在没有得到结果之前， 该调用就不返回。 异步则是相反，调用在发出之后，这个调用就直接返回了，所以没有返回结果 阻塞/非阻塞关注的是程序在等待调用结果（消息，返回值）时的状态. 阻塞调用是指调用结果返回之前，当前线程会被挂起。调用线程只有在得到结果之后才会返回。 非阻塞调用指在不能立刻得到结果之前，该调用不会阻塞当前线程。 粗一看， 好像同步/ 非同步， 阻塞/非阻塞 是两种维度的概念， 可以分别对待， 但是稍微推敲一下就会发现上述的解释根本难以自圆其说。 如果“同步” 是发起了一个调用后， 没有得到结果之前不返回， 那它毫无疑问就是被\"阻塞\"了（即调用进程处于 “waiting” 状态）。 如果调用发出了以后就直接返回了， 毫无疑问， 这个进程没有被“阻塞”。 所以， 上述的解释是不准确的。 让我们看一下《操作系统概念（第九版）》中有关进程间通信的部分是如何解释的： 翻译一下就是： 进程间的通信时通过 send() 和 receive() 两种基本操作完成的。具体如何实现这两种基础操作，存在着不同的设计。 消息的传递有： 阻塞式发送（blocking send）. 发送方进程会被一直阻塞， 直到消息被接受方进程收到。 非阻塞式发送（nonblocking send）。 发送方进程调用 send() 后， 立即就可以其他操作。 阻塞式接收（blocking receive） 接收方调用 receive() 后一直阻塞， 直到消息到达可用。 非阻塞式接受（nonblocking receive） 接收方调用 receive() 函数后， 要么得到一个有效的结果， 要么得到一个空值， 即不会被阻塞。 上述不同类型的发送方式和不同类型的接收方式，可以自由组合。 也就是说， 从进程级通信的维度讨论时， 阻塞和同步（非阻塞和异步）就是一对同义词， 且需要针对发送方和接收方作区分对待。 0.2 system call 层面 0.2.1 先修知识 用户空间和内核空间 进程切换 系统调用（system call） 中断（interrupt） 进程的阻塞 用户空间和内核空间 操作系统为了支持多个应用同时运行，需要保证不同进程之间相对独立（一个进程的崩溃不会影响其他的进程 ， 恶意进程不能直接读取和修改其他进程运行时的代码和数据）。 因此操作系统内核需要拥有高于普通进程的权限， 以此来调度和管理用户的应用程序。 于是内存空间被划分为两部分，一部分为内核空间，一部分为用户空间，内核空间存储的代码和数据具有更高级别的权限。内存访问的相关硬件在程序执行期间会进行访问控制（ Access Control），使得用户空间的程序不能直接读写内核空间的内存。 有《微机原理》 课程基础同学可以 Google 搜索 DPL, CPL 这两个关键字了解硬件层面的内存访问权限控制细节 进程切换 上图展示了进程切换中几个最重要的步骤： 当一个程序正在执行的过程中， 中断（interrupt） 或 系统调用（system call） 发生可以使得 CPU 的控制权会从当前进程转移到操作系统内核。 操作系统内核负责保存进程 i 在 CPU 中的上下文（程序计数器， 寄存器）到 PCB 中。 从 PCB 取出进程 j 的CPU 上下文， 将 CPU 控制权转移给进程 j ， 开始执行进程 j 的指令。 几个底层概念的通俗（不严谨）解释： 中断（interrupt） CPU 微处理器有一个中断信号位， 在每个CPU时钟周期的末尾, CPU会去检测那个中断信号位是否有中断信号到达， 如果有， 则会根据中断优先级决定是否要暂停当前执行的指令， 转而去执行处理中断的指令。 （其实就是 CPU 层级的 while 轮询） 时钟中断( Clock Interrupt ) 一个硬件时钟会每隔一段（很短）的时间就产生一个中断信号发送给 CPU，CPU 在响应这个中断时， 就会去执行操作系统内核的指令， 继而将 CPU 的控制权转移给了操作系统内核， 可以由操作系统内核决定下一个要被执行的指令。 系统调用（system call） system call 是操作系统提供给应用程序的接口。 用户通过调用 systemcall 来完成那些需要操作系统内核进行的操作， 例如硬盘， 网络接口设备的读写等。 从上述描述中， 可以看出来， 操作系统在进行进切换时，需要进行一系列的内存读写操作， 这带来了一定的开销： 对于一个运行着 UNIX 系统的现代 PC 来说， 进程切换至少需要花费 300 us 的时间 进程阻塞 上图展示了一个进程的不同状态： New。 进程正在被创建. Running. 进程的指令正在被执行 Waiting. 进程正在等待一些事件的发生（例如 I/O 的完成或者收到某个信号）。 Ready. 进程在等待被操作系统调度。 Terminated. 进程执行完毕（可能是被强行终止的）。 我们所说的 “阻塞”是指进程在发起了一个系统调用（System Call） 后， 由于该系统调用的操作不能立即完成，需要等待一段时间，于是内核将进程挂起为等待 （waiting）状态， 以确保它不会被调度执行， 占用 CPU 资源。 友情提示： 在任意时刻， 一个 CPU 核心上（processor）只可能运行一个进程 。 2.2.2 I/O System Call 这里再重新审视 阻塞/非阻塞 IO 这个概念， 其实阻塞和非阻塞描述的是进程的一个操作是否会使得进程转变为“等待”的状态， 但是为什么我们总是把它和 IO 连在一起讨论呢？ 原因是， 阻塞这个词是与系统调用 System Call 紧紧联系在一起的， 因为要让一个进程进入 等待（waiting） 的状态, 要么是它主动调用 wait() 或 sleep() 等挂起自己的操作， 另一种就是它调用 System Call, 而 System Call 因为涉及到了 I/O 操作， 不能立即完成， 于是内核就会先将该进程置为等待状态， 调度其他进程的运行， 等到 它所请求的 I/O 操作完成了以后， 再将其状态更改回 ready 。 操作系统内核在执行 System Call 时， CPU 需要与 IO 设备完成一系列物理通信上的交互， 其实再一次会涉及到阻塞和非阻塞的问题， 例如， 操作系统发起了一个读硬盘的请求后， 其实是向硬盘设备通过总线发出了一个请求，它即可以阻塞式地等待IO 设备的返回结果，也可以非阻塞式的继续其他的操作。 在现代计算机中，这些物理通信操作基本都是异步完成的， 即发出请求后， 等待 I/O 设备的中断信号后， 再来读取相应的设备缓冲区。 但是，大部分操作系统默认为用户级应用程序提供的都是阻塞式的系统调用 （blocking systemcall）接口， 因为阻塞式的调用，使得应用级代码的编写更容易（代码的执!行顺序和编写顺序是一致的）。 但同样， 现在的大部分操作系统也会提供非阻塞I/O 系统调用接口（Nonblocking I/O system call）。 一个非阻塞调用不会挂起调用程序， 而是会立即返回一个值， 表示有多少bytes 的数据被成功读取（或写入）。 非阻塞I/O 系统调用( nonblocking system call )的另一个替代品是 异步I/O系统调用 （asychronous system call）。 与非阻塞 I/O 系统调用类似，asychronous system call 也是会立即返回， 不会等待 I/O 操作的完成， 应用程序可以继续执行其他的操作， 等到 I/O 操作完成了以后，操作系统会通知调用进程（设置一个用户空间特殊的变量值 或者 触发一个 signal 或者 产生一个软中断 或者 调用应用程序的回调函数）。 此处， 非阻塞I/O 系统调用( nonblocking system call ) 和 异步I/O系统调用 （asychronous system call）的区别是： 一个非阻塞I/O 系统调用 read() 操作立即返回的是任何可以立即拿到的数据， 可以是完整的结果， 也可以是不完整的结果， 还可以是一个空值。 而异步I/O系统调用 read（）结果必须是完整的， 但是这个操作完成的通知可以延迟到将来的一个时间点。 下图展示了同步I/O 与 异步 I/O 的区别 （非阻塞 IO 在下图中没有绘出）. 注意， 上面提到的 非阻塞I/O 系统调用( nonblocking system call ) 和 异步I/O系统调用 都是非阻塞式的行为（non-blocking behavior）。 他们的差异仅仅是返回结果的方式和内容不同。 0.3 总结 同步(阻塞),异步(非阻塞)在进程级层面是一对同义词 但是在system Call 层面,有所区别 1 socket入门 1.1 socket简介 Socket是应用层与TCP/IP协议族通信的中间软件抽象层，它是一组接口。在设计模式中，Socket其实就是一个门面模式，它把复杂的TCP/IP协议族隐藏在Socket接口后面，对用户来说，一组简单的接口就是全部，让Socket去组织数据，以符合指定的协议。 你会使用它们吗？ 前人已经给我们做了好多的事了，网络间的通信也就简单了许多，但毕竟还是有挺多工作要做的。以前听到Socket编程，觉得它是比较高深的编程知识，但是只要弄清Socket编程的工作原理，神秘的面纱也就揭开了。 一个生活中的场景。你要打电话给一个朋友，先拨号，朋友听到电话铃声后提起电话，这时你和你的朋友就建立起了连接，就可以讲话了。等交流结束，挂断电话结束此次交谈。 生活中的场景就解释了这工作原理，也许TCP/IP协议族就是诞生于生活中，这也不一定。 图3 ​ 先从服务器端说起。服务器端先初始化Socket，然后与端口绑定(bind)，对端口进行监听(listen)，调用accept阻塞，等待客户端连接。在这时如果有个客户端初始化一个Socket，然后连接服务器(connect)，如果连接成功，这时客户端与服务器端的连接就建立了。客户端发送数据请求，服务器端接收请求并处理请求，然后把回应数据发送给客户端，客户端读取数据，最后关闭连接，一次交互结束。 1.2 socket基本操作 bind linsten,connect accept read,write close 1.3 socket和tcp的关系 1.3.1、socket中TCP的三次握手建立连接详解 我们知道tcp建立连接要进行“三次握手”，即交换三个分组。大致流程如下： 客户端向服务器发送一个SYN J 服务器向客户端响应一个SYN K，并对SYN J进行确认ACK J+1 客户端再想服务器发一个确认ACK K+1 只有就完了三次握手，但是这个三次握手发生在socket的那几个函数中呢？请看下图： [ 图1、socket中发送的TCP三次握手 从图中可以看出，当客户端调用connect时，触发了连接请求，向服务器发送了SYN J包，这时connect进入阻塞状态；服务器监听到连接请求，即收到SYN J包，调用accept函数接收请求向客户端发送SYN K ，ACK J+1，这时accept进入阻塞状态；客户端收到服务器的SYN K ，ACK J+1之后，这时connect返回，并对SYN K进行确认；服务器收到ACK K+1时，accept返回，至此三次握手完毕，连接建立。 总结：客户端的connect在三次握手的第二个次返回，而服务器端的accept在三次握手的第三次返回。 1.3.2、socket中TCP的四次握手释放连接详解 上面介绍了socket中TCP的三次握手建立过程，及其涉及的socket函数。现在我们介绍socket中的四次握手释放连接的过程，请看下图： [ 图2、socket中发送的TCP四次握手 图示过程如下： 某个应用进程首先调用close主动关闭连接，这时TCP发送一个FIN M； 另一端接收到FIN M之后，执行被动关闭，对这个FIN进行确认。它的接收也作为文件结束符传递给应用进程，因为FIN的接收意味着应用进程在相应的连接上再也接收不到额外数据； 一段时间之后，接收到文件结束符的应用进程调用close关闭它的socket。这导致它的TCP也发送一个FIN N； 接收到这个FIN的源发送端TCP对它进行确认。 这样每个方向上都有一个FIN和ACK。 2 五大io 模型 1.阻塞IO模型 　　最传统的一种IO模型，即在读写数据过程中会发生阻塞现象。 　　当用户线程发出IO请求之后，内核会去查看数据是否就绪，如果没有就绪就会等待数据就绪，而用户线程就会处于阻塞状态，用户线程交出CPU。当数据就绪之后，内核会将数据拷贝到用户线程，并返回结果给用户线程，用户线程才解除block状态。 典型的阻塞IO模型的例子为：data = socket.read(); 如果数据没有就绪，就会一直阻塞在read方法。 2.非阻塞IO模型 　　当用户线程发起一个read操作后，并不需要等待，而是马上就得到了一个结果。如果结果是一个error时，它就知道数据还没有准备好，于是它可以再次发送read操作。一旦内核中的数据准备好了，并且又再次收到了用户线程的请求，那么它马上就将数据拷贝到了用户线程，然后返回。 　　所以事实上，在非阻塞IO模型中，用户线程需要不断地询问内核数据是否就绪，也就说非阻塞IO不会交出CPU，而会一直占用CPU。典型的非阻塞IO模型一般如下： while(true){ data = socket.read(); if(data!= error){ ​ 处理数据 ​ break; } } 　　但是对于非阻塞IO就有一个非常严重的问题，在while循环中需要不断地去询问内核数据是否就绪，这样会导致CPU占用率非常高，因此一般情况下很少使用while循环这种方式来读取数据。 3.多路复用IO模型 　　多路复用IO模型是目前使用得比较多的模型。Java NIO实际上就是多路复用IO。 　　在多路复用IO模型中，会有一个线程不断去轮询多个socket的状态，只有当socket真正有读写事件时，才真正调用实际的IO读写操作。因为在多路复用IO模型中，只需要使用一个线程就可以管理多个socket，系统不需要建立新的进程或者线程，也不必维护这些线程和进程，并且只有在真正有socket读写事件进行时，才会使用IO资源，所以它大大减少了资源占用。 　　在Java NIO中，是通过selector.select()去查询每个通道是否有到达事件，如果没有事件，则一直阻塞在那里，因此这种方式会导致用户线程的阻塞。 　　也许有朋友会说，我可以采用多线程+ 阻塞IO 达到类似的效果，但是由于在多线程 + 阻塞IO 中，每个socket对应一个线程，这样会造成很大的资源占用，并且尤其是对于长连接来说，线程的资源一直不会释放，如果后面陆续有很多连接的话，就会造成性能上的瓶颈。 　　而多路复用IO模式，通过一个线程就可以管理多个socket，只有当socket真正有读写事件发生才会占用资源来进行实际的读写操作。因此，多路复用IO比较适合连接数比较多的情况。 　　另外多路复用IO为何比非阻塞IO模型的效率高是因为在非阻塞IO中，不断地询问socket状态时通过用户线程去进行的，而在多路复用IO中，轮询每个socket状态是内核在进行的，这个效率要比用户线程要高的多。 　　不过要注意的是，多路复用IO模型是通过轮询的方式来检测是否有事件到达，并且对到达的事件逐一进行响应。因此对于多路复用IO模型来说，一旦事件响应体很大，那么就会导致后续的事件迟迟得不到处理，并且会影响新的事件轮询。 4.信号驱动IO模型 　　在信号驱动IO模型中，当用户线程发起一个IO请求操作，会给对应的socket注册一个信号函数，然后用户线程会继续执行，当内核数据就绪时会发送一个信号给用户线程，用户线程接收到信号之后，便在信号函数中调用IO读写操作来进行实际的IO请求操作。这个一般用于UDP中，对TCP套接口几乎是没用的，原因是该信号产生得过于频繁，并且该信号的出现并没有告诉我们发生了什么事情 5.异步IO模型 　　异步IO模型才是最理想的IO模型，在异步IO模型中，当用户线程发起read操作之后，立刻就可以开始去做其它的事。而另一方面，从内核的角度，当它受到一个asynchronous read之后，它会立刻返回，说明read请求已经成功发起了，因此不会对用户线程产生任何block。然后，内核会等待数据准备完成，然后将数据拷贝到用户线程，当这一切都完成之后，内核会给用户线程发送一个信号，告诉它read操作完成了。也就说用户线程完全不需要关心实际的整个IO操作是如何进行的，只需要先发起一个请求，当接收内核返回的成功信号时表示IO操作已经完成，可以直接去使用数据了。 　　也就说在异步IO模型中，IO操作的两个阶段都不会阻塞用户线程，这两个阶段都是由内核自动完成，然后发送一个信号告知用户线程操作已完成。用户线程中不需要再次调用IO函数进行具体的读写。这点是和信号驱动模型有所不同的，在信号驱动模型中，当用户线程接收到信号表示数据已经就绪，然后需要用户线程调用IO函数进行实际的读写操作；而在异步IO模型中，收到信号表示IO操作已经完成，不需要再在用户线程中调用iO函数进行实际的读写操作。 　　注意，异步IO是需要操作系统的底层支持，在Java 7中，提供了Asynchronous IO。简称AIO 前面四种IO模型实际上都属于同步IO，只有最后一种是真正的异步IO，因为无论是多路复用IO还是信号驱动模型，IO操作的第2个阶段都会引起用户线程阻塞，也就是内核进行数据拷贝的过程都会让用户线程阻塞。 两种高性能IO设计模式 在传统的网络服务设计模式中，有两种比较经典的模式： 　　一种是多线程，一种是线程池。 　　对于多线程模式，也就说来了client，服务器就会新建一个线程来处理该client的读写事件，如下图所示： 这种模式虽然处理起来简单方便，但是由于服务器为每个client的连接都采用一个线程去处理，使得资源占用非常大。因此，当连接数量达到上限时，再有用户请求连接，直接会导致资源瓶颈，严重的可能会直接导致服务器崩溃。 　　因此，为了解决这种一个线程对应一个客户端模式带来的问题，提出了采用线程池的方式，也就说创建一个固定大小的线程池，来一个客户端，就从线程池取一个空闲线程来处理，当客户端处理完读写操作之后，就交出对线程的占用。因此这样就避免为每一个客户端都要创建线程带来的资源浪费，使得线程可以重用。 　　但是线程池也有它的弊端，如果连接大多是长连接，因此可能会导致在一段时间内，线程池中的线程都被占用，那么当再有用户请求连接时，由于没有可用的空闲线程来处理，就会导致客户端连接失败，从而影响用户体验。因此，线程池比较适合大量的短连接应用。 　　因此便出现了下面的两种高性能IO设计模式：Reactor和Proactor。 在Reactor模式中，会先对每个client注册感兴趣的事件，然后有一个线程专门去轮询每个client是否有事件发生，当有事件发生时，便顺序处理每个事件，当所有事件处理完之后，便再转去继续轮询，如下图所示： 从这里可以看出，上面的五种IO模型中的多路复用IO就是采用Reactor模式。注意，上面的图中展示的 是顺序处理每个事件，当然为了提高事件处理速度，可以通过多线程或者线程池的方式来处理事件。Java NIO使用的就是这种 　　在Proactor模式中，当检测到有事件发生时，会新起一个异步操作，然后交由内核线程去处理，当内核线程完成IO操作之后，发送一个通知告知操作已完成，可以得知，异步IO模型采用的就是Proactor模式。Java AIO使用的这种 3 多路复用io模型详解 3.0 基础 基础 在讲解之前，我们需要先知道计算机是如何接受网络数据的。简单来讲，就是当网络数据到达网卡的时候，网卡会通过中断控制器向CPU发送中断信号，CPU接受到中断信号的时候会根据接受到的中断向量号调用提前在中段描述符表中注册好的中断处理程序，中断处理程序会保存当前正在执行的程序的上下文，然后将网卡的中的数据复制到内核缓冲区，在等待空闲的时间将内核缓冲区的数据复制到用户缓冲区中供用户进程处理。 而我们知道，对于服务端建立socket的过程如下： //创建socket int s = socket(AF_INET, SOCK_STREAM, 0); //绑定 bind(s, ...) //监听 listen(s, ...) //接受客户端连接 int c = accept(s, ...) //接收客户端数据 recv(c, ...); //将数据打印出来 printf(...) 当和客户端成功完成3次握手后，便会调用recv阻塞等待接收客户端发送过来的数据。 实际上，当某个进程A执行到创建socket语句的时候， 操作系统就会创建一个由文件系统管理的socket对象。socket包含了发送缓冲区、接收缓冲区、等待队列等。其中发送缓冲区和接收缓冲区就是我们TCP流量控制中用到的滑动窗口，而TCP本身就是全双工的，既可以发送数据，又可以接收数据，因此会有2个缓冲区。而等待队列指向的是所有等待该socket事件的进程。 image.png 当程序执行到recv的时候，操作系统会将进程A从工作队列移动到该socket的等待队列中（传个引用），进程B、C继续调度执行，A进程被阻塞 image.png 当socket接收到数据，操作系统便会将在该socket的等待队列上的进程重新放回工作队列中，由内核调度器继续调度执行，该进程变成运行状态，继续执行代码，由于socket的接收缓冲区已有了数据，recv便可接收返回的数据。 那么当数据到来的时候，操作系统如何知道是哪一个socket呢？一个进程又是如何监听多个socket的数据呢？ 因为一条TCP连接对应一个四元组，TCP首部的信息中含有接收方的端口号信息，而一个socket对应着一个端口号，因此可以根据TCP头部的信息找到对应的socket，将数据复制到socket的接收缓冲区中；而进程正是通过IO多路复用的形式来监听多个socket（select和epoll）。 3.1 select 如下的代码中，准备一个数组fds，存放需要监视的所有socket，然后调用select，如果fds中所有的socket都没有数据，select会阻塞，直到有一个socket收到数据，select返回，唤醒进程，用户可以遍历fds，通过FD_ISSET判断哪个socket收到了数据，然后做出处理。 int s = socket(AF_INET, SOCK_STREAM, 0); bind(s, ...) listen(s, ...) int fds[] = 存放需要监听的socket while(1){ int n = select(..., fds, ...) for(int i=0; i 加入进程A同时监视如下图的sock1、sock2、sock3，调用select后操作系统会将进程A分别加入这3个socket的等待队列中。 当任何一个socket收到数据后，中断处理程序唤醒线程，加入工作队列 然后A继续执行，只需遍历一遍socket列表，就可以得到就绪的socket。 但是简单的方法往往有缺点，主要是： 其一，每次调用select都需要将进程加入到所有监视socket的等待队列，每次唤醒都需要从每个队列中移除。这里涉及了两次遍历，而且每次都要将整个fds列表传递给内核，有一定的开销。正是因为遍历操作开销大，出于效率的考量，才会规定select的最大监视数量，默认只能监视1024个socket。 其二，进程被唤醒后，程序并不知道哪些socket收到数据，还需要遍历一次。 那么，有没有减少遍历的方法？有没有保存就绪socket的方法？这两个问题便是epoll技术要解决的。 补充说明： 本节只解释了select的一种情形。当程序调用select时，内核会先遍历一遍socket，如果有一个以上的socket接收缓冲区有数据，那么select直接返回，不会阻塞。这也是为什么select的返回值有可能大于1的原因之一。如果没有socket有数据，进程才会阻塞。 3.2 EPOLL 如下图所示，当某个进程调用epoll_create方法时，内核会创建一个eventpoll对象（也就是程序中epfd所代表的对象）。eventpoll对象也是文件系统中的一员，和socket一样，它也会有等待队列。 创建eventpoll对象后，可以通过epoll_ctl添加或者删除需要监听的socket。以添加socket为例，如下图，如果通过epoll_ctl添加sock1、sock2和sock3的监视，内核会将eventpoll添加到这三个socket的等待队列中。 image.png 当socket收到数据后，中断程序会操作eventpoll对象，而不是直接操作进程。 当socket收到数据后，中断程序会给eventpoll的“就绪列表”添加socket引用。如下图展示的是sock2和sock3收到数据后，中断程序让rdlist引用这两个socket。 所以eventpoll相当于一个中间层，位于socket和进程之间，socket的数据通过改变eventpoll的就序列表来改变进程状态。 假设计算机中正在运行进程A和进程B，在某时刻进程A运行到了epoll_wait语句。如下图所示，内核会将进程A放入eventpoll的等待队列中，阻塞进程。 image.png 当socket接收到数据，中断程序一方面修改rdlist，另一方面唤醒eventpoll等待队列中的进程，进程A再次进入运行状态（如下图）。也因为rdlist的存在，进程A可以知道哪些socket发生了变化。 image.png 3.3 poll 和epoll没有本质上的区别,就是没有socket数量的限制(用链表存储) "},"计算机网络/3.数据链路层.html":{"url":"计算机网络/3.数据链路层.html","title":"3.数据链路层","keywords":"","body":"一 定义(一) 基本概念(二) 功能描述二 组帧(一) 封装,定界,同步(二) 组帧的方法三 差错控制(一) 冗余编码(二) 检错编码(三) 纠错编码(海明码)四 流量控制 ⭐(一) 相关定义(二) 停止-等待协议(三)多帧滑动窗口与后退N帧协议(GBN)(四)多帧滑动窗口与选择重传协议（SR）五 数据链路层设备网卡网桥交换机一 定义 (一) 基本概念 结点: 主机,路由器 链路: 网络中两个节点的物理通道,链路的传输介质主要有双绞线,光纤等,为物理层的概念 数据链路: 网络中两个节点之间的逻辑通道, 把实现控制数据传输协议的硬件和软件加到链路上就构成了数据链路,为数据链路层的概念 帧: 数据链路层将网络层的分组封装成帧进行传输 (二) 功能描述 数据链路层在物理层的基础上向网络层提供服务,其主要作用是加强物理层的比特流传输,将物理层提供的可能出错的物理连接,改造为 逻辑上无差错的数据链路,使之对网络层表现为一条无差错的链路.其最基本的服务是将源自网络层的数据可靠的传输到相临节点的目标机网络层. 为网络层提供服务 (无确认的无连接服务,有确认无连接服务,有确认面向连接服务) 链路管理 (即连接的建立,维持,释放) 组帧 流量控制 差错控制 二 组帧 (一) 封装,定界,同步 封装成帧 指将来自网络层的数据,在其前后分别添加首部和尾部,以构成帧(称为组帧) 首部和尾部包含许多控制信息,他们的的一个重要作用是确定帧的界限(帧定界) 帧同步 接收方应当能从接受到的二进制比特流中区分出帧的起始和终止 透明传输 指不管所传输的数据是什么样的比特组合,都应该能在链路上传输. 也即,当所传输的比特组合恰巧与某一个控制信息完全一样时,就必须采取何时的措施,使接收方不会将这样的数据 误以为使某种控制信息.这样才能保证数据链路层的传输是透明的 (二) 组帧的方法 字符计数法 字符填充的首尾定界法 发送方在封装帧时，数据的可能有图中两种类型，传输数据时可能会出现图中所述的错误 那么我们如何去解决这种错误呢？ 我们可以在特殊字符（SOH、EOT、ESC）前面填充一个转义字符来区分 发送方在封装帧时，进行扫描，扫描到SOH、EOT、ESC（转义字符）时在其前面添加转义字符，以区分，告诉接受方这个和特殊字符相同的字符是数据，当然这些约定由双方之间的协议完成 零比特填充的首尾标志法 以01111110为头尾界限,为了不使出现信息位中出现的比特流0111110被误判位帧的首位标志,发送端在信息位中连续遇到5个1时,就 在其后插入一个0,想反,接收方遇到5个1,去除一个0,以恢复原信息 违规编码法 该编码将1编码位'高-低',将0编码位'低-高',而'低-低','高-高'为违规编码,因此利用该特性,作为界限 三 差错控制 实际的通信链路都不是理想的,比特在传输过程可能会产生差错,1变成0,0变成1.这就是比特差错 通常利用编码技术进行差错控制,主要有两类: 检错编码: 接收方发现出错,通知发送方重传 纠错编码: 接收方不仅能发现出错,还能纠错 (一) 冗余编码 数据链路层的编码针对的是一组比特,他通过冗余码的技术实现一组二进制比特在传输过程中是否出现了差错 冗余编码: 在数据发出前,先按照某种关系附加上一定的冗余位,构成一个符合某一规则的码字后再发出 当发送的有效数据编码时,相应的冗余位也会随之变化,接收方可根据码字是否符合规则,从而判断是否出错 (二) 检错编码 纠错编码主要借助冗余编码技术 奇偶校验码 循环冗余码（CRC） 原码如何编码生成CRC码？ 首先，发送端和接受端会有一个生成多项式G(x)约定,生成多项式G(x)的最高次幂为R。任意一个二进制数码都可用一个系数为0或1的多项式与之对应。比如：二进制数码 1101 对应的G(x)=1X3+1X2+0X1+1X0= X3+X2+1 在发送端，将要传送的K位二进制信息码左移R位，将它与生成多项式G(x)所对应的的二进制数码进行模2除法，产生余数，生成一个R位检验码，并附在信息码后，构成一个新的二进制码（CRC）码，共K+R位。 模2运算：分为模2加、模2减、模2乘、模2除，不考虑进位和借位。 例题： 设生成多项式G(x)=X3+X2+1 ,信息码为 101001 ，求对应的CRC码 。 分析：校验位长度：R=3 , 信息码长度：K=6 , CRC码长度：N=R+K= 9 生成多项式对应二进制码：1101 （1）信息码左移R位 发送端将原信息码左移R位，低位补0：101001 000 （2）模2除法得余数 方法：发送端用移位后的信息码 101001000 除以G(x)所对应的二进制数码 1101 求余数，余数除得够就写1，不够就写0，直到余数小于 1101 ，余数即为校验位的数值。 图中即为具体计算步骤，得到最后的结果CRC码为：101001 001，然后发送端将CRC码101001 001发送给接收端。 （3）如何检错和纠正错误？ 接收端收到CRC码后，用生成的CRC码除以生成多项式G(x)所对应的的二进制数码，若余数为0，则信息码在传输过程中没有产生错误，数据正确。 若接受到的CRC码为C9C8C7C6C5C4C3C2C1= 101001011,除以G(x)所对应的二进制码1101得到余数为010，不为0，说明数据在传输过程中产生错误。010=2（10）说明C2出错，将C2取反即可纠正错误。 (三) 纠错编码(海明码) 关于海明码这里有篇文章简单易懂: https://www.cnblogs.com/lesroad/p/8688634.html 四 流量控制 ⭐ (一) 相关定义 流量控制 流量控制是数据链路层的一种功能，流量控制对数据链路上的帧的发送速率进行控制，以使接收方有足够的缓冲空间来接受每个帧 流量控制的基本方法是由接收方控制发送方发送数据的速率 常见的流量控制方式有两种：停止-等待协议、滑动窗口协议 可靠传输机制 可靠传输机制是为了使数据可以正确稳定的传输和接收而制定的规则。 数据链路层的可靠传输通常使用确认和超时重传两种机制来完成。 确认是一种无数据的控制帧，这种控制帧使得接收方可以让发送方知道哪些内容被正确接收。有些情况下为了提高传输效率，将确认捎带在一个回复帧中，称为捎带确认。 超时重传是指发送方在发送某一个数据帧以后就开始一个计时器，在一定时间内如果没有得到发送的数据帧的确认帧，那么就重新发送该数据帧，直到发送成功为止。 自动重传请求（Auto Repeat reQuest，ARQ），通过接收方请求发送方重传出错的数据帧来恢复出错的帧，是通信中用于处理信道所带来差错的方法之一。 传统自动重传请求分为三种，即停等式（Stop-and-Wait）ARQ、后退N帧（Go-Back-N）ARQ以及选择性重传（Selective Repeat）ARQ。后两种协议是滑动窗口技术与请求重发技术的结合，由于窗口尺寸开到足够大，帧在线路上可以连续地流动，因此又称为连续ARQ协议。 滑动窗口机制 滑动窗口协议的基本原理就是在任意时刻，发送方都维持了一个连续的允许发送的帧的序号，称为发送窗口；同时，接收方也维持了一个连续的允许接收的帧的序号，称为接收窗口。 发送窗口和接收窗口的序号的上下界不一定要一样，甚至大小也可以不同。 不同的滑动窗口协议窗口大小一般不同。 发送方窗口内的序列号代表了那些已经被发送，但是还没有被确认的帧，或者是那些可以被发送的帧。 在发送端，每收到一个确认帧，发送窗口就向前滑动一个帧的位置，当发送窗口内没有可以发送的帧（即窗口内的帧全部是已发送但未收到确认的帧），发送方就会停止发送，直到收到接受方发送的确认帧使窗口移动，窗口内有可以发送的帧，之后才开始继续发送。 在接受端，当收到数据帧后，将窗口向前移一个位置，并发回确认帧，若收到的数据帧落在接受窗口之外则一律丢弃。 滑动窗口有以下重要特性： 只有接受窗口向前滑动时（同时接受方发送确认帧），发送窗口才有可能（只有发送方收到确认帧才是一定）向前滑动。 从滑动窗口的概念看，停止-等待协议、后退N帧协议和选择重传协议只有在发送窗口大小和接收窗口大小有所差别。 停止-等待协议：发送窗口大小=1，接受窗口大小=1； 后退N帧协议：发送窗口大小>1，接受窗口大小=1； 选择重传协议：发送窗口大小>1，接受窗口大小>1； 当接受窗口的大小为1时，可保证帧的有序接受。 三者关系 可靠传输: 传输的稳定和正确 流量控制: 传输的速率 二者可用滑动窗口来解决 (二) 停止-等待协议 停止-等待协议也称为单帧滑动窗口与停止-等待协议 当发送窗口和接收窗口的大小固定为1时，滑动窗口协议退化为停等协议（stop-and-wait）。 该协议规定发送方每发送一帧后就要停下来，等待接收方已正确接收的确认（acknowledgement）返回后才能继续发送下一帧。 由于接收方需要判断接收到的帧是新发的帧还是重新发送的帧，因此发送方要为每一个帧加一个序号。 由于停等协议规定只有一帧完全发送成功后才能发送新的帧，因而只用一比特来编号就够了。 1.无差错情况 2.有差错情况 数据帧丢失或检测到帧出错 发送方在发送一帧后,会在本地缓存他的副本,若出现差错,可进行重传 ACK确认帧丢失 ACK确认帧迟到超时 3.性能分析 由于停等协议要为每一个帧进行确认后才继续发送下一帧，大大降低了信道利用率，因此又提出了后退n帧协议（GBN）和选择重传协议（SR）。 (三)多帧滑动窗口与后退N帧协议(GBN) 后退n协议中，发送方在发完一个数据帧后，不停下来等待应答帧，而是连续发送若干个数据帧，即使在连续发送过程中收到了接收方发来的应答帧，也可以继续发送。且发送方在每发送完一个数据帧时都要设置超时定时器。只要在所设置的超时时间内仍未收到确认帧，就要重发相应的数据帧。 如图所示,源站向目的站发送数据帧.当发送完0帧后,可用继续发送1,2帧.由于连续发送了多帧,所以确认帧必须要指明是对哪一帧进行确认. 为了减少开销,GBN协议还规定接收端不一定每收到一个真确帧就返回一个确认帧,而是可用在连续收到好几个正确的确认帧后,才对最后一个数据帧发送确认信息. 这就是说,对第N帧的确认就代表从当前N到之前的所有帧都成功接收了,且希望下一次接收N+1 帧 如下图,虽然有在有差错的第2帧后又接收到了正确的5个帧,但接收方必须抛弃这些帧,并重复发送一个确认帧ACK1,请求发送发重发1之后的帧 而接收方发现帧2超时,也即久久没有收到ACK2,就会重新发送帧2及其之后的帧 从这里不难看出，后退n协议一方面因连续发送数据帧而提高了效率，但另一方面，在重传时又必须把原来已正确传送过的数据帧进行重传（仅因这些数据帧之前有一个数据帧出了错），这种做法又使传送效率降低。 由此可见，若传输信道的传输质量很差因而误码率较大时，连续测协议不一定优于停止等待协议。 注意,根据累计确认机制,发送方的发送帧号等于接收方最大ACK数+1,如: 因为接收端可以累积确认，所以只要看最大的确认帧就行，所以接下来发送方要重发的帧数为4 (四)多帧滑动窗口与选择重传协议（SR） 在后退n协议中，接收方若发现错误帧就不再接收后续的帧，即使是正确到达的帧，这显然是一种浪费。由此诞生了SR（SELECTICE REPEAT）。 SR工作原理：当接收方发现某帧出错后，其后继续送来的正确的帧虽然不能立即递交给接收方的高层，但接收方仍可收下来，存放在一个缓冲区中，同时要求发送方重新传送出错的那一帧。一旦收到重新传来的帧后，就可以将已存于缓冲区中的其余帧一并按正确的顺序递交上一层。 显然，选择重发减少了浪费，但要求接收方有足够大的缓冲区空间。 五 数据链路层设备 网卡 　　网卡是局域网中提供各种网络设备与网络通信介质相连的接口，全名是网络接口卡，也叫网络适配器，其品种和质量的好坏直接影响网络的性能和网上所运行软件的效果。网卡作为一种I/O接口卡插在主机板的拓展槽上，其基本结构包括接口控制电路、数据缓冲器、数据链路控制、编码解码电路、内收发器、介质接口装置等6大部分。 　　网卡用于物理层和数据链路层。涉及帧的发送与接收、帧的封装与拆分、数据的编码与解码等功能。 　　 　　网卡MAC地址 　　每一网卡在出厂时都被分配了一个全球唯一的地址标识，该标识被称为网卡地址或媒体访问控制地址，由于该地址是固化在网卡上的，所以又被称为物理地址或硬件地址。网卡由48bit长度的二进制数组成。 网桥 　　网桥（Bridge）是早期的两端口二层网络设备，用来连接不同网段。网桥的两个端口分别有一条独立的交换信道，不是共享一条背板总线，可隔离冲突域。网桥比集线器（Hub）性能更好，集线器上各端口都是共享同一条背板总线的。后来，网桥被具有更多端口、同时也可隔离冲突域的交换机（Switch）所取代。 　　（1）网桥的基本特征 1．网桥在数据链路层上实现局域网互连； 2．网桥能够互连两个采用不同数据链路层协议、不同传输介质与不同传输速率的网络 3．网桥以接收、存储、地址过滤与转发的方式实现互连的网络之间的通信； 4．网桥需要互连的网络在数据链路层以上采用相同的协议 5．网桥可以分隔两个网络之间的通信量，有利于改善互连网络的性能与安全性。 　　（2）网桥原理 　　网桥将两个相似的网络连接起来，并对网络数据的流通进行管理。它工作于数据链路层，不但能扩展网络的距离或范围，而且可提高网络的性能、可靠性和安全性。网络1和网络2通过网桥连接后，网桥接收网络1发送的数据包，检查数据包中的地址，如果地址属于网络1，它就将其放弃，相反，如果是网络2的地址，它就继续发送给网络2.这样可利用网桥隔离信息，将同一个网络号划分成多个网段（属于同一个网络号），隔离出安全网段，防止其他网段内的用户非法访问。由于网络的分段，各网段相对独立（属于同一个网络号），一个网段的故障不会影响到另一个网段的运行。 　　网桥可以是专门硬件设备，也可以由计算机加装的网桥软件来实现，这时计算机上会安装多个网络适配器（网卡）。 　　网桥的功能在延长网络跨度上类似于中继器，然而它能提供智能化连接服务，即根据帧的终点地址处于哪一网段来进行转发和滤除。网桥对站点所处网段的了解是靠“自学习”实现的，有透明网桥、转换网桥、封装网桥、源路由选择网桥。 交换机 　　交换机也是工作在数据链路层的网络互连设备，是局域网组网中最常用也是最主要的网络设备之一。交换机的种类很多，如以太网交换机、FDDI交换机、帧中继交换机、ATM交换机和令牌交换机等。 　　（1）交换机的功能 　　所有交换机的基本功能都是相同的，即接收帧、寻找通向目的地址的端口、发送帧。 　　（2）交换机的工作方式 　　一些交换机在发送帧前，可以帮助网络检查更多的帧信息，而不仅仅是检查源地址和目的地址。正是基于这些区别，交换机有4种方式。 1）直通交换 2）无碎片帧交换 3）存储转发交换 自适应交换 "},"计算机网络/4.网络层1.html":{"url":"计算机网络/4.网络层1.html","title":"4.网络层(一)","keywords":"","body":"一 网络层的功能1.主要任务2.异构网络互联3.路由选择与转发4.拥塞控制二 数据交换1.电路交换2.报文交换3.分组交换三 路由算法1.静态路由与动态路由2.层次路由四 IPV4 ⭐1.TCP/IP协议栈2.IPv4分组3.网络层转发分组的流程4.IPv4与NAT5 子网划分与子网掩码6.地址解析协议 —ARP7.动态主机配置协议—DHCP8.网际控制报文协议— ICMP一 网络层的功能 1.主要任务 将分组从源端传到目的端,为分组交换网上的不同主机提供通信服务. 网络层的传输单位是数据报 2.异构网络互联 所谓异构网络互联，是指将两个以上的不同的计算机网络，通过一定的方法， 用一种或多种通信处理设备(即中间设备)相互连接起来，以构成更大的网络系统。中间设备又称中间系统或中继系统。 根据所在的层次，·中继系统分为以下4种: 1)物理层中继系统:中继器，集线器(Hub)。 2)数据链路层中继系统:网桥或交换机。 3)网络层中继系统:路由器。 4)网络层以上的中继系统:网关。 使用物理层或数据链路层的中继系统时,只是单纯吧一个网络扩大了,而从网络层的角度来看,它仍然是一个个独立的网络, 不能称之为网络互连.网络互联通常是指用路由器进行网络互联和路由选择.路由器是一台专门的计算机,用于在互联网中进行路由选择和转发 TCP/IP 体系再网络互联上采用的做法是再网络层(即IP层) 采用标准化的协议, 即使相互连接的网络是异构的,但由于都是用相同的网际协议(Internet Protocol,IP),因此可以把互联后的网络称之为一个虚拟IP网络 也即使这些性能各异的网络在网络层看起来就像是一个统一的网络,简称IP网络 3.路由选择与转发 路由器主要完成两个功能:是路由选择 (确定哪一 条路径)，二是分组转发 (当一个分组 到达时所采取的动作)。 1)路由选择。指按照复杂的分布式算法，根据从各相邻路由器所得到的关于整个网络拓扑 的变化情况，动态地改变所选择的路由。 2)分组转发。指路由器根据转发表将用户的IP数据报从合适的端口转发出去。 路由表是根据路由选择算法得出的，而转发表是从路由表得出的。 路由表则需要对网络拓扑变化的计算最优化,转发表的结构应当使查找过程最优化。 在讨论路由选择的原理时，往往不去区分转发表和路由表，而是笼统地使用路由表一词。 4.拥塞控制 在通信子网中，因出现过量的分组而引起网络性能下降的现象称为拥塞。 判断网络是否进入拥塞状态的方法是，观察网络的吞吐量与网络负载的关系:如果随着网络负载的增加，网络的吞吐量明显小于正常的吞吐量，那么网络就可能已进入“轻度拥塞”状态;如果网络的负载继续增大，而网络的吞吐量下降到零，那么网络就可能已进入死锁状态。 拥塞控制的作用是确保子网能够承载所达到的流量，这是一一个全局性的过程，涉及各方面的行为:主机、路由器及路由器内部的转发处理过程等。单一地增加资源并不能解决拥塞。 流量控制和拥塞控制的区别: ·流量控制往往是指在发送端和接收端之间的点对点通信量的控制。流量控制所要做的是抑制发送端发送数据的速率，以便使接收端来得及接收。 拥塞控制必须确保通信子网能够传送待传送的数据，是一一个全局性的问题，涉及网络中所有的主机、路由器及导致网络传输能力下降的所有因素。 拥塞控制的方法有两种: 1)·开环控制。在设计网络时事先将有关发生拥塞的因素考虑周到，力求网络在工作时不产生拥塞。这是一种静态的预防方法。一旦整个系统启动并运行，中途就不再需要修改。开环控制手段包括确定何时可接收新流量、何时可丢弃分组及丢弃哪些分组，确定何种调度决策等。所有这些手段的共性是，在做决定时不考虑当前网络的状态。 2)闭环控制。事先不考虑有关发生拥塞的各种因素，采用监测网络系统去监视，及时检测哪里发生了拥塞，然后将拥塞信息传到合适的地方，以便调整网络系统的运行，并解决出现的问题。闭环控制是基于反馈环路的概念，是一种动态的方法。 二 数据交换 应用层 报文 传输层 报文段/用户数据段 网络层 IP数据报/分组 数据链路层 帧 物理层 比特 1.电路交换 在进行数据传输前，两个结点之间必须先建立一条专用 (双方独占)的物理通信路径(由通信双方之间的交换设备和链路逐段连接而成)，该路径可能经过许多中间结点。这一路径在整个数据传输期间一直被独占，直到通信结束后才被释放。 因此，电路交换技术分为三个阶段:连接建立、数据传输和连接释放。 2.报文交换 数据交换的单位是报文，报文携带有源地址，目标地址，数据等信息。 报文交换的主要特点是：存储接受到的报文，判断其目标地址以选择路由，最后，在下一跳路由空闲时，将数据转发给下一跳路由。 中文名报文交换，外文名Message switching。 报文交换技术的优点如下: 1)无须建立连接。报文交换不需要为通信双方预先建立一条专用的通信线路，不存在建立 连接时延，用户可以随时发送报文。 2)动态分配线路。当发送方把报文交给交换设备时，交换设备先存储整个报文，然后选择 一条合适的空闲线路，将报文发送出去。 3)提高线路可靠性。如果某条传输路径发生故障，那么可重新选择另一条路径传输数据， 因此提高了传输的可靠性。 4)提高线路利用率。通信双方不是固定占有一条通信线路，而是在不同的时间一段一段地 部分占有这条物理通道，因而大大提高了通信线路的利用率。 5)提供多目标服务。一个报文可以同时发送给多个目的地址，这在电路交换中是很难实现的。 报文交换技术的缺点如下: 1)由于数据进入交换结点后要经历存储、转发这一过程，因此会引起转发时延(包括接收 报文、检验正确性、排队、发送时间等)。 2)报文交换对报文的大小没有限制，这就要求网络结点需要有较大的缓存空间。 注意:报文交换主要使用在早期的电报通信网中，现在较少使用，通常被较先进的分组交换 方式所取代。 3.分组交换 同报文交换一样，分组交换也采用存储转发方式，但解决了报文交换中大报文传输的问题。分组交换限制了每次传送的数据块大小的上限，把大的数据块划分为合理的小数据块，再加上一些必要的控制信息(如源地址、目的地址和编号信息等)，构成分组(Packet)。 网络结点根据控制信息把分组送到下一结点，下一结点接收到分组后，暂时保存并排队等待传输，然后根据分组控制信息选择它的下一个结点，直到到达目的结点。到达目地之后的数据分组再重新组合起来，形成一条完整的数据。 分组交换的优点如下: 1)无建立时延。不需要为通信双方预先建立一条专用的通信线路，不存在连接建立时延，用户可随时发送分组。 2)线路利用率高。通信双方不是固定占有- -条通信线路，而是在不同的时间一-段一段地部分占有这条物理通路，因而大大提高了通信线路的利用率。 3)简化了存储管理(相对于报文交换)。因为分组的长度固定，相应的缓冲区的大小也固定，在交换结点中存储器的管理通常被简化为对缓冲区的管理，相对比较容易。 4)加速传输。分组是逐个传输的，可以使后-一个分组的存储操作与前一一个分组的转发操作并行，这种流水线方式减少了报文的传输时间。此外，传输一个分组所 需的缓冲区比传输一次报文所需的缓冲区小得多，这样因缓冲区不足而等待发送的概率及时间也必然少得多。 5)减少了出错概率和重发数据量。因为分组较短，其出错概率必然减小，所以每次重发的数据量也就大大减少，这样不仅提高了可靠性，也减少了传输时延。 分组交换的缺点如下: 1)存在传输时延。尽管分组交换比报文交换的传输时延少，但相对于电路交换仍存在存储转发时延，而且其结点交换机必须具有更强的处理能力。 2)需要传输额外的信息量。每个小数据块都要加上源地址、目的地址和分组编号等信息，从而构成分组，因此使得传送的信息量增大了5%~10%，一定程度上降低了通信效率，增加了处理的时间，使控制复杂，时延增加。 3)当分组交换采用数据报服务时，可能会出现失序、丢失或重复分组，分组到达目的结点时，要对分组按编号进行排序等工作，因此很麻烦。若采用虚电路服务，虽无失序问题，但有呼叫建立、数据传输和虚电路释放三个过程。 分组交换根据其通信子网向端点系统提供的服务，还可进一步分为面向连接的虚电路方式和 无连接的数据报方式。这两种服务方式都由网络层提供。要注意数据报方式和虚电路方式是分组 交换的两种方式。 三 路由算法 1.静态路由与动态路由 路由器转发分组是通过路由表转发的，而路由表是通过各种算法得到的。从能否随网络的通 信量或拓扑自适应地进行调整变化来划分，路由算法可分为两大类：静态路由与动态路由。、 静态路由指需要网络管理员手工配置路由信息. 动态路由(又称自适应路路由算法),指路由器上的路由表项使根据相互连接的路由器之间彼此交换信息,然后按照一定的 算法优化出阿里的.这些路由信息会在一定时间内不断的更新,以适应网络的变化.常用的动态路由算法可分为两类: 距离-向量路由算法(RIP) 链路状态路由算法(OSPF) 链路状态路由算法和距离向量路由算法的比较 在距离-向量路由算法中，每个结点仅与它的直接邻居交谈，它为它的邻居提供从自己到网络中所有其他结点的最低费用估计。 在链路状态路由算法中，每个结点通过广播的方式与所有其他结点交谈，但它仅告诉它们与它直接相连的链路的费用。 相较之下，距离-向量路由算法有可能遇到路由环路等问题。 2.层次路由 当网络规模扩大时,路由器的路由表成比例的增大.这回消耗极大的时间和资源来扫描路由表 因此路由选择必须要按照层次的方法进行 因特网将整个互联网划分为许多较小的自治系统(AS)(一个自治系统又包含很多的局域网),每个自治系统有权 选择本系统采用何种的路由选择协议. 而如果两个自治系统需要通信,就需要一种在两个自治系统之间的协议来屏蔽这些差异. 四 IPV4 ⭐ 1.TCP/IP协议栈 之前我们介绍过IP在TCP/IP协议族中的位置: 2.IPv4分组 IPv4即现在普遍使用的IP(版本4).IP定义数据传送的基本单元---IP分组及其确切的数据格式 IP也包括一套规则,指明分组如何处理,错误怎样控制 1.IPv4分组的格式 一个IP分组由首部和数据两部分组成。首部前一部分的长度固定，共20B，是所有IP分组必须具有的。在首部固定部分的后面是一些可选字段，其长度可变，用来提供错误检测及安全等机制。 可变部分格式: 1)版本。指IP的版本，目前广泛使用的版本号为4。 2)首部长度。占4位,1B。以32位为单位，最大值为60B (15*4B)。最常用的首部长度是20B,此时不使用任何选项(即可选字段)。 3)总长度。占16位,2B。指首部和数据之和的长度，单位为字节，因此数据报的最大长度为216-1= 65535B。以太网帧的最大传送单元(MTU)为1500B，因此当一个IP数据报封装成帧时，数据报的总长度(首部加数据) - -定不能超过下面数据链路层的MTU值。 4)标识。占16位。它是一个计数器，每产生一个数据报就加1,并赋值给标识字段。但它并不是“序号”(因为IP是无连接服务)。当一个数据报的长度超过网络的MTU时，必须分片，此时每个数据报片都复制一次标识号， 以便能正确重装成原来的数据报。 5)标志。占3位。标志字段的最低位为MF, MF= 1表示后面还有分片，MF= 0表示最后一个分片。” 标志字段中间的一位是DF,只有当DF =0时才允许分片。 6)片偏移。占13位。它指出较长的分组在分片后，某片在原分组中的相对位置。片偏移以8个字节为偏移单位,8B，即每个分片的长度一定是8B (64 位)的整数倍。 7)首部校验和。占16位。IP数据报的首部校验和只校验分组的首部，而不校验数据部分。 8)生存时间(TTL)。占8位。数据报在网络中可通过的路由器数的最大值，标识分组在网络中的寿命，以确保分组不会永远在网络中循环。路由器在转发分组前，先把TTL减1。若TTL被减为0，则该分组必须丢弃。 9)协议。占8位。指出此分组携带的数据使用何种协议，即分组的数据部分应交给哪个传输层协议，如TCP、UDP等。其中值为6表示TCP,值为17表示UDP。 10)源地址字段。占4B，标识发送方的IP地址。 11)目的地址字段`。占4B，标识接收方的IP地址。 2.IP数据报分片 一个链路层数据报能承载的最大数据量称为最大传送单元(MTU)。因为IP数据报被封装在链路层数据报中，因此链路层的MTU严格地限制着IP数据报的长度，而且在IP数据报的源与目的地路径上的各段链路可能使用不同的链路层协议，有不同的MTU。例如，以太网的MTU为1500B，而许多广域网的MTU不超过576B。当IP数据报的总长度大于链路MTU时，就需要将IP数据报中的数据分装在两个或多个较小的IP数据报中，这些较小的数据报称为片。 在上面的IP分组格式中,有两个重要的字段 标志位 中间为DF :dont Fragment (1禁止分片,0允许分片) 最低为MF more Fragment (1代表后面还有分片) 片偏移 指较长分组分片后,某片在原分组中的相对位置,以8B为单位 下面举个列子: 3.网络层转发分组的流程 1)从数据报的首部提取目的主机的IP地址D，得出目的网络地址N。 2)若网络N与此路由器直接相连，则把数据报直接交付给目的主机D，这称为路由器的直接交付;否则是间接交付，执行步骤3)。 3)若路由表中有目的地址为D的特定主机路由(对特定的目的主机指明一个特定的路由，通常是为了控制或测试网络，或出于安全考虑才采用的)，则把数据报传送给路由表中所指明的下一跳路由器;否则，执行步骤4)。 4)若路由表中有到达网络N的路由，则把数据报传送给路由表指明的下一跳路由器;否则，执行步骤5)。 5)若路由表中有一个默认路由，则把数据报传送给路由表中所指明的默认路由器;否则，执行步骤6)。 6)报告转发分组出错。 4.IPv4与NAT 1.IPv4地址 组成与分类 连接到因特网上的每台主机(或路由器)都分配一个32比特的全球唯一标识符，即IP地址。传统的IP地址是分类的地址，分为A、B、C、D、E五类。 无论哪类IP地址，都由网络号和主机号两部分组成。 即IP地址::= {, }。 其中网络号标志主机(或路由器)所连接到的网络。–个网络号在整个因特网范围内必须是唯一的。 主机号标志该主机(或路由器)。一台主机号在它前面的网络号所指明的网络范围内必须是唯一的。 由此可见，一个IP地址在整个因特网范围内是唯–的。 特殊地址 在各类IP地址中，有些IP地址具有特殊用途，不用做主机的IP地址: 常见IP地址ABC类 A类地址可用的网络数为27-2，减2的原因是:第一，网络号字段全为0的IP地址是保留地址，意思是“本网络”;第二，网络号为127的IP地址是环回测试地址。 B类地址的可用网络数为214-1，减1的原因是128.0这个网络号是不可指派的。 C类地址的可用网络数为221-1,减1的原因是网络号为192.0.0 的网络是不可指派的。 IP地址有以下重要特点: 1)每个IP地址都由网络号和主机号两部分组成，因此IP地址是–种分等级的地址结构。分等级的好处是:①IP地址管理机构在分配IP地址时只分配网络号(第一级)， 而主机号(第二级)则由得到该网络的单位自行分配，方便了IP 地址的管理;②路由器仅根据目的主机所连接的网络号来转发分组(而不考虑目标主机号)，从而减小了路由表所占的存储空间。 2）IP 地址是标志一台主机(或路由器)和一条链路的接口。当一台主机同时连接到两个网络时，该主机就必须同时具有两个相应的IP地址，每个IP地址的网络号必须与所在网络的网络号相同，且这两个IP地址的网络号是不同的。因此IP网络上的一一个路由器必然至少应具有两个IP地址(路由器每个端口必须至少分配一个IP地址)。 3)用转发器或桥接器(网桥等)连接的若干LAN仍然是同一个网络(同一个广播域)，因此该LAN中所有主机的IP地址的网络号必须相同，但主机号必须不同。 4)在IP地址中，所有分配到网络号的网络(无论是LAN还是WAN)都是平等的。 5)在同一个局域网上的主机或路由器的IP地址中的网络号必须是一样的。路由器总是具有两个或两个以上的IP地址，路由器的每个端口都有一个不同网络号的IP地址。 2.NAT 网络地址转化 NAT简介 网络地址转换(NAT)是指通过将专用网络地址(如Intranet)转换为公用地址(如Internet),从而对外隐藏内部管理的IP地址。它使得整个专用网只需要一个全球IP地址(分配给NAT转化器)就可以与因特网连通，由于专用网本地IP地址是可重用的，所以NAT大大节省了IP地址的消耗。同时，它隐藏了内部网络结构，从而降低了内部网络受到攻击的风险。 私有IP地址 此外，为了网络安全，划出了部分IP地址为私有IP地址。私有IP地址只用于LAN,不用于WAN连接(因此私有IP地址不能直接用于Internet,必须通过网关利用NAT把私有IP地址转换为Internet中合法的全球IP地址后才能用于Internet), 并且允许私有IP地址被LAN重复使用。这有效地解决了IP地址不足的问题。私有IP地址网段如下: 专用互联网/本地互联网 在因特网中的所有路由器，对目的地址是私有地址的数据报一律不进行转发。这种采用私有IP地址的互联网络称为专用互联网或本地互联网。私有IP地址也称可重用地址。 如何实现私有IP地址上网 使用NAT时需要在专用网连接到因特网的路由器上安装NAT软件，NAT路由器至少有一个有效的外部全球地址。 使用本地地址的主机和外界通信时，·NAT路由器使用NAT转换表将本地地址转换成全球地址，或将全球地址转换成本地地址。 NAT转换表中存放着{本地IP地址:端口}到{全球IP地址:端口}的映射。 通过{ip地址:端口}这样的映射方式， 可让多个私有IP地址映射到同一一个全球IP地址。 5 子网划分与子网掩码 1.子网划分 两级IP地址的缺点:IP地址空间的利用率有时很低;给每个物理网络分配一个网络号会使路由表变得太大而使网络性能变坏;两级的IP地址不够灵活。 于是,在IP地址中又增加了一个“子网号字段”，使两级IP地址变成了三级IP地址。这种做法称为子网划分。子网划分已成为因特网的正式标准协议 ●子网划分纯属一个单位内部的事情。单位对外仍然表现为没有划分子网的网络。 ●从主机号借用若干比特作为子网号，当然主机号也就相应减少了相同的比特。三级IP地址的结构如下: IP地址={,, }。 ●凡是从其他网络发送给本单位某台主机的IP数据报,仍然是根据IP数据报的目的网络号，先找到连接到本单位网络上的路由器。然后该路由器在收到IP数据报后，按目的网络号.和子网号找到目的子网。最后把IP数据报直接交付给目的主机。 2.子网掩码 子网掩码的引入，为了告诉主机或路由器对一个A类、B类、C类网络进行了子网划分，使用子网掩码来表达对原网络中主机号的借位。为了使外部可以连接子网内的网络。 子网掩码是一个与IP地址相对应的、长32bit的二进制串,它由一串1和跟随的一串0组成。 其中，1对应于IP地址中的网络号及子网号，而0对应于主机号。 计算机只需将IP地址和其对应的子网掩码逐位“与”(逻辑AND运算)，就可得出相应子网的网络地址。 现在的因特网标准规定:所有的网络都必须使用子网掩码。如果一个网络未划分子网，那么就采用默认子网掩码。 A、B、C类地址的默认子网掩码分别为255.0.0.0、 255.255.0.0、255.255.255.0. 例如，某主机的IP地址192.168.5.56，子网掩码为255.255.255.0,进行逐位“与”运算后，得出该主机所在子网的网络号为192.168.5.0。 列题: 某主机的IP地址为180.80.77.55,子网掩码为255.255.252.0. 因为77 二进制为:0100110 252为:11111100 相与后,可得76 也即,该子网的网络地址为:180.80.76.0 使用子网时的分组转发 由于子网掩码是一个网络或一个子网的重要属性，所以路由器在相互之间交换路由信息时，必须把自己所在网络(或子网)的子网掩码告诉对方。路由表中的每个条目，除要给出目的网络地址和下一跳地址外，还要同时给出该目的网络的子网掩码。 在使用子网掩码的情况下: 1)一台主机在设置IP地址信息的同时，必须设置·子网掩码。 2)同属于一个子网的所有主机及路由器的相应端口，必须设置相同的子网掩码。 3)路由器的路由表中，所包含信息的主要内容必须有目的网络地址、子网掩码、下一 跳地址。 使用子网掩码时路由器的分组转发算法: 1)从收到的分组的首部提取目的IP地址，记为D。 2)先判断是否为直接交付。对路由器直接相连的网络逐个进行检查:用各网络的子网掩码和D逐位相“与”，看结果是否和相应的网络地址匹配。若匹配，则将分组直接交付，否则间接交付，执行步骤3)。 3)若路由表中有目的地址为D的特定主机路由，则将分组传送给路由表中所指明的下一跳路由器;否则，执行4)。 4)对路由表中的每一行(目的网络地址、子网掩码、下一跳地址)中的子网掩码和D逐位相“与”，其结果为N。若N与该行的目的网络地址匹配，则将分组传送给该行指明的下一跳路由器;否则，执行步骤5)。 5)若路由表中有一个默认路由，则将分组传送给路由表中所指明的默认路由器;否则，执行步骤6)。 6)报告转发分组出错。 3.无类域间路由选择CIDR 6.地址解析协议 —ARP 1.IP地址与硬件地址 IP地址是网络层使用的地址，它是分层次等级的。 硬件地址是数据链路层使用的地址(如MAC地址)，它是平面式的。 通过数据封装，把IP数据报分组封装为MAC帧后，数据链路层看不见数据报分组中的IP地址。 2.ARP (Address ResolutionProtocol) 无论网络层使用什么协议，在实际网络的链路上传送数据帧时，最终必须使用硬件地址。所以需要一种方法来完成 IP地址到MAC地址的映射，这就是地址解析协议(Address ResolutionProtocol, ARP)。 每台主机都设有-一个ARP高速缓存，用来存放本局域网上各主机和路由器的IP地址到MAC地址的映射表，称ARP表。使用ARP来动态维护此ARP表。 工作过程: 7.动态主机配置协议—DHCP (Dynamic Host Configuration Protocol) 动态主机配置协议(Dynamic Host Configuration Protocol, DHCP)常用于给主机动态地分配IP地址，它提供了即插即用联网的机制，这种机制允许一台计算机加入新的网络和获取IP地址而不用手工参与。 DHCP是应用层协议，它是基于UDP的。 DHCP的工作原理如下: 使用client/server方式 需要IP地址的主机在启动时就向DHCP服务器广播发送发送报文，这时该主机就成为DHCP客户。本地网络上所有主机都能收到此广播报文，但只有DHCP服务器才回答此广播报文。 DHCP服务器先在其数据库中查找该计算机的配置信息。若找到，则返回找到的信息。 若找不到，则从服务器的IP地址池中取一个地址分配给该计 算机。DHCP服务器的回答报文称为提供报文 8.网际控制报文协议— ICMP 举例来讲，一个新搭建好的网络，往往需要先进行一个简单的测试，来验证网络是否通畅；但是IP协议并不提供可靠传输，所以IP协议并不会通知传输层是否丢包及丢包的原因。所以才有了ICMP协议的存在。 ICMP协议即Internet控制报文协议。它是TCP/IP协议簇的一个子协议，用于在IP主机与路由器间传递控制消息。这里的控制消息指的是：网络是否通畅、主机是否可达、路由是否可用等网络本身的消息。 1.ICMP功能 （1）确认IP包是否成功到达目的地址 （2）通知在发送过程中IP包被丢弃的原因 （3）它是基于IP协议工作的，但并不是传输层的功能，所以把它归结为网络层协议 （4）ICMP只能搭配IPv4使用，若是IPv6，则需要用ICMPv6 2.ICMP报文格式 ICMP大概分为两类报文： （1）通知出错原因 （2）用于诊断查询 ICMP常用类型如下表： 3. ping命令 （1）ping + IP地址，可以连接同局域网下的主机 （2）ping + 域名，比如ping+www.baidu.com，就可以与百度连接，得到百度的一个应答报文。这里与ping+IP地址类似，因为一个域名可以通过DNS解析为IP地址 （3）ping命令不仅可以检验网络的验证性，同时也会统计响应时间和TTL（生存周期） （4）对端收到之后，会返回一个ICMP回送应答（Echo Reply） 要注意的是：telnet是23端口，ssh是22端口，ping不关心端口号。因为ping命令基于ICMP协议，是网络层的内容，而端口号则是传输层的内容。 "},"计算机网络/5.网络层2.html":{"url":"计算机网络/5.网络层2.html","title":"5.网络层(二)","keywords":"","body":"1.IPV6的诞生1.诞生原因2.数据格式3.基本地址类型4.IPV6与IPV42.IP组播1.组播2.组播地址3.IGMP3.移动IP4.路由器1.IPV6的诞生 1.诞生原因 解决IP地址耗尽问题的措施有以下三种: ①采用无类别编址CIDR，使IP地址的分配更加 合理; ②采用网络地址转换(NAT)方法以节省全球IP地址; ③采用具有更大地址空间的新版本的IPv6。 其中前两种方法只是延长了IPv4 地址分配结束的时间,治标不治本，只有第三种方法从根本上解决了IP地址的耗尽问题。 2.数据格式 地址格式 IPv6的128位地址每16位划分为一段，总共8段，每段用冒号隔开，这种表示方法叫做“冒号十六进制表示法”，如下： fe80:0000:0001:0000:0440:44ff:1233:5678 压缩格式 有时候首先格式中的IP地址中有好多0，就可以把连续的一段0压缩为 :: ,即用冒号表示，但是一个IP地址中只能有一个::， 如下： fe80:0000:0000:0000:0000:0000:0001:0000 —>fe80::0001:0000 内嵌IPv4地址的IPv6 在IPv4向IPv6过度的过程中，IPv4的地址会内嵌到IPv6中去，因此在IPv6地址的第一部分使用IPv6的格式（十六进制表示），第二部分使用IPv4的格式（十进制表示）。 如下： 0:0:0:0:0:0:192.168.12.1 或者 ::192.168.12.1 0:0:0:0:0:FFFF:192.168.12.1 或者 ::FFFF:192.168.12.1 3.基本地址类型 IPv6数据报的目的地址可以是以下三种基本类型地址之一.: 1)单播。单播就是传统的点对点通信。 2)多播。多播是一点对多点的通信，分组被交付到一-组计算机的每台计算机。 3)任播。这是IPv6增加的一-种类型。任播的目的站是-组计算机，但数据报在交付时只交付其中的一台计算机，通常是距离最近的一台计算机。 4.IPV6与IPV4 1.对比 2.过渡策略 2.IP组播 1.组播 IP数据报传输的三种方式 以用户看视频为例，进一步了解单播、广播和多播三种传输方式 为了能够支持像视频点播和视频会议这样的多媒体应用，网络必须实施某种有效的组播机制。 使用多个单播传送来仿真组播总是可能的，但这会引起主机上大量的处理开销和网络上太多的交通量。 人们所需要的组播机制是让源计算机一次发送的单个分组可以抵达用一个组地址标识的若干目标主机，并被它们正确接收。 2.组播地址 3.IGMP 1.定义 作用: 让路由器知道本局域网上是否又主机参加或退出了某个组播组 ICMP和IGMP都是使用IP数据包传播报文 2.工作方式 某主机要加入组播组时,改主机向组播组的组播地址发送一个IGMP报文,声明自己想要加入该组 本地组播路由器收到IGMP报文后,要利用组播路由选择协议把这组成员关系发送到因特网上的其他组播路由器 本地组播路由器周期性的探寻本地局域网上的主机,以便知道这些主机是否还是该组播组的成员 3.移动IP 支持移动性的因特网体系结构与协议共称为移动IP,它是为了满足移动结点(计算机、服务、网段等)在移动中保持其连接性而设计的。 更确切地说，移动IP技术是指移动结点以固定的网络IP地址实现跨越不同网段的漫游功能，并保证基于网络IP的网络权限在漫游过程中不发生任何改变。 移动IP的目标是把分组自动地投递给移动结点。一个移动结点是把其连接点从一个网络或子网改变到另-一个网络或子网的主机。 使用移动IP，一个移动结点可以在不改变其IP地址的情况下改变其驻留位置。 举个列子: 具体实现: 4.路由器 "},"计算机网络/6.传输层.html":{"url":"计算机网络/6.传输层.html","title":"6.传输层","keywords":"","body":"一 传输层功能1.进程之间的逻辑通信2.复用和分用3.差错检查4.两种协议5.端口与套接字二 UDP协议1.特点2.首部格式3.差错检验三 TCP协议1.协议特点2.首部格式3.连接管理4.可靠传输5.流量控制6.拥塞控制一 传输层功能 1.进程之间的逻辑通信 与网络层的区别是，网络层提供的是主机之间的逻辑通信。 从网络层来说，通信的双方是两台主机，IP 数据报的首部给出了这两台主机的IP地址。 但“两台主机之间的通信”实际上是两台主机中的应用进程之间的通信，应用进程之间的通信又称端到端的逻辑通信。 这里“逻辑通信”的意思是:传输层之间的通信好像是沿水平方向传送数据，但事实上这两个传输层之间并没有–条水平方向的物理连接。 2.复用和分用 复用是指发送方不同的应用进程都可使用同一个传输层协议传送数据; 分用是指接收方的传输层在剥去报文的首部后能够把这些数据正确交付到目的应用进程。 注意： 传输层的复用分用功能与网络层的复用分用功能不同。 网络层的复用是指发送方不同协议的数据都可以封装成IP数据报发送出去, 网络层的分用是指接收方的网络层在剥去首部后把数据交付给相应的协议。 3.差错检查 网络层只检查IP数据报的首部，不检验数据部分是否出错。 因此传输层需要对数据部分进行差错检测 4.两种协议 传输层只有两种协议:TCP和UDP,且只能二选一使用 TCP是面向连接,提供可靠传输的服务 UDP面向无连接,不可靠服务 5.端口与套接字 端口 端口能够让应用层的各种应用进程将其数据通过端口向下交付给传输层，以及让传输层知道应当将其报文段中的数据向上通过端口交付给应用层相应的进程。 端口是传输层服务访问点(TSAP)，它在传输层的作用类似于IP地址在网络层的作用或MAC地址在数据链路层的作用，只不过IP地址和MAC地址标识的是主机，而端口标识的是主机中的应用进程。 数据链路层的SAP是MAC地址，网络层的SAP是IP地址，传输层的SAP是端口。 在协议栈层间的抽象的协议端口是软件端口，它与路由器或交换机上的硬件端口是完全不同的概念。 硬件端口是不同硬件设备进行交互的接口，而软件端口是应用层的各种协议进程与传输实体进行层间交互的一种地址。 传输层使用的是软件端口。 套接字 (详细可看计算机网络篇第二篇文章) 二 UDP协议 1.特点 1)UDP无须建立连接。 UDP不会引入建立连接的时延。 试想如果DNS运行在TCP而非UDP.上，那么DNS的速度会慢很多。 HTTP使用TCP而非UDP，是因为对于基于文本数据的Web网页来说，可靠性是至关重要的。 2)无连接状态。 TCP需要在端系统中维护连接状态。此连接状态包括接收和发送缓存、拥塞控制参数和序号与确认号的参数。 而UDP不维护连接状态，也不跟踪这些参数。 因此，某些专用应用服务器使用UDP时，一般都能支持更多的活动客户机。 3)分组首部开销小。 TCP有20B的首部开销，而UDP仅有8B的开销。 4)应用层能更好地控制要发送的数据和发送时间。 UDP没有拥塞控制，因此网络中的拥塞不会影响主机的发送效率。 某些实时应用要求以稳定的速度发送，能容忍一些数据的丢失，但不允许有较大的时延，而UDP正好满足这些应用的需求。 5)UDP常用于一次性传输较少数据的网络应用 如DNS、SNMP等，因为对于这些应用，若采用TCP，则将为连接创建、维护和拆除带来不小的开销。 UDP也常用于多媒体应用(如IP电话、实时视频会议、流媒体等)，显然，可靠数据传输对这些应用来说并不是最重要的，但TCP的拥塞控制会导致数据出现较大的延迟，这是它们不可容忍的。 6)UDP提供尽最大努力的交付，即不保证可靠交付 但这并不意味着应用对数据的要求是不可靠的，因此所有维护传输可靠性的工作需要用户在应用层来完成。 应用实体可以根据应用的需求来灵活设计自己的可靠性机制。 7)UDP是面向报文的。 发送方UDP对应用层交下来的报文，在添加首部后就向下交付给IP层， 既不合并，也不拆分，而是保留这些报文的边界; 接收方UDP对IP层交上来UDP用户数据报，在去除首部后就原封不动地交付给上层应用进程，一次交付一个完整的报文。 因此报文不可分割，是UDP数据报处理的最小单位。 2.首部格式 1)源端口。源端口号。在需要对方回信时选用，不需要时可用全0。 2)目的端口。目的端口号。这在终点交付报文时必须使用到。 3)长度。UDP数据报的长度(包括首部和数据)，其最小值是8 (仅有首部)。 4)校验和。检测UDP数据报在传输中是否有错。有错就丢弃。该字段是可选的，当源主机不想计算校验和时，则直接令该字段为全0。 3.差错检验 UDP 检验和提供了差错检测的功能。这是基于端到端原则实现的。但是 UDP 的检验和并不提供差错回复的能力。 原理 对发送方的 UDP 报文段的所有 16 比特字的和进行反码运算，当求和遇见溢出的时候，进行回卷（回卷的补充在下面），得到的结果放在 UDP 报文段中的检验和字段 什么是回卷 所谓 “回卷” 就是当进行 16 比特的加法运算的时候，如果进位到 17位，则将第 17 位和后 16 位进行加法和运算。 我们将在下面的小例子中实际操练。 假设我们有 3 个 16 比特的字，分别如下 0110011001100000 0101010101010101 1000111100001100 123 第一步：对 3 个 16 比特的字依次相加 0110 0110 0110 0000 + 0101 0101 0101 0101 + 1000 1111 0000 1100 = 0100 1010 1100 0010 1 注意，在最后一次加法的过程中，发生了回卷，看下面，多了第 17 位，要消除第 17 位 这两个数相加得到 此时用 1 + 0100 1010 1100 0001 = 0100 1010 1100 0010 第二步：对和进行反码运算 0100 1010 1100 0010 的反码 1011 0101 0011 1101. 1 第三步：将这个值放入校验和中 第四步：在接收方中，将全部的 4 个 16比特的字（包含了校验和）加在一起，没有差错的话，就是 1111 1111 1111 1111 三 TCP协议 1.协议特点 面向连接 点对点服务 可靠服务 可靠有序,不丢不重 全双工通信 面向字节流 2.首部格式 TCP传送的数据单元称为报文段。一个TCP报文段分为TCP首部和TCP数据两部分，整个TCP报文段作为IP数据报的数据部分封装在IP数据报中 其首部的前20B是固定的。TCP报文段的首部最短为20B，后面有4N字节是根据需要而增加的选项，通常长度为4B的整数倍。 TCP报文段既可以用来运载数据，又可以用来建立连接、释放连接和应答。 1)源端口和目的端口字段。各占2B。端口是运输层与应用层的服务接口，运输层的复用和分用功能都要通过端口实现。 2)序号字段。占4B。TCP是面向字节流的(即TCP传送时是逐个字节传送的)，所以TCP连接传送的数据流中的每个字节都编上一个序号。序号字段的值指的是本报文段所发送的数据的第一个字节的序号。 3)确认号字段。占4B,是期望收到对方的下一个报文段的数据的第一个字节的序号。若确认号为N，则表明到序号N- 1为止的所有数据都已正确收到。 4)数据偏移(即首部长度)。占4位，这里不是IP数据报分片的那个数据偏移，而是表示首部长度， 5)紧急位URG。URG= 1时，表明紧急指针字段有效。它告诉系统报文段中有紧急数据，应尽快传送(相当于高优先级的数据)。但URG需要和紧急指针配套使用，即数据从第一个字节到紧急指针所指字节就是紧急数据。 7)确认位ACK。只有当ACK= 1时确认号字段才有效。当ACK=0时，确认号无效。TCP规定，在连接建立后所有传送的报文段都必须把ACK置1. 8)推送位PSH (Push)。 接收TCP收到PSH= 1的报文段，就尽快地交付给接收应用进程而不再等到整个缓存都填满后再向上交付。 9)复位位RST (Reset)。RST=1时，表明TCP连接中出现严重差错(如主机崩溃或其他原因)，必须释放连接，然后再重新建立运输连接。 10)同步位SYN。同步SYN= 1表示这是一个连接请求或连接接收报文。当SYN=1, ACK=0时，表明这是一个连接请求报文，对方若同意建立连接，则在响应报文中使用SYN=1, ACK=1。即SYN= 1表示这是一个连接请求或连接接收报文。 11)终止位FIN (Finish)。用来释放一个连接。FIN= 1表明此报文段的发送方的数据已发送完毕，并要求释放传输连接。 12)窗口字段。占2B。它指出现在允许对方发送的数据量，接收方的数据缓存空间是有限的，因此用窗口值作为接收方让发送方设置其发送窗口的依据，单位为字节。 例如，假设确认号是701，窗口字段是1000。这表明，从701号算起，发送此报文段的接收方方还有接收1000B数据(字节序号为701 ~1700)的接收缓存空间。 13)校验和。占2B。校验和字段检验的范围包括首部和数据两部分。 14)紧急指针字段。占16 位，指出在本报文段中紧急数据共有多少字节(紧急数据放在本报文段数据的最前面)。 15)选项字段。长度可变。TCP最初只规定了一种选项，即最大报文段长度(Maximum SegmentSize，MSS)。MSS是TCP报文段中的数据字段的最大长度。窗口扩大、时间戳、选择确认 16)填充字段。这是为了使整个首部长度是4B的整数倍。填充0. 3.连接管理 关于三次握手,四次挥手的连接管理内容,可看第一篇文章 4.可靠传输 序号 确认 重传 5.流量控制 在通信过程中，接收方根据自己接收缓存的大小，动态地调整发送方的发送窗口大小，这称为接收窗口rwnd, 即调整TCP报文段首部中的“窗口”字段值，来限制发送方向网络注入报文的速率。 同时，发送方根据其对当前网络拥塞程序的估计而确定的窗口值，这称为拥塞窗口cwnd，其大小与网络的带宽和时延密切相关。 例如，在通信中，有效数据只从A发往B，而B仅向A发送确认报文，这时B可以通过设置确认报文段首部的窗口字段来将rwnd通知给A。 rwnd 即接收方允许连续接收的最大能力，单位是字节。 发送方A总是根据最新收到的rwnd值来限制自己发送窗口的大小，从而将未确认的数据量控制在rwnd大小之内，保证A不会使B的接收缓存溢出。 当然，A的发送窗口的实际大小取rwnd和cwnd中的最小值 传输层和数据链路层的流量控制的区别是: 传输层定义端到端用户之间的流量控制，数据链路层定义两个中间的相邻结点的流量控制。 另外，数据链路层的滑动窗口协议的窗口大小不能动态变化，传输层的则可以动态变化。 6.拥塞控制 定义 所谓拥塞控制，是指防止过多的数据注入网络，保证网络中的路由器或链路不致过载。出现拥塞时，端点并不了解到拥塞发生的细节，对通信连接的端点来说，拥塞往往表现为通信时延的增加。当然，拥塞控制和流量控制也有相似的地方，即它们都通过控制发送方发送数据的速率来达到控制效果。 与流量控制的对比 拥塞控制是让网络能够承受现有的网络负荷，是一个全局性的过程，涉及所有的主机、所有的路由器，以及与降低网络传输性能有关的所有因素。 流量控制往往是指点对点的通信量的控制，即接收端控制发送端，它所要做的是抑制发送端发送数据的速率，以便使接收端来得及接收。 例如： 某个链路的传输速率为10Gb/s,某巨型机向一台PC以1Gb/s的速率传送文件，显然网络的带宽是足够大的，不存在拥塞问题，但如此高的发送速率将导致PC可能来不及接收，因此必须进行流量控制。 但若有100万台PC在此链路上以1Mb/s的速率传送文件，则现在的问题就变为网络的负载是否超过了现有网络所能承受的范围。就像我们上网一样，有时候加载会很慢，提示访问请求过多，请稍后再试，就是网络产生了拥塞，带宽小，一下不能支持给多个请求终端发送数据。 为了更好地对传输层进行拥塞控制，因特网建议标准定义了以下4种算法:慢开始、拥塞避免、快重传、快恢复。 窗口 在通信过程中，接收方根据自己接收缓存的大小，动态地调整发送方的发送窗口大小，这称为接收窗口rwnd, 即调整TCP报文段首部中的“窗口”字段值，来限制发送方向网络注入报文的速率。 同时，发送方根据其对当前网络拥塞程序的估计而确定的窗口值，这称为拥塞窗口cwnd，其大小与网络的带宽和时延密切相关。 慢开始与拥塞避免 慢开始 在TCP刚刚连接好并开始发送TCP报文段时，先令拥塞窗口cwnd= 1,即一个最大报文段长度MSS.每收到一个对新报文段的确认后，将cwnd加1,即增大一个 MSS.用这样的方法逐步增大发送方的拥塞窗口cwnd,可使分组注入网络的速率更加合理。 例如，A向B发送数据，发送时A的拥塞窗口为2,那么A一次可以发送两个TCP报文段，经过一个RTT后(也称一个传输轮次)，A收到B对刚才两个报文的确认，于是把拥塞窗口调整为4，下一次发送时就可一次发送4个报文段。 使用慢开始算法后，每经过一个传输轮次(即往返时延RTT)，拥塞窗口cwnd就会加倍，即cwnd的大小指数式增长。这样，慢开始一直把拥塞窗口cwnd增大到一个规定的慢开始门限ssthresh(阈值)，然后改用拥塞避免算法。 拥塞避免 拥塞避免算法的做法如下:发送端的拥塞窗口cwnd每经过- -一个往返时延RTT就增加一个MSS的大小，而不是加倍,使cwnd按线性规律缓慢增长(即加法增大),而当出现一次超时(网络拥塞)时，令慢开始门限ssthresh等于当前cwnd的一半(即乘法减小)。 根据cwnd的大小执行不同的算法，可归纳如下: ●当cwnd ssthresh时，停止使用慢开始算法而改用拥塞避免算法。 ●当cwnd = sthresh时，既可使用慢开始算法，又可使用拥塞避免算法(通常做法)。 快重传和块恢复 快重传 快重传和快恢复算法是对慢开始和拥塞避免算法的改进。 (1)快重传 在TCP可靠传输机制中，快重传技术使用了冗余ACK来检测丢包的发生。同样，冗余ACK也用于网络拥塞的检测(丢了包当然意味着网络可能出现了拥塞)。快重传并非取消重传计时器，而是在某些情况下可更早地重传丢失的报文段。 当发送方连续收到三个重复的ACK报文时，直接重传对方尚未收到的报文段，而不必等待那个报文段设置的重传计时器超时。 (2)快恢复 快恢复算法的原理如下: 发送端收到连续三个冗余ACK (即重复确认)时，执行“乘法减小”算法，把慢开始门限ssthresh 设置为出现拥塞时发送方cwnd的一半。 与慢开始(慢开始算法将拥塞窗口cwnd设置为1)的不同之处是，它把cwnd的值设置为慢开始门限ssthresh改变后的数值，然后开始执行拥塞避免算法(“ 加法增大”)，使拥塞窗口缓慢地线性增大。 由于跳过了cwnd从1起始的慢开始过程,所以被称为快恢复。 在流量控制中，发送方发送数据的量由接收方决定，而在拥塞控制中，则由发送方自己通过检测网络状况来决定。 总结 实际上，慢开始、拥塞避免、快重传和快恢复几种算法应是同时应用在拥塞控制机制之中的 当发送方检测到超时的时候，就采用慢开始和拥塞避免， 当发送方接收到冗余ACK时，就采用快重传和快恢复。 注意： 发送方发送窗口的实际大小由流量控制和拥塞控制共同决定。 因此，当题目中同时出现接收端窗口(rwnd) 和拥塞窗口(cwnd) 时，发送方实际的发送窗口大小是由rwnd和cwnd中较小的那一个确定的。 "},"计算机网络/7.应用层.html":{"url":"计算机网络/7.应用层.html","title":"7.应用层(⭐)","keywords":"","body":"一 概述1.功能2.网络应用模型二 DNS域名解析协议1.DNS系统2.域名3.域名服务器4.域名解析过程三 万维网与HTTP协议1.www万维网2.HTTP—超文本传输协议3.常用应用程序的协议及端口号四 FTP 文件传输协议1.功能2.工作原理3.控制连接和数据连接一 概述 1.功能 功能 相关协议 文件访问和传输 FTP 电子邮件 SMTP 虚拟终端 HTTP 查询服务和远程作业登录 DNS 2.网络应用模型 client/server P2P 在P2P模型中，各计算机没有固定的客户和服务器划分。相反，任意一一对计算机一称为对等方(Peer)， 直接相互通信。 P2P 模型从本质上来看仍然使用客户/服务器方式，每个结点既作为客户访问其他结点的资源，也作为服务器提供资源给其他结点访问。 当前比较流行的P2P应用有PPlive、Bittorrent 和电驴等。 与C/S模型相比，P2P 模型的优点主要体现如下: 1)减轻了服务器的计算压力，消除了对某个服务器的完全依赖，可以将任务分配到各个结点上，因此大大提高了系统效率和资源利用率(例如，播放流媒体时对服务器的压力过大，而通过P2P模型，可以利用大量的客户机来提供服务)。 2)多个客户机之间可以直接共享文档。 3)可扩展性好，传统服务器有响应和带宽的限制，因此只能接受- -定 数量的请求。 4)网络健壮性强，单个结点的失效不会影响其他部分的结点。 P2P模型也有缺点。在获取服务的同时,还要给其他结点提供服务，因此会占用较多的内存，影响整机速度。 例如，经常进行P2P下载还会对硬盘造成较大的损伤。据某互联网调研机构统计，当前P2P程序已占互联网50%~90%的流量，使网络变得非常拥塞，因此各大ISP (互联网服务2提供商，如电信、网通等)通常都对P2P应用持反对态度。 二 DNS域名解析协议 1.DNS系统 域名系统(Domain Name System, DNS)是因特网使用的命名系统，用来把便于人们记忆的具有特定含义的主机名(如www.BitHachi.com)转换为便于机器处理的IP地址。 相对于IP地址，人们更喜欢使用具有特定含义的字符串来标识因特网上的计算机。 DNS系统采用客户/服务器模型，其协议运行在UDP之上，使用53号端口。 从概念上可将DNS分为3部分:层次域名空间、域名服务器和解析器。 某台主机访问网站www.bithachi.cn网站为例，DNS的大致流程 2.域名 因特网采用层次树状结构的命名方法。采用这种命名方法，任何一个连接到因特网的主机或路由器，都有一个唯一的层次结构名称，即域名(Domain Name)。 域(Domain)是名字空间中一个可被管理的划分。 域还可以划分为子域，而子域还可以继续划分为子域的子域，这样就形成 了顶级域、二级域、三级域等。 在域名系统中，每个域分别由不同的组织进行管理。每个组织都可以将它的域再分成一定数目的子域，并将这些子域委托给其他组织去管理。 例如，管理CN域的中国将EDU.CN子域授权给中国教育和科研计算机网(CERNET)来管理。 域名空间的树状结构： 每个域名都由标号序列组.成，而各标号之间用点(“.”)隔开。 关于域名中的标号有以下几点需要注意: 1)标号中的英文不区分大小写。. 2)标号中除连字符(-) 外不能使用其他的标点符号。 3)每个标号不超过63个字符，多标号组成的完整域名最长不超过255个字符。 4)级别最低的域名写在最左边，级别最高的顶级域名写在最右边。 顶级域名(Top Level Domain, TLD)分为如下三大类: 1)国家顶级域名(nTLD)。国家和某些地区的域名，如“.cn”表示中国，“.us”表示美国，.uk”表示英国。 2)通用顶级域名(gTLD)。 常见的有“.com” (公司)、“.net\" (网络服务机构)、“.org”(非营利性组织)和“.gov\" (国家或政府部门)等。 3)基础结构域名。这种顶级域名只有一个，即arpa,用于反向域名解析，因此又称反向域名。反向域名解析与通常的正向域名解析相反，提供IP地址到域名的对应，反向域名格式如：X.X.X.in-addr.arpa。很多网络服务提供商要求访问的IP地址具有反向域名解析的结果，否则不提供服务。 国家顶级域名下注册的二级域名均由该国家自行确定。 3.域名服务器 因特网的域名系统被设计成一个联机分布式的数据库系统，并采用客户/服务器模型。 域名到IP地址的解析是由运行在域名服务器上的程序完成的，一个服务器所负责管辖的(或有权限的)范围称为区(不以“域”为单位)，各单位根据具体情况来划分自己管辖范围的区，但在一个区中的所有结点必须是能够连通的，每个区设置相应的权限域名服务器，用来保存该区中的所有主机的域名到IP地址的映射。 每个域名服务器不但能够进行一些域名到IP地址的解析，而且还必须具有连向其他域名服务器的信息。当自己不能进行域名到IP地址的转换时，能够知道到什么地方去找其他域名服务器。 DNS使用了大量的域名服务器，它们以层次方式组织。没有一台域名服务器具有因特网上所有主机的映射，相反，该映射分布在所有的DNS上。 采用分布式设计的DNS，是一个在因特网上实现分布式数据库的精彩范例。主要有4种类型的域名服务器。 （1）根域名服务器 根域名服务器是最高层次的域名服务器，所有的根域名服务器都知道所有的顶级域名服务器的IP地址。 根域名服务器也是最重要的域名服务器，不管是哪个本地域名服务器，若要对因特网上任何一个域名进行解析，只要自己无法解析，就首先要求助于根域名服务器。 因特网上有13个根域名服务器，尽管我们将这13个根域名服务器中的每个都视为单个服务器，但每个“服务器”实际上是冗余服务器的集群，以提供安全性和可靠性。 需要注意的是，根域名服务器用来管辖顶级域(如.com)， 通常它并不直接把待查询的域名直接转换成IP地址，而是告诉本地域名服务器下一步应当找哪个顶级域名服务器进行查询。 （2）顶级域名服务器 这些域名服务器负责管理在该顶级域名服务器注册的所有二级域名。 收到DNS查询请求时,就给出相应的回答(可能是最后的结果，也可能是下一步应当查找的域名服务器的IP地址)。 （3）授权域名服务器(权限域名服务器) 每台主机都必须在授权域名服务器处登记。为了更加可靠地工作，一台主机最好至少有两个授权域名服务器。 实际上，许多域名服务器都同时充当本地域名服务器和授权域名服务器。 授权域名服务器总能将其管辖的主机名转换为该主机的IP地址。 （4）本地域名服务器 本地域名服务器对域名系统非常重要。 每个因特网服务提供者(ISP)， 或一所大学，甚至一所大学中的各个系，都可以拥有一个本地域名服务器。 当一台主机发出DNS查询请求时，这个查询请求报文就发送给该主机的本地域名服务器。 事实上，我们在Windows系统中配置“本地连接”时，就需要填写DNS地址，这个地址就是本地DNS (域名服务器)的地址。 4.域名解析过程 域名解析是指把域名映射成为IP地址或把IP地址映射成域名的过程。前者称为正向解析，后者称为反向解析。 当客户端需要域名解析时，通过本机的DNS客户端构造一个DNS请求报文，以UDP数据报方式发往本地域名服务器。 域名解析有两种方式:递归查询和递归与迭代相结合的查询。 （1）递归查询方式 递归查询的过程如下图所示， 由于该方法给根域名服务造成的负载过大，所以在实际中几乎不使用。 （2） 递归与迭代相结合 常用递归与迭代相结合的查询方式如下图所示 假定某客户机想获知域名为y.abc.com主机的IP地址，域名解析的过程(共使用8个UDP报文)如下: ①客户机向其本地域名服务器发出DNS请求报文。 ②本地域名服务器收到请求后，查询本地缓存，若没有该记录，则以DNS客户的身份向根域名服务器发出解析请求。 ③根域名服务器收到请求后，判断该域名属于.com域，将对应的顶级域名服务器dns.com的IP地址返回给本地域名服务器。 ④本地域名服务器向顶级域名服务器dns.com发出解析请求报文。 ⑤顶级域名服务器dns.com收到请求后，判断该域名属于abc.com域，因此将对应的授权域名服务器dns.abc.com的IP地址返回给本地域名服务器。 ⑥本地域名服务器向授权域名服务器dns.abc.com发起解析请求报文。 ⑦授权域名服务器dns.abc.com收到请求后，将查询结果返回给本地域名服务器。 ⑧本地域名服务器将查询结果保存到本地缓存，同时返回给客户机。 为了提高DNS的查询效率，并减少因特网上的DNS查询报文数量，在域名服务器中广泛地使用了高速缓存。 当一个DNS服务器接收到DNS查询结果时，它能将该DNS信息缓存在高速 缓存中。这样，当另一个相同的域名查询到达该DNS服务器时，该服务器就能够直接提供所要求的IP地址，而不需要再去向其他DNS服务器询问。 因为主机名和IP地址之间的映射不是永久的，所以DNS服务器将在一段时间后丢弃高速缓存中的信息。 三 万维网与HTTP协议 1.www万维网 （1）什么是万维网？ 万维网(World Wide Web, WWW)是一个资料空间，在这个空间中: 一样有用的事物称为一样“资源”,并由一个全域“统一资源定位符”(URL)标识。 这些资源通过超文本传输协议(HTTP)传送给使用者，通过单击链接来获取资源。 万维网使用链接的方法能让用户非常方便地从因特网上的一个站点访问另一个站点，从而主动地按需获取丰富的信息。 超文本标记语言(HyperText Markup Language，HTML)使得万维网页面的设计者可以很方便地用一个超链接从本页面的某处链接到因特网上的任何一个万维网页面，并能够在自己的计算机屏幕上显示这些页面。 （2）万维网的组成 万维网的内核部分是由三个标准构成的: 1)统一资源定位符(URL)。 负责标识万维网上的各种文档，并使每个文档在整个万维网的范围内具有唯一的标识符URL。 2)超文本传输协议(HTTP)。 一个应用层协议，它使用TCP连接进行可靠的传输，HTTP是万维网客户程序和服务器程序之间交互所必须严格遵守的协议。 3)超文本标记语言(HTML)。 一种文档结构的标记语言，它使用一些约定的标记对页面上的各种信息(包括文字、声音、图像、视频等)、格式进行描述。 （3）URL—统一资源定位符 URL是对可以从因特网上得到的资源的位置和访问方法的一种简洁表示。URL相当于一个文件名在网络范围的扩展。 URL的一般形式是: ://:/。 常见的有http、ftp 等; 是存放资源的主机在因特网中的域名，也可以是IP地址; 和有时可以省略。 在URL中不区分大小写。 （3）万维网工作流程 万维网以客户/服务器方式工作。 浏览器是在用户计算机上的万维网客户程序，而万维网文档所驻留的计算机则运行服务器程序，这台计算机称万维网服务器。 客户程序向服务器程序发出请求，服务器程序向客户程序送回客户所要的文档。工作流程如下: ① Web 用户使用浏览器(指定URL)与Web服务器建立连接，并发送浏览请求。 ② Web服务器把URL转换为文件路径，并返回信息给Web浏览器。 ③ 通信完成，关闭连接。 2.HTTP—超文本传输协议 HTTP定义了浏览器(万维网客户进程)怎样向万维网服务器请求万维网文档，以及服务器怎样把文档传送给浏览器。 从层次的角度看，HTTP是面向事务的(Transaction-oriented) 应用层 协议，它规定了在浏览器和服务器之间的请求和响应的格式与规则，是万维网上能够可靠地交换文件(包括文本、声音、图像等各种多媒体文件)的重要基础。 （1）HTTP操作过程 在浏览器和服务器之间的请求与响应的交互，必须遵循规定的格式和规则，这些格式和规则就是HTTP。 因此HTTP有两类报文: 请求报文(从Web客户端向Web服务器发送服务请求) 响应报文(从Web服务器对Web客户端请求的回答)。 从协议执行过程来说，浏览器要访问WWW服务器时，首先要完成对www服务器的域名解析。 一旦获得了服务器的IP地址，浏览器就通过TCP向服务器发送连接建立请求。万维网的大致工作过程如图所示 每个万维网站点都有一个服务器进程，它不断地监听TCP的端口80(默认)，当监听到连接请求后便与浏览器建立连接。 TCP连接建立后, 浏览器就向服务器发送请求获取某个Web页面的HTTP请求。 服务器收到HTTP请求后，将构建所请求Web页的必需信息，并通过HTTP响应返回给浏览器。 浏览器再将信息进行解释, .然后将Web页显示给用户。 最后，TCP连接释放。 （2）HTTP特点 HTTP是无状态的。也就是说，同一个客户第二次访问同一个服务器上的页面时，服务器的响应与第一次被访问时的相同。 因为服务器并不记得曾经访问过的这个客户，也不记得为该客户曾经服务过多少次。 HTTP的无状态特性简化了服务器的设计，使服务器更容易支持大量并发的HTTP请求。 在实际应用中，通常使用Cookie 加数据库的方式来跟踪用户的活动(如记录用户最近浏览的商品等)。 Cookie 是一个存储在用户主机中的文本文件，里面含有一-串“识别码”，如“123456”，用于Web服务识别用户。 Web服务器根据Cookie就能从数据库中查询到该用户的活动记录，进而执行一些个性化的工作，如根据用户之前浏览过的商品向其推荐新产品等。 HTTP采用TCP作为运输层协议，保证了数据的可靠传输。 HTTP不必考虑数据在传输过程中被丢弃后又怎样被重传。 但是，HTTP本身是无连接的。也就是说，虽然HTTP使用了TCP连接，但通信的双方在交换HTTP报文之前不需要先建立HTTP连接。 HTTP既可以使用非持久连接，也可以使用持久连接(HTTP/1.1支持)。 对于非持久连接，每个网页元素对象(如JPEG图形、Flash 等)的传输都需要单独建立一个TCP连接，如图6.12所示(第三次握手的报文段中捎带了客户对万维网文档的请求)。 也就是说，请求一个万维网文档所需的时间是该文档的传输时间(与文档大小成正比)加上两倍往返时间RTT(一个RTT用于TCP连接，另一个RTT用于请求和接收文档)。 持久连接，是指万维网服务器在发送响应后仍然保持这条连接，使同一个客户和服务器可以继续在这条连接上传送后续的HTTP请求与响应报文，如图6.13所示。 持久连接又分为非流水线和流水线两种方式。 对于非流水线方式，客户在收到前一个响应后才能发出下一个请求; HTTP/1.1 的默认方式是使用流水线的持久连接。这种情况下，客户每遇到一个对象引用就立即发出一个请求，因而客户可以逐个地连续发出对各个引用对象的请求。如果所有的请求和响应都是连续发送的，那么所有引用的对象共计经历1个RTT延迟，而不是像非流水线方式那样，每个引用都必须有1个RTT延迟。 （3）HTTP报文结构 HTTP是面向文本的(Text-Oriented)， 因此报文中的每个字段都是一一些ASCII码串，并且每个字段的长度都是不确定的。有两类HTTP报文: ●请求报文: 从客户向服务器发送的请求报文 ●响应报文: 从服务器到客户的回答 HTTP请求报文和响应报文都由三个部分组成。 这两种报文格式的区别是开始行不同。 开始行:用于区分是请求报文还是响应报文。 在请求报文中的开始行称为请求行； 响应报文中的开始行称为状态行； 开始行的三个字段之间都以空格分隔，最后的“CR\"和“LF”分别代表“回车”和“换行”。 请求报文的“请求行”有三个内容:方法、请求资源的URL及HTTP的版本 其中，“方法”是对所请求对象进行的操作，这些方法实际上也就是一些命令。 首部行:用来说明浏览器、服务器或报文主体的一些信息。 首部可有几行，但可不使用。 在每个首部行都有首部字段名和它的值，每行在结束的地方都要有“回车”和\"换行”。 整个首部结束时，还有一空行将首部行和后面的实体主体分开。 实体主体:在请求报文中一般不用这个字段，而在响应报文中也可能没有这个字段。 3.常用应用程序的协议及端口号 四 FTP 文件传输协议 1.功能 文件传输协议( File Transfer Protocol, FTP)是因特网上使用得最广泛的文件传输协议。 FTP提供交互式的访问，允许客户指明文件的类型与格式，并允许文件具有存取权限。 它屏蔽了各计算机系统的细节，因而适合于在异构网络中的任意计算机之间传送文件。 FTP提供以下功能: ①提供不同种类主机系统(硬、软件体系等都可以不同)之间的文件传输能力。 ②以用户权限管理的方式提供用户对远程FTP服务器上的文件管理能力。 ③以匿名FTP的方式提供公用文件共享的能力。 2.工作原理 FTP采用客户/服务器（C/S）的工作方式，它使用TCP可靠的传输服务。 一个FTP服务器进程可同时为多个客户进程提供服务。 依照FTP协议提供服务，进行文件传送的计算机就是FTP服务器。 连接FTP服务器，遵循FTP协议与服务器传送文件的电脑就是FTP客户端。 FTP的服务器进程由两大部分组成: 一个主进程，负责接收新的请求; 若干从属进程，负责处理单个请求。 其工作步骤如下: ①打开熟知端口21 (控制端口)，使客户进程能够连接上。 ②等待客户进程发连接请求。 ③启动从属进程来处理客户进程发来的请求。主进程与从属进程并发执行，从属进程对客户进程的请求处理完毕后即终止。 ④回到等待状态，继续接收其他客户进程的请求。 FTP服务器必须在整个会话期间保留用户的状态信息。 特别是服务器必须把指定的用户账户与控制连接联系起来，服务器必须追踪用户在远程目录树上的当前位置。 3.控制连接和数据连接 FTP在工作时使用两个并行的TCP连接: 一个是控制连接(端口号21)， 一个是数据连接(端口号20)。 使用两个不同的端口号可使协议更加简单和更容易实现。 1.控制连接 服务器监听21号端口，等待客户连接，建立在这个端口.上的连接称为控制连接，控制连接用来传输控制信息(如连接请求、传送请求等)，并且控制信息都以7位ASCII格式传送。 FTP客户发出的传送请求，通过控制连接发送给服务器端的控制进程,但控制连接并不用来传送文件。 在传输文件时还可以使用控制连接(如客户在传输中途发一个中止传输的命令)，因此控制连接在整个会话期间一直保持打开状态。 2. 数据连接 服务器端的控制进程在接收到FTP客户发来的文件传输请求后，就创建“数据传送进程”和“数据连接”。 数据连接用来连接客户端和服务器端的数据传送进程，数据传送进程实际完成文件的传送，在传送完毕后关闭“数据传送连接”并结束运行。 因为FTP使用了一个分离的控制连接，所以也称FTP的控制信息是带外(Out-of-band) 传送的。 使用FTP时，若要修改服务器上的文件，则需要先将此文件传送到本地主机,然后再将修改后的文件副本传送到原服务器。 网络文件系统(NFS)允许进程打开一个远程文件，并在该文件的某个特定位置开始读写数据。这样，NFS可使用户复制一个大文件中的一个很小的片段，而不需要复制整个大文件。 "},"计算机网络/10.HTTP相关协议.html":{"url":"计算机网络/10.HTTP相关协议.html","title":"8.HTTP相关及发展","keywords":"","body":"1.HTTP相关知识1.HTTP长连接和短连接2.HTTP缓存2.HTTPS1.加密算法：2.详解总结3.HTTP1.0、HTTP1.1 和 HTTP2.0 的区别一、HTTP的历史二、HTTP的基本优化三、HTTP1.0和HTTP1.1的一些区别四、HTTPS与HTTP的一些区别五、SPDY：HTTP1.x的优化六、HTTP2.0性能惊人七、HTTP2.0：SPDY的升级版八、HTTP2.0和HTTP1.X相比的新特性九、HTTP2.0的升级改造十、附注1.HTTP相关知识 1.HTTP长连接和短连接 什么是长连接 HTTP长短连接的区别在于使用的TCP的长连接还是短连接。 在HTTP 1.0中默认使用的是短连接，而从HTTP 1.1 之后默认的连接都变为长连接。长短连接的区别?本质上是有TCP连接来决定的，为什么这么说呢?因为TCP是一个向双通道，他可以保持一段时间不关闭，这样就有了长连接和短连接的区别了。比方说：在数据的传输完成后，保持TCP连接不中断，等待相同域名再次请求时，继续使用这个TCP连接通道进行数据传输。这个就是长连接。 举个例子吧，比如你需要邮件一个东西给你的朋友，HTTP协议指的就是你需要填写的那个快递单，你寄件的时候填写的那个快递单的动作就相当于进行了一次HTTP请求。而你的快递需要通过交通运输工具来运送吧，可以是货车、货车、高铁、飞机等等。而TCP协议指的就是那个运送快递的运输工具。因为需要运输啊，就需要有道路啊，地上是公路或者铁路，天上是航线。那么这个运输道路就是TCP连接，因为这个道路是双向的，可以运过来也可以运过去。所以，TCP连接也叫作双向的数据通道。 如果这个道路长时间都有车来运送货物，那就叫作长连接。如果运送一段时间后，需要休整没有车经过了，那么就叫做短连接。 因此我们可以看出来，所谓的HTTP连接指的就是TCP的连接。TCP连接是可以保持一段时间不中断的就是长连接，发起一次请求后就主动断开的就是短连接，所以就有了长连接和短连接一说。 那么问题来了，怎么样的连接才能被称之为 ----长连接呢? 长连接的条件： 需要将HTTP 的头部，Connection设置为 keep-alive,但是这里有一个问题出现了，是不是只需要设置Connection 为 keep-alive就算是长连接了呢?这个问题很明显，当然不是的，你需要在服务器和客户端都要进行设置。 第二， 我们日常生活中所用的HTTP请求是不是长连接呢?答案很明显，当然是的了。因为现在的HTTP使用的都是1.1协议了，你细心观察一下就会发现，它的Connection 都是设置为keep-alive. 那么现在你是不是对keep-alive，很好奇啊。什么是Connection 设置为keep-alive 呢?接下来，我就带你们来揭开它神秘的面纱。 Keep-alive 又为何方神圣? 我们知道啊，HTTP协议采用的都是“请求-应答”模式，当Connection 为非keep-alive模式，则每一次的 请求-应答任务，服务器和客户端都需要重新建立一次连接。任务完成后，断开连接。而当Connection为keep-alive模式，会使服务器与客户端一直保持连接的状态。当再有新的请求任务发生时，就不需要重新建立。节约时间，也不用耗费资源。 有上面的图可以看出来： 短连接的步骤： 长连接的步骤： 接下来我们就聊一聊 长连接和短连接分别在什么场景下使用? 长连接使用场景：长连接多用于频繁操作，多次请求的网络应答响应，而且是一对一，点对点的通信。 例如：数据库的连接用长连接，如果使用短连接频繁的应答响应会造成socket报错，同时也会浪费资源。 短连接的使用场景：短连接怎不会那么耗费资源，因为不需要长时间占用TCP连接。因此，像Web网站中的http服务一般都是用的短连接。因为长连接会占用一定的资源，而像淘宝，京东等网站这样频繁的被用户访问，赶上双十一都是上亿规模的访问量，如果使用长连接，每个用户都占用一个TCP连接通道，那么服务器的压力可想而知。所以，这种情况下 使用短连接效果比较好。 2.HTTP缓存 Web 缓存大致可以分为：数据库缓存、服务器端缓存（代理服务器缓存、CDN 缓存）、浏览器缓存。 浏览器缓存也包含很多内容： HTTP 缓存、indexDB、cookie、localstorage 等等。这里我们只讨论 HTTP 缓存相关内容。 在具体了解 HTTP 缓存之前先来明确几个术语： 缓存命中率：从缓存中得到数据的请求数与所有请求数的比率。理想状态是越高越好。 过期内容：超过设置的有效时间，被标记为“陈旧”的内容。通常过期内容不能用于回复客户端的请求，必须重新向源服务器请求新的内容或者验证缓存的内容是否仍然准备。 验证：验证缓存中的过期内容是否仍然有效，验证通过的话刷新过期时间。 失效：失效就是把内容从缓存中移除。当内容发生改变时就必须移除失效的内容。 浏览器缓存主要是 HTTP 协议定义的缓存机制。HTML meta 标签，例如 含义是让浏览器不缓存当前页面。但是代理服务器不解析 HTML 内容，一般应用广泛的是用 HTTP 头信息控制缓存。 浏览器缓存分类 浏览器缓存分为强缓存和协商缓存，浏览器加载一个页面的简单流程如下： 浏览器先根据这个资源的http头信息来判断是否命中强缓存。如果命中则直接加在缓存中的资源，并不会将请求发送到服务器。 如果未命中强缓存，则浏览器会将资源加载请求发送到服务器。服务器来判断浏览器本地缓存是否失效。若可以使用，则服务器并不会返回资源信息，浏览器继续从缓存加载资源。 如果未命中协商缓存，则服务器会将完整的资源返回给浏览器，浏览器加载新资源，并更新缓存。 强缓存 命中强缓存时，浏览器并不会将请求发送给服务器。在Chrome的开发者工具中看到http的返回码是200，但是在Size列会显示为(from cache)。 强缓存是利用http的返回头中的Expires或者Cache-Control两个字段来控制的，用来表示资源的缓存时间。 Expires 缓存过期时间，用来指定资源到期的时间，是服务器端的具体的时间点。也就是说，Expires=max-age + 请求时间，需要和Last-modified结合使用。但在上面我们提到过，cache-control的优先级更高。 Expires是Web服务器响应消息头字段，在响应http请求时告诉浏览器在过期时间前浏览器可以直接从浏览器缓存取数据，而无需再次请求。 该字段会返回一个时间，比如Expires:Thu,31 Dec 2037 23:59:59 GMT。这个时间代表着这个资源的失效时间，也就是说在2037年12月31日23点59分59秒之前都是有效的，即命中缓存。这种方式有一个明显的缺点，由于失效时间是一个绝对时间，所以当客户端本地时间被修改以后，服务器与客户端时间偏差变大以后，就会导致缓存混乱。于是发展出了Cache-Control。 Cache-Control Cache-Control是一个相对时间，例如Cache-Control:3600，代表着资源的有效期是3600秒。由于是相对时间，并且都是与客户端时间比较，所以服务器与客户端时间偏差也不会导致问题。 Cache-Control与Expires可以在服务端配置同时启用或者启用任意一个，同时启用的时候Cache-Control优先级高。 Cache-Control 可以由多个字段组合而成，主要有以下几个取值： max-age 指定一个时间长度，在这个时间段内缓存是有效的，单位是s。例如设置 Cache-Control:max-age=31536000，也就是说缓存有效期为（31536000 / 24 / 60 * 60）天，第一次访问这个资源的时候，服务器端也返回了 Expires 字段，并且过期时间是一年后。 在没有禁用缓存并且没有超过有效时间的情况下，再次访问这个资源就命中了缓存，不会向服务器请求资源而是直接从浏览器缓存中取。 s-maxage 同 max-age，覆盖 max-age、Expires，但仅适用于共享缓存，在私有缓存中被忽略。 public 表明响应可以被任何对象（发送请求的客户端、代理服务器等等）缓存。 private 表明响应只能被单个用户（可能是操作系统用户、浏览器用户）缓存，是非共享的，不能被代理服务器缓存。 no-cache 强制所有缓存了该响应的用户，在使用已缓存的数据前，发送带验证器的请求到服务器。不是字面意思上的不缓存。 no-store 禁止缓存，每次请求都要向服务器重新获取数据。 7、must-revalidate指定如果页面是过期的，则去服务器进行获取。这个指令并不常用，就不做过多的讨论了。 协商缓存 若未命中强缓存，则浏览器会将请求发送至服务器。服务器根据http头信息中的Last-Modify/If-Modify-Since或Etag/If-None-Match来判断是否命中协商缓存。如果命中，则http返回码为304，浏览器从缓存中加载资源。 Last-Modify/If-Modify-Since 浏览器第一次请求一个资源的时候，服务器返回的header中会加上Last-Modify，Last-modify是一个时间标识该资源的最后修改时间，例如Last-Modify: Thu,31 Dec 2037 23:59:59 GMT。 当浏览器再次请求该资源时，发送的请求头中会包含If-Modify-Since，该值为缓存之前返回的Last-Modify。服务器收到If-Modify-Since后，根据资源的最后修改时间判断是否命中缓存。 如果命中缓存，则返回http304，并且不会返回资源内容，并且不会返回Last-Modify。由于对比的服务端时间，所以客户端与服务端时间差距不会导致问题。但是有时候通过最后修改时间来判断资源是否修改还是不太准确（资源变化了最后修改时间也可以一致）。于是出现了ETag/If-None-Match。 ETag/If-None-Match 与Last-Modify/If-Modify-Since不同的是，Etag/If-None-Match返回的是一个校验码（ETag: entity tag）。ETag可以保证每一个资源是唯一的，资源变化都会导致ETag变化。ETag值的变更则说明资源状态已经被修改。服务器根据浏览器上发送的If-None-Match值来判断是否命中缓存。 ETag扩展说明 我们对ETag寄予厚望，希望它对于每一个url生成唯一的值，资源变化时ETag也发生变化。神秘的Etag是如何生成的呢？以Apache为例，ETag生成靠以下几种因子 文件的i-node编号，此i-node非彼iNode。是Linux/Unix用来识别文件的编号。是的，识别文件用的不是文件名。使用命令’ls –I’可以看到。 文件最后修改时间 文件大小 生成Etag的时候，可以使用其中一种或几种因子，使用抗碰撞散列函数来生成。所以，理论上ETag也是会重复的，只是概率小到可以忽略。 既生Last-Modified何生Etag？ 你可能会觉得使用Last-Modified已经足以让浏览器知道本地的缓存副本是否足够新，为什么还需要Etag（实体标识）呢？HTTP1.1中Etag的出现主要是为了解决几个Last-Modified比较难解决的问题： Last-Modified标注的最后修改只能精确到秒级，如果某些文件在1秒钟以内，被修改多次的话，它将不能准确标注文件的修改时间 如果某些文件会被定期生成，当有时内容并没有任何变化，但Last-Modified却改变了，导致文件没法使用缓存 3.有可能存在服务器没有准确获取文件修改时间，或者与代理服务器时间不一致等情形 Etag是服务器自动生成或者由开发者生成的对应资源在服务器端的唯一标识符，能够更加准确的控制缓存。Last-Modified与ETag是可以一起使用的，服务器会优先验证ETag，一致的情况下，才会继续比对Last-Modified，最后才决定是否返回304。 用户行为与缓存 浏览器缓存行为还有用户的行为有关！！！ 用户操作 Expires/Cache-Control Last-Modified/Etag 地址栏回车 有效 有效 页面链接跳转 有效 有效 新开窗口 有效 有效 前进、后退 有效 有效 F5刷新 无效 有效 Ctrl+F5刷新 无效 无效 总结: 浏览器第一次请求： 浏览器再次请求时： 2.HTTPS 1、HTTP 协议（HyperText Transfer Protocol，超文本传输协议）：是客户端浏览器或其他程序与Web服务器之间的应用层通信协议 。 2、HTTPS 协议（HyperText Transfer Protocol over Secure Socket Layer）：可以理解为HTTP+SSL/TLS， 即 HTTP 下加入 SSL 层，HTTPS 的安全基础是 SSL，因此加密的详细内容就需要 SSL，用于安全的 HTTP 数据传输。 如上图所示 HTTPS 相比 HTTP 多了一层 SSL/TLS SSL（Secure Socket Layer，安全套接字层）：1994年为 Netscape 所研发，SSL 协议位于 TCP/IP 协议与各种应用层协议之间，为数据通讯提供安全支持。 TLS（Transport Layer Security，传输层安全）：其前身是 SSL，它最初的几个版本（SSL 1.0、SSL 2.0、SSL 3.0）由网景公司开发，1999年从 3.1 开始被 IETF 标准化并改名，发展至今已经有 TLS 1.0、TLS 1.1、TLS 1.2 三个版本。SSL3.0和TLS1.0由于存在安全漏洞，已经很少被使用到。TLS 1.3 改动会比较大，目前还在草案阶段，目前使用最广泛的是TLS 1.1、TLS 1.2。 1.加密算法： 1、对称加密 有流式、分组两种，加密和解密都是使用的同一个密钥。 例如：DES、AES-GCM、ChaCha20-Poly1305等 2、非对称加密 加密使用的密钥和解密使用的密钥是不相同的，分别称为：公钥、私钥，公钥和算法都是公开的，私钥是保密的。非对称加密算法性能较低，但是安全性超强，由于其加密特性，非对称加密算法能加密的数据长度也是有限的。 例如：RSA、DSA、ECDSA、 DH、ECDHE 3、哈希算法 将任意长度的信息转换为较短的固定长度的值，通常其长度要比信息小得多，且算法不可逆。 例如：MD5、SHA-1、SHA-2、SHA-256 等 4、数字签名 签名就是在信息的后面再加上一段内容（信息经过hash后的值），可以证明信息没有被修改过。hash值一般都会加密后（也就是签名）再和信息一起发送，以保证这个hash值不被修改。 2.详解 一、HTTP 访问过程 抓包如下： 如上图所示，HTTP请求过程中，客户端与服务器之间没有任何身份确认的过程，数据全部明文传输，“裸奔”在互联网上，所以很容易遭到黑客的攻击，如下： 可以看到，客户端发出的请求很容易被黑客截获，如果此时黑客冒充服务器，则其可返回任意信息给客户端，而不被客户端察觉，所以我们经常会听到一词“劫持”，现象如下： 下面两图中，浏览器中填入的是相同的URL，左边是正确响应，而右边则是被劫持后的响应 所以 HTTP 传输面临的风险有： （1） 窃听风险：黑客可以获知通信内容。 （2） 篡改风险：黑客可以修改通信内容。 （3） 冒充风险：黑客可以冒充他人身份参与通信。 二、HTTP 向 HTTPS 演化的过程 第一步：为了防止上述现象的发生，人们想到一个办法：对传输的信息加密（即使黑客截获，也无法破解） 如上图所示，此种方式属于对称加密，双方拥有相同的密钥，信息得到安全传输，但此种方式的缺点是： （1）不同的客户端、服务器数量庞大，所以双方都需要维护大量的密钥，维护成本很高 （2）因每个客户端、服务器的安全级别不同，密钥极易泄露 第二步：既然使用对称加密时，密钥维护这么繁琐，那我们就用非对称加密试试 如上图所示，客户端用公钥对请求内容加密，服务器使用私钥对内容解密，反之亦然，但上述过程也存在缺点： （1）公钥是公开的（也就是黑客也会有公钥），所以第 ④ 步私钥加密的信息，如果被黑客截获，其可以使用公钥进行解密，获取其中的内容 第三步：非对称加密既然也有缺陷，那我们就将对称加密，非对称加密两者结合起来，取其精华、去其糟粕，发挥两者的各自的优势 如上图所示 （1）第 ③ 步时，客户端说：（咱们后续回话采用对称加密吧，这是对称加密的算法和对称密钥）这段话用公钥进行加密，然后传给服务器 （2）服务器收到信息后，用私钥解密，提取出对称加密算法和对称密钥后，服务器说：（好的）对称密钥加密 （3）后续两者之间信息的传输就可以使用对称加密的方式了 遇到的问题： （1）客户端如何获得公钥 （2）如何确认服务器是真实的而不是黑客 第四步：获取公钥与确认服务器身份 1、获取公钥 （1）提供一个下载公钥的地址，回话前让客户端去下载。（缺点：下载地址有可能是假的；客户端每次在回话前都先去下载公钥也很麻烦） （2）回话开始时，服务器把公钥发给客户端（缺点：黑客冒充服务器，发送给客户端假的公钥） 2、那有木有一种方式既可以安全的获取公钥，又能防止黑客冒充呢？ 那就需要用到终极武器了：SSL 证书（申购） 如上图所示，在第 ② 步时服务器发送了一个SSL证书给客户端，SSL 证书中包含的具体内容有： （1）证书的发布机构CA （2）证书的有效期 （3）公钥 （4）证书所有者 （5）签名 ……… 3、客户端在接受到服务端发来的SSL证书时，会对证书的真伪进行校验，以浏览器为例说明如下： （1）首先浏览器读取证书中的证书所有者、有效期等信息进行一一校验 （2）浏览器开始查找操作系统中已内置的受信任的证书发布机构CA，与服务器发来的证书中的颁发者CA比对，用于校验证书是否为合法机构颁发 （3）如果找不到，浏览器就会报错，说明服务器发来的证书是不可信任的。 （4）如果找到，那么浏览器就会从操作系统中取出 颁发者CA 的公钥，然后对服务器发来的证书里面的签名进行解密 （5）浏览器使用相同的hash算法计算出服务器发来的证书的hash值，将这个计算的hash值与证书中签名做对比 （6）对比结果一致，则证明服务器发来的证书合法，没有被冒充 （7）此时浏览器就可以读取证书中的公钥，用于后续加密了 4、所以通过发送SSL证书的形式，既解决了公钥获取问题，又解决了黑客冒充问题，一箭双雕，HTTPS加密过程也就此形成 所以相比HTTP，HTTPS 传输更加安全 （1） 所有信息都是加密传播，黑客无法窃听。 （2） 具有校验机制，一旦被篡改，通信双方会立刻发现。 （3） 配备身份证书，防止身份被冒充。 总结 综上所述，相比 HTTP 协议，HTTPS 协议增加了很多握手、加密解密等流程，虽然过程很复杂，但其可以保证数据传输的安全。所以在这个互联网膨胀的时代，其中隐藏着各种看不见的危机，为了保证数据的安全，维护网络稳定，建议大家多多推广HTTPS。 3.HTTP1.0、HTTP1.1 和 HTTP2.0 的区别 一、HTTP的历史 版本 产生时间 内容 发展现状 HTTP/0.9 1991年 不涉及数据包传输，规定客户端和服务器之间通信格式，只能GET请求 没有作为正式的标准 HTTP/1.0 1996年 传输内容格式不限制，增加PUT、PATCH、HEAD、 OPTIONS、DELETE命令 正式作为标准 HTTP/1.1 1997年 持久连接(长连接)、节约带宽、HOST域、管道机制、分块传输编码 2015年前使用最广泛 HTTP/2 2015年 多路复用、服务器推送、头信息压缩、二进制协议等 逐渐覆盖市场 早在 HTTP 建立之初，主要就是为了将超文本标记语言(HTML)文档从Web服务器传送到客户端的浏览器。也是说对于前端来说，我们所写的HTML页面将要放在我们的 web 服务器上，用户端通过浏览器访问url地址来获取网页的显示内容，但是到了 WEB2.0 以来，我们的页面变得复杂，不仅仅单纯的是一些简单的文字和图片，同时我们的 HTML 页面有了 CSS，Javascript，来丰富我们的页面展示，当 ajax 的出现，我们又多了一种向服务器端获取数据的方法，这些其实都是基于 HTTP 协议的。同样到了移动互联网时代，我们页面可以跑在手机端浏览器里面，但是和 PC 相比，手机端的网络情况更加复杂，这使得我们开始了不得不对 HTTP 进行深入理解并不断优化过程中。 二、HTTP的基本优化 影响一个 HTTP 网络请求的因素主要有两个：带宽和延迟。 带宽：如果说我们还停留在拨号上网的阶段，带宽可能会成为一个比较严重影响请求的问题，但是现在网络基础建设已经使得带宽得到极大的提升，我们不再会担心由带宽而影响网速，那么就只剩下延迟了。 延迟： 浏览器阻塞（HOL blocking）：浏览器会因为一些原因阻塞请求。浏览器对于同一个域名，同时只能有 4 个连接（这个根据浏览器内核不同可能会有所差异），超过浏览器最大连接数限制，后续请求就会被阻塞。 DNS 查询（DNS Lookup）：浏览器需要知道目标服务器的 IP 才能建立连接。将域名解析为 IP 的这个系统就是 DNS。这个通常可以利用DNS缓存结果来达到减少这个时间的目的。 建立连接（Initial connection）：HTTP 是基于 TCP 协议的，浏览器最快也要在第三次握手时才能捎带 HTTP 请求报文，达到真正的建立连接，但是这些连接无法复用会导致每次请求都经历三次握手和慢启动。三次握手在高延迟的场景下影响较明显，慢启动则对文件类大请求影响较大。 三、HTTP1.0和HTTP1.1的一些区别 HTTP1.0最早在网页中使用是在1996年，那个时候只是使用一些较为简单的网页上和网络请求上，而HTTP1.1则在1999年才开始广泛应用于现在的各大浏览器网络请求中，同时HTTP1.1也是当前使用最为广泛的HTTP协议。 主要区别主要体现在： 缓存处理，在HTTP1.0中主要使用header里的If-Modified-Since,Expires来做为缓存判断的标准，HTTP1.1则引入了更多的缓存控制策略例如Entity tag，If-Unmodified-Since, If-Match, If-None-Match等更多可供选择的缓存头来控制缓存策略。 带宽优化及网络连接的使用，HTTP1.0中，存在一些浪费带宽的现象，例如客户端只是需要某个对象的一部分，而服务器却将整个对象送过来了，并且不支持断点续传功能，HTTP1.1则在请求头引入了range头域，它允许只请求资源的某个部分，即返回码是206（Partial Content），这样就方便了开发者自由的选择以便于充分利用带宽和连接。 错误通知的管理，在HTTP1.1中新增了24个错误状态响应码，如409（Conflict）表示请求的资源与资源的当前状态发生冲突；410（Gone）表示服务器上的某个资源被永久性的删除。 Host头处理，在HTTP1.0中认为每台服务器都绑定一个唯一的IP地址，因此，请求消息中的URL并没有传递主机名（hostname）。但随着虚拟主机技术的发展，在一台物理服务器上可以存在多个虚拟主机（Multi-homed Web Servers），并且它们共享一个IP地址。HTTP1.1的请求消息和响应消息都应支持Host头域，且请求消息中如果没有Host头域会报告一个错误（400 Bad Request）。 长连接，HTTP 1.1支持长连接（PersistentConnection）和请求的流水线（Pipelining）处理，在一个TCP连接上可以传送多个HTTP请求和响应，减少了建立和关闭连接的消耗和延迟，在HTTP1.1中默认开启Connection： keep-alive，一定程度上弥补了HTTP1.0每次请求都要创建连接的缺点。 四、HTTPS与HTTP的一些区别 HTTPS协议需要到CA申请证书，一般免费证书很少，需要交费。 HTTP协议运行在TCP之上，所有传输的内容都是明文，HTTPS运行在SSL/TLS之上，SSL/TLS运行在TCP之上，所有传输的内容都经过加密的。 HTTP和HTTPS使用的是完全不同的连接方式，用的端口也不一样，前者是80，后者是443。 HTTPS可以有效的防止运营商劫持，解决了防劫持的一个大问题。 五、SPDY：HTTP1.x的优化 2012年google如一声惊雷提出了SPDY的方案，优化了HTTP1.X的请求延迟，解决了HTTP1.X的安全性，具体如下： 降低延迟，针对HTTP高延迟的问题，SPDY优雅的采取了多路复用（multiplexing）。多路复用通过多个请求stream共享一个tcp连接的方式，解决了 blocking的问题，降低了延迟同时提高了带宽的利用率。 请求优先级（request prioritization）。多路复用带来一个新的问题是，在连接共享的基础之上有可能会导致关键请求被阻塞。SPDY允许给每个request设置优先级，这样重要的请求就会优先得到响应。比如浏览器加载首页，首页的html内容应该优先展示，之后才是各种静态资源文件，脚本文件等加载，这样可以保证用户能第一时间看到网页内容。 header压缩。前面提到HTTP1.x的header很多时候都是重复多余的。选择合适的压缩算法可以减小包的大小和数量。 基于HTTPS的加密协议传输，大大提高了传输数据的可靠性。 服务端推送（server push），采用了SPDY的网页，例如我的网页有一个sytle.css的请求，在客户端收到sytle.css数据的同时，服务端会将sytle.js的文件推送给客户端，当客户端再次尝试获取sytle.js时就可以直接从缓存中获取到，不用再发请求了。SPDY构成图： SPDY位于HTTP之下，TCP和SSL之上，这样可以轻松兼容老版本的HTTP协议(将HTTP1.x的内容封装成一种新的frame格式)，同时可以使用已有的SSL功能。 六、HTTP2.0性能惊人 HTTP/2: the Future of the Internet https://link.zhihu.com/?target=https://http2.akamai.com/demo 是 Akamai 公司建立的一个官方的演示，用以说明 HTTP/2 相比于之前的 HTTP/1.1 在性能上的大幅度提升。 同时请求 379 张图片，从Load time 的对比可以看出 HTTP/2 在速度上的优势。 七、HTTP2.0：SPDY的升级版 HTTP2.0可以说是SPDY的升级版（其实原本也是基于SPDY设计的），但是，HTTP2.0 跟 SPDY 仍有不同的地方，如下： HTTP2.0和SPDY的区别： HTTP2.0 支持明文 HTTP 传输，而 SPDY 强制使用 HTTPS HTTP2.0 消息头的压缩算法采用 HPACK http://http2.github.io/http2-spec/compression.html，而非 SPDY 采用的 DEFLATE http://zh.wikipedia.org/wiki/DEFLATE 八、HTTP2.0和HTTP1.X相比的新特性 新的二进制格式（Binary Format），HTTP1.x的解析是基于文本。基于文本协议的格式解析存在天然缺陷，文本的表现形式有多样性，要做到健壮性考虑的场景必然很多，二进制则不同，只认0和1的组合。基于这种考虑HTTP2.0的协议解析决定采用二进制格式，实现方便且健壮。 多路复用（MultiPlexing），即连接共享，即每一个request都是是用作连接共享机制的。一个request对应一个id，这样一个连接上可以有多个request，每个连接的request可以随机的混杂在一起，接收方可以根据request的 id将request再归属到各自不同的服务端请求里面。 header压缩，如上文中所言，对前面提到过HTTP1.x的header带有大量信息，而且每次都要重复发送，HTTP2.0使用encoder来减少需要传输的header大小，通讯双方各自cache一份header fields表，既避免了重复header的传输，又减小了需要传输的大小。 服务端推送（server push），同SPDY一样，HTTP2.0也具有server push功能。 九、HTTP2.0的升级改造 前文说了HTTP2.0其实可以支持非HTTPS的，但是现在主流的浏览器像chrome，firefox表示还是只支持基于 TLS 部署的HTTP2.0协议，所以要想升级成HTTP2.0还是先升级HTTPS为好。 当你的网站已经升级HTTPS之后，那么升级HTTP2.0就简单很多，如果你使用NGINX，只要在配置文件中启动相应的协议就可以了，可以参考NGINX白皮书，NGINX配置HTTP2.0官方指南 https://www.nginx.com/blog/nginx-1-9-5/。 使用了HTTP2.0那么，原本的HTTP1.x怎么办，这个问题其实不用担心，HTTP2.0完全兼容HTTP1.x的语义，对于不支持HTTP2.0的浏览器，NGINX会自动向下兼容的。 十、附注 HTTP2.0的多路复用和HTTP1.X中的长连接复用有什么区别？ HTTP/1.0 一次请求-响应，建立一个连接，用完关闭；每一个请求都要建立一个连接； HTTP/1.1 长连接,Pipeling解决方式为:先建立一个长连接,长连接可处理多个请求.若干个请求排队串行化单线程处理，后面的请求等待前面请求的返回才能获得执行机会，一旦有某请求超时等，后续请求只能被阻塞，毫无办法，也就是人们常说的线头阻塞； HTTP/2 引入二进制数据帧和流的概念，其中帧对数据进行顺序标识，这样浏览器收到数据之后，就可以按照序列对数据进行合并，而不会出现合并后数据错乱的情况。同样是因为有了序列，服务器就可以并行的传输数据。 HTTP/2 对同一域名下所有请求都是基于流，也就是说同一域名不管访问多少文件，也只建立一路连接。 服务器推送到底是什么？ 服务端推送能把客户端所需要的资源伴随着index.html一起发送到客户端，省去了客户端重复请求的步骤。正因为没有发起请求，建立连接等操作，所以静态资源通过服务端推送的方式可以极大地提升速度。具体如下： 普通的客户端请求过程： 服务端推送的过程： 为什么需要头部压缩？ 假定一个页面有100个资源需要加载（这个数量对于今天的Web而言还是挺保守的）, 而每一次请求都有1kb的消息头（这同样也并不少见，因为Cookie和引用等东西的存在）, 则至少需要多消耗100kb来获取这些消息头。HTTP2.0可以维护一个字典，差量更新HTTP头部，大大降低因头部传输产生的流量。具体参考：HTTP/2 头部压缩技术介绍 HTTP2.0多路复用有多好？ HTTP 性能优化的关键并不在于高带宽，而是低延迟。TCP 连接会随着时间进行自我「调谐」，起初会限制连接的最大速度，如果数据成功传输，会随着时间的推移提高传输的速度。这种调谐则被称为 TCP 慢启动。由于这种原因，让原本就具有突发性和短时性的 HTTP 连接变的十分低效。 HTTP/2 通过让所有数据流共用同一个连接，可以更有效地使用 TCP 连接，让高带宽也能真正的服务于 HTTP 的性能提升。 "},"计算机网络/9.面试题二.html":{"url":"计算机网络/9.面试题二.html","title":"面试题总结一","keywords":"","body":"1.谈下你对五层网络协议体系结构的理解? 2.简单说下每一层对应的网络协议有哪些? 3.ARP 协议的工作原理？ 4.谈下你对 IP 地址分类的理解? 5.TCP 的主要特点是什么？ 6.UDP 的主要特点是什么？ 7.TCP 和 UDP 的区别？ 8.TCP 和 UDP 分别对应的常见应用层协议有哪些? 9.详细说下 TCP 三次握手的过程？ 10.为什么两次握手不可以呢？ 11.为什么不需要四次握手？ 12.Server 端收到 Client 端的 SYN 后，为什么还要传回 SYN？ 13.传了 SYN，为什么还要传 ACK？ 14.详细说下 TCP 四次挥手的过程？ 15.为什么 TIME-WAIT 状态必须等待 2MSL 的时间呢？ 16.为什么第二次跟第三次不能合并, 第二次和第三次之间的等待是什么? 17.保活计时器的作用？ 18.TCP 协议是如何保证可靠传输的？ 19.谈谈你对停止等待协议的理解？ 20.谈谈你对 ARQ 协议的理解？ 21.谈谈你对滑动窗口的了解？ 22.、谈下你对流量控制的理解？ 23.谈下你对 TCP 拥塞控制的理解？ 使用了哪些算法？ 24.什么是粘包? 25.TCP 黏包是怎么产生的？ 26.怎么解决拆包和粘包？ 27.你对 HTTP 状态码有了解吗？ 28.forward 和 redirect 的区别？ 29.HTTP 方法有哪些？ 30.说下 GET 和 POST 的区别？ 31.在浏览器中输入 URL 地址到显示主页的过程 32.DNS 的解析过程？ 33.谈谈你对域名缓存的了解？ 34. 谈下你对 HTTP 长连接和短连接的理解? 分别应用于哪些场景？ 35.谈下 HTTP 1.0 和 1.1、1.2 的主要变化？ 36.HTTPS 的工作过程？ 37.HTTP 和 HTTPS 的区别？ 38.HTTPS 的优缺点？ 39.什么是数字签名? 40.什么是数字证书？ 41.什么是对称加密和非对称加密？ 1、谈下你对五层网络协议体系结构的理解？ 学习计算机网络时我们一般采用折中的办法，也就是中和 OSI 和 TCP/IP 的优点，采用一种只有五层协议的体系结构，这样既简洁又能将概念阐述清楚。 1. 应用层 应用层（application-layer）的任务是通过应用进程间的交互来完成特定网络应用。应用层协议定义的是应用进程（进程：主机中正在运行的程序）间的通信和交互的规则。对于不同的网络应用需要不同的应用层协议。在互联网中应用层协议很多，如域名系统 DNS，支持万维网应用的 HTTP 协议，支持电子邮件的 SMTP 协议等等。我们把应用层交互的数据单元称为报文。 2. 运输层 运输层（transport layer）的主要任务就是负责向两台主机进程之间的通信提供通用的数据传输服务。应用进程利用该服务传送应用层报文。“通用的”是指并不针对某一个特定的网络应用，而是多种应用可以使用同一个运输层服务。 由于一台主机可同时运行多个线程，因此运输层有复用和分用的功能。所谓复用就是指多个应用层进程可同时使用下面运输层的服务，分用和复用相反，是运输层把收到的信息分别交付上面应用层中的相应进程。 3. 网络层 在计算机网络中进行通信的两个计算机之间可能会经过很多个数据链路，也可能还要经过很多通信子网。网络层的任务就是选择合适的网间路由和交换结点， 确保数据及时传送。在发送数据时，网络层把运输层产生的报文段或用户数据报封装成分组和包进行传送。在 TCP / IP 体系结构中，由于网络层使用 IP 协议，因此分组也叫 IP 数据报，简称数据报。 4. 数据链路层 数据链路层（data link layer）通常简称为链路层。两台主机之间的数据传输，总是在一段一段的链路上传送的，这就需要使用专门的链路层的协议。在两个相邻节点之间传送数据时，数据链路层将网络层交下来的 IP 数据报组装成帧，在两个相邻节点间的链路上传送帧。每一帧包括数据和必要的控制信息（如：同步信息，地址信息，差错控制等）。 在接收数据时，控制信息使接收端能够知道一个帧从哪个比特开始和到哪个比特结束。这样，数据链路层在收到一个帧后，就可从中提出数据部分，上交给网络层。控制信息还使接收端能够检测到所收到的帧中有无差错。如果发现差错，数据链路层就简单地丢弃这个出了差错的帧，以避免继续在网络中传送下去白白浪费网络资源。如果需要改正数据在链路层传输时出现差错（这就是说，数据链路层不仅要检错，而且还要纠错），那么就要采用可靠性传输协议来纠正出现的差错。这种方法会使链路层的协议复杂些。 5. 物理层 在物理层上所传送的数据单位是比特。物理层（physical layer）的作用是实现相邻计算机节点之间比特流的透明传送，尽可能屏蔽掉具体传输介质和物理设备的差异。使其上面的数据链路层不必考虑网络的具体传输介质是什么。“透明传送比特流”表示经实际电路传送后的比特流没有发生变化，对传送的比特流来说，这个电路好像是看不见的。 2、简单说下每一层对应的网络协议有哪些？ 计算机五层网络体系中涉及的协议非常多，下面就常用的做了列举： 3、ARP 协议的工作原理？ 网络层的 ARP 协议完成了 IP 地址与物理地址的映射。首先，每台主机都会在自己的 ARP 缓冲区中建立一个 ARP 列表，以表示 IP 地址和 MAC 地址的对应关系。当源主机需要将一个数据包要发送到目的主机时，会首先检查自己 ARP 列表中是否存在该 IP 地址对应的 MAC 地址：如果有，就直接将数据包发送到这个 MAC 地址；如果没有，就向本地网段发起一个 ARP 请求的广播包，查询此目的主机对应的 MAC 地址。 此 ARP 请求数据包里包括源主机的 IP 地址、硬件地址、以及目的主机的 IP 地址。网络中所有的主机收到这个 ARP 请求后，会检查数据包中的目的 IP 是否和自己的 IP 地址一致。如果不相同就忽略此数据包；如果相同，该主机首先将发送端的 MAC 地址和 IP 地址添加到自己的 ARP 列表中，如果 ARP 表中已经存在该 IP 的信息，则将其覆盖，然后给源主机发送一个 ARP 响应数据包，告诉对方自己是它需要查找的 MAC 地址；源主机收到这个 ARP 响应数据包后，将得到的目的主机的 IP 地址和 MAC 地址添加到自己的 ARP 列表中，并利用此信息开始数据的传输。如果源主机一直没有收到 ARP 响应数据包，表示 ARP 查询失败。 4、谈下你对 IP 地址分类的理解？ IP 地址是指互联网协议地址，是 IP 协议提供的一种统一的地址格式，它为互联网上的每一个网络和每一台主机分配一个逻辑地址，以此来屏蔽物理地址的差异。IP 地址编址方案将 IP 地址空间划分为 A、B、C、D、E 五类，其中 A、B、C 是基本类，D、E 类作为多播和保留使用，为特殊地址。 每个 IP 地址包括两个标识码（ID），即网络 ID 和主机 ID。同一个物理网络上的所有主机都使用同一个网络 ID，网络上的一个主机（包括网络上工作站，服务器和路由器等）有一个主机 ID 与其对应。A~E 类地址的特点如下： A 类地址：以 0 开头，第一个字节范围：0~127； B 类地址：以 10 开头，第一个字节范围：128~191； C 类地址：以 110 开头，第一个字节范围：192~223； D 类地址：以 1110 开头，第一个字节范围为 224~239； E 类地址：以 1111 开头，保留地址 5、TCP 的主要特点是什么？ TCP 是面向连接的。（就好像打电话一样，通话前需要先拨号建立连接，通话结束后要挂机释放连接）； 每一条 TCP 连接只能有两个端点，每一条 TCP 连接只能是点对点的（一对一）； TCP 提供可靠交付的服务。通过 TCP 连接传送的数据，无差错、不丢失、不重复、并且按序到达； TCP 提供全双工通信。TCP 允许通信双方的应用进程在任何时候都能发送数据。TCP 连接的两端都设有发送缓存和接收缓存，用来临时存放双方通信的数据； 面向字节流。TCP 中的“流”（Stream）指的是流入进程或从进程流出的字节序列。“面向字节流”的含义是：虽然应用程序和 TCP 的交互是一次一个数据块（大小不等），但 TCP 把应用程序交下来的数据仅仅看成是一连串的无结构的字节流。 6、UDP 的主要特点是什么？ UDP 是无连接的； UDP 使用尽最大努力交付，即不保证可靠交付，因此主机不需要维持复杂的链接状态（这里面有许多参数）； UDP 是面向报文的,不会拆分应用层的数据； UDP 没有拥塞控制，因此网络出现拥塞不会使源主机的发送速率降低（对实时应用很有用，如 直播，实时视频会议等）； UDP 支持一对一、一对多、多对一和多对多的交互通信； UDP 的首部开销小，只有 8 个字节，比 TCP 的 20 个字节的首部要短。 7、TCP 和 UDP 的区别？ TCP 提供面向连接的服务。在传送数据之前必须先建立连接，数据传送结束后要释放连接。TCP 不提供广播或多播服务。由于 TCP 要提供可靠的，面向连接的运输服务（TCP 的可靠体现在 TCP 在传递数据之前，会有三次握手来建立连接，而且在数据传递时，有确认、窗口、重传、拥塞控制机制，在数据传完后，还会断开连接用来节约系统资源），这难以避免增加了许多开销，如确认，流量控制，计时器以及连接管理等。这不仅使协议数据单元的首部增大很多，还要占用许多处理机资源。 UDP 在传送数据之前不需要先建立连接，远地主机在收到 UDP 报文后，不需要给出任何确认。虽然 UDP 不提供可靠交付，但在某些情况下 UDP 确是一种最有效的工作方式（一般用于即时通信），比如：QQ 语音、 QQ 视频 、直播等等。 8、TCP 和 UDP 分别对应的常见应用层协议有哪些？ 1. TCP 对应的应用层协议 FTP：定义了文件传输协议，使用 21 端口。常说某某计算机开了 FTP 服务便是启动了文件传输服务。下载文件，上传主页，都要用到 FTP 服务。 Telnet：它是一种用于远程登陆的端口，用户可以以自己的身份远程连接到计算机上，通过这种端口可以提供一种基于 DOS 模式下的通信服务。如以前的 BBS 是-纯字符界面的，支持 BBS 的服务器将 23 端口打开，对外提供服务。 SMTP：定义了简单邮件传送协议，现在很多邮件服务器都用的是这个协议，用于发送邮件。如常见的免费邮件服务中用的就是这个邮件服务端口，所以在电子邮件设置-中常看到有这么 SMTP 端口设置这个栏，服务器开放的是 25 号端口。 POP3：它是和 SMTP 对应，POP3 用于接收邮件。通常情况下，POP3 协议所用的是 110 端口。也是说，只要你有相应的使用 POP3 协议的程序（例如 Fo-xmail 或 Outlook），就可以不以 Web 方式登陆进邮箱界面，直接用邮件程序就可以收到邮件（如是163 邮箱就没有必要先进入网易网站，再进入自己的邮-箱来收信）。 HTTP：从 Web 服务器传输超文本到本地浏览器的传送协议。 2. UDP 对应的应用层协议 DNS：用于域名解析服务，将域名地址转换为 IP 地址。DNS 用的是 53 号端口。 SNMP：简单网络管理协议，使用 161 号端口，是用来管理网络设备的。由于网络设备很多，无连接的服务就体现出其优势。 TFTP(Trival File Transfer Protocal)：简单文件传输协议，该协议在熟知端口 69 上使用 UDP 服务。 9、详细说下 TCP 三次握手的过程？ 1. 三次握手 TCP 建立连接的过程叫做握手，握手需要在客户和服务器之间交换三个 TCP 报文段。 最初客户端和服务端都处于 CLOSED(关闭) 状态。本例中 A（Client） 主动打开连接，B（Server） 被动打开连接。 一开始，B 的 TCP 服务器进程首先创建传输控制块TCB，准备接受客户端进程的连接请求。然后服务端进程就处于 LISTEN(监听) 状态，等待客户端的连接请求。如有，立即作出响应。 第一次握手：A 的 TCP 客户端进程也是首先创建传输控制块 TCB。然后，在打算建立 TCP 连接时，向 B 发出连接请求报文段，这时首部中的同步位 SYN=1，同时选择一个初始序号 seq = x。TCP 规定，SYN 报文段（即 SYN = 1 的报文段）不能携带数据，但要消耗掉一个序号。这时，TCP 客户进程进入 SYN-SENT（同步已发送）状态。 第二次握手：B 收到连接请求报文后，如果同意建立连接，则向 A 发送确认。在确认报文段中应把 SYN 位和 ACK 位都置 1，确认号是 ack = x + 1，同时也为自己选择一个初始序号 seq = y。请注意，这个报文段也不能携带数据，但同样要消耗掉一个序号。这时 TCP 服务端进程进入 SYN-RCVD（同步收到）状态。 第三次握手：TCP 客户进程收到 B 的确认后，还要向 B 给出确认。确认报文段的 ACK 置 1，确认号 ack = y + 1，而自己的序号 seq = x + 1。这时 ACK 报文段可以携带数据。但如果不携带数据则不消耗序号，这种情况下，下一个数据报文段的序号仍是 seq = x + 1。这时，TCP 连接已经建立，A 进入 ESTABLISHED（已建立连接）状态。 10、为什么两次握手不可以呢？ 为了防止已经失效的连接请求报文段突然又传送到了 B，因而产生错误。比如下面这种情况：A 发出的第一个连接请求报文段并没有丢失，而是在网路结点长时间滞留了，以致于延误到连接释放以后的某个时间段才到达 B。本来这是一个早已失效的报文段。但是 B 收到此失效的链接请求报文段后，就误认为 A 又发出一次新的连接请求。于是就向 A 发出确认报文段，同意建立连接。 对于上面这种情况，如果不进行第三次握手，B 发出确认后就认为新的运输连接已经建立了，并一直等待 A 发来数据。B 的许多资源就这样白白浪费了。 如果采用了三次握手，由于 A 实际上并没有发出建立连接请求，所以不会理睬 B 的确认，也不会向 B 发送数据。B 由于收不到确认，就知道 A 并没有要求建立连接。 11、为什么不需要四次握手？ 有人可能会说 A 发出第三次握手的信息后在没有接收到 B 的请求就已经进入了连接状态，那如果 A 的这个确认包丢失或者滞留了怎么办？ 我们需要明白一点，完全可靠的通信协议是不存在的。在经过三次握手之后，客户端和服务端已经可以确认之前的通信状况，都收到了确认信息。所以即便再增加握手次数也不能保证后面的通信完全可靠，所以是没有必要的。 12、Server 端收到 Client 端的 SYN 后，为什么还要传回 SYN？ 接收端传回发送端所发送的 SYN 是为了告诉发送端，我接收到的信息确实就是你所发送的信号了。 SYN 是 TCP / IP 建立连接时使用的握手信号。在客户机和服务器之间建立正常的 TCP 网络连接时，客户机首先发出一个 SYN 消息，服务器使用 SYN-ACK 应答表示接收到了这个消息，最后客户机再以 ACK(Acknowledgement[汉译：确认字符，在数据通信传输中，接收站发给发送站的一种传输控制字符。它表示确认发来的数据已经接受无误]）消息响应。这样在客户机和服务器之间才能建立起可靠的 TCP 连接，数据才可以在客户机和服务器之间传递。 13、传了 SYN，为什么还要传 ACK？ 双方通信无误必须是两者互相发送信息都无误。传了 SYN，证明发送方到接收方的通道没有问题，但是接收方到发送方的通道还需要 ACK 信号来进行验证。 14、详细说下 TCP 四次挥手的过程？ 据传输结束后，通信的双方都可以释放连接。现在 A 和 B 都处于 ESTABLISHED 状态。 第一次挥手：A 的应用进程先向其 TCP 发出连接释放报文段，并停止再发送数据，主动关闭 TCP 连接。A 把连接释放报文段首部的终止控制位 FIN 置 1，其序号 seq = u（等于前面已传送过的数据的最后一个字节的序号加 1），这时 A 进入 FIN-WAIT-1（终止等待1）状态，等待 B 的确认。请注意：TCP 规定，FIN 报文段即使不携带数据，也将消耗掉一个序号。 第二次挥手：B 收到连接释放报文段后立即发出确认，确认号是 ack = u + 1，而这个报文段自己的序号是 v（等于 B 前面已经传送过的数据的最后一个字节的序号加1），然后 B 就进入 CLOSE-WAIT（关闭等待）状态。TCP 服务端进程这时应通知高层应用进程，因而从 A 到 B 这个方向的连接就释放了，这时的 TCP 连接处于半关闭（half-close）状态，即 A 已经没有数据要发送了，但 B 若发送数据，A 仍要接收。也就是说，从 B 到 A 这个方向的连接并未关闭，这个状态可能会持续一段时间。A 收到来自 B 的确认后，就进入 FIN-WAIT-2(终止等待2)状态，等待 B 发出的连接释放报文段。 第三次挥手：若 B 已经没有要向 A 发送的数据，其应用进程就通知 TCP 释放连接。这时 B 发出的连接释放报文段必须使 FIN = 1。假定 B 的序号为 w（在半关闭状态，B 可能又发送了一些数据）。B 还必须重复上次已发送过的确认号 ack = u + 1。这时 B 就进入 LAST-ACK(最后确认)状态，等待 A 的确认。 第四次挥手：A 在收到 B 的连接释放报文后，必须对此发出确认。在确认报文段中把 ACK 置 1，确认号 ack = w + 1，而自己的序号 seq = u + 1（前面发送的 FIN 报文段要消耗一个序号）。然后进入 TIME-WAIT(时间等待) 状态。请注意，现在 TCP 连接还没有释放掉。必须经过时间等待计时器设置的时间 2MSL（MSL：最长报文段寿命）后，A 才能进入到 CLOSED 状态，然后撤销传输控制块，结束这次 TCP 连接。当然如果 B 一收到 A 的确认就进入 CLOSED 状态，然后撤销传输控制块。所以在释放连接时，B 结束 TCP 连接的时间要早于 A。 15、为什么 TIME-WAIT 状态必须等待 2MSL 的时间呢？ 1.为了保证 A 发送的最后一个 ACK 报文段能够到达 B。这个 ACK 报文段有可能丢失，因而使处在 LAST-ACK 状态的 B 收不到对已发送的 FIN + ACK 报文段的确认。B 会超时重传这个 FIN+ACK 报文段，而 A 就能在 2MSL 时间内（超时 + 1MSL 传输）收到这个重传的 FIN+ACK 报文段。接着 A 重传一次确认，重新启动 2MSL 计时器。最后，A 和 B 都正常进入到 CLOSED 状态。如果 A 在 TIME-WAIT 状态不等待一段时间，而是在发送完 ACK 报文段后立即释放连接，那么就无法收到 B 重传的 FIN + ACK 报文段，因而也不会再发送一次确认报文段，这样，B 就无法按照正常步骤进入 CLOSED 状态。 16、为什么第二次跟第三次不能合并, 第二次和第三次之间的等待是什么? 当服务器执行第二次挥手之后, 此时证明客户端不会再向服务端请求任何数据, 但是服务端可能还正在给客户端发送数据（可能是客户端上一次请求的资源还没有发送完毕），所以此时服务端会等待把之前未传输完的数据传输完毕之后再发送关闭请求。 17、保活计时器的作用? 除时间等待计时器外，TCP 还有一个保活计时器（keepalive timer）。设想这样的场景：客户已主动与服务器建立了 TCP 连接。但后来客户端的主机突然发生故障。显然，服务器以后就不能再收到客户端发来的数据。因此，应当有措施使服务器不要再白白等待下去。这就需要使用保活计时器了。 服务器每收到一次客户的数据，就重新设置保活计时器，时间的设置通常是两个小时。若两个小时都没有收到客户端的数据，服务端就发送一个探测报文段，以后则每隔 75 秒钟发送一次。若连续发送 10个 探测报文段后仍然无客户端的响应，服务端就认为客户端出了故障，接着就关闭这个连接。 18、TCP 协议是如何保证可靠传输的？ 数据包校验：目的是检测数据在传输过程中的任何变化，若校验出包有错，则丢弃报文段并且不给出响应，这时 TCP 发送数据端超时后会重发数据； 对失序数据包重排序：既然 TCP 报文段作为 IP 数据报来传输，而 IP 数据报的到达可能会失序，因此 TCP 报文段的到达也可能会失序。TCP 将对失序数据进行重新排序，然后才交给应用层； 丢弃重复数据：对于重复数据，能够丢弃重复数据； 应答机制：当 TCP 收到发自 TCP 连接另一端的数据，它将发送一个确认。这个确认不是立即发送，通常将推迟几分之一秒； 超时重发：当 TCP 发出一个段后，它启动一个定时器，等待目的端确认收到这个报文段。如果不能及时收到一个确认，将重发这个报文段； 流量控制：TCP 连接的每一方都有固定大小的缓冲空间。TCP 的接收端只允许另一端发送接收端缓冲区所能接纳的数据，这可以防止较快主机致使较慢主机的缓冲区溢出，这就是流量控制。TCP 使用的流量控制协议是可变大小的滑动窗口协议。 19、谈谈你对停止等待协议的理解？ 停止等待协议是为了实现可靠传输的，它的基本原理就是每发完一个分组就停止发送，等待对方确认。在收到确认后再发下一个分组；在停止等待协议中，若接收方收到重复分组，就丢弃该分组，但同时还要发送确认。主要包括以下几种情况：无差错情况、出现差错情况（超时重传）、确认丢失和确认迟到、确认丢失和确认迟到。 20、谈谈你对 ARQ 协议的理解？ 自动重传请求 ARQ 协议 停止等待协议中超时重传是指只要超过一段时间仍然没有收到确认，就重传前面发送过的分组（认为刚才发送过的分组丢失了）。因此每发送完一个分组需要设置一个超时计时器，其重传时间应比数据在分组传输的平均往返时间更长一些。这种自动重传方式常称为自动重传请求 ARQ。 连续 ARQ 协议 连续 ARQ 协议可提高信道利用率。发送方维持一个发送窗口，凡位于发送窗口内的分组可以连续发送出去，而不需要等待对方确认。接收方一般采用累计确认，对按序到达的最后一个分组发送确认，表明到这个分组为止的所有分组都已经正确收到了。 21、谈谈你对滑动窗口的了解？ TCP 利用滑动窗口实现流量控制的机制。滑动窗口（Sliding window）是一种流量控制技术。早期的网络通信中，通信双方不会考虑网络的拥挤情况直接发送数据。由于大家不知道网络拥塞状况，同时发送数据，导致中间节点阻塞掉包，谁也发不了数据，所以就有了滑动窗口机制来解决此问题。 TCP 中采用滑动窗口来进行传输控制，滑动窗口的大小意味着接收方还有多大的缓冲区可以用于接收数据。发送方可以通过滑动窗口的大小来确定应该发送多少字节的数据。当滑动窗口为 0 时，发送方一般不能再发送数据报，但有两种情况除外，一种情况是可以发送紧急数据，例如，允许用户终止在远端机上的运行进程。另一种情况是发送方可以发送一个 1 字节的数据报来通知接收方重新声明它希望接收的下一字节及发送方的滑动窗口大小。 在数据链路层中,根据发送发和接收方滑动窗口的大小,可分为以下几种重传机制: 停止-等待协议：发送窗口大小=1，接受窗口大小=1； 后退N帧协议：发送窗口大小>1，接受窗口大小=1； 选择重传协议：发送窗口大小>1，接受窗口大小>1； 22、谈下你对流量控制的理解？ TCP 利用滑动窗口实现流量控制。流量控制是为了控制发送方发送速率，保证接收方来得及接收。接收方发送的确认报文中的窗口字段可以用来控制发送方窗口大小，从而影响发送方的发送速率。将窗口字段设置为 0，则发送方不能发送数据。 23、谈下你对 TCP 拥塞控制的理解？ 使用了哪些算法？ 拥塞控制和流量控制不同，前者是一个全局性的过程，而后者指点对点通信量的控制。在某段时间，若对网络中某一资源的需求超过了该资源所能提供的可用部分，网络的性能就要变坏。这种情况就叫拥塞。 拥塞控制就是为了防止过多的数据注入到网络中，这样就可以使网络中的路由器或链路不致于过载。拥塞控制所要做的都有一个前提，就是网络能够承受现有的网络负荷。拥塞控制是一个全局性的过程，涉及到所有的主机，所有的路由器，以及与降低网络传输性能有关的所有因素。相反，流量控制往往是点对点通信量的控制，是个端到端的问题。流量控制所要做到的就是抑制发送端发送数据的速率，以便使接收端来得及接收。 为了进行拥塞控制，TCP 发送方要维持一个拥塞窗口(cwnd) 的状态变量。拥塞控制窗口的大小取决于网络的拥塞程度，并且动态变化。发送方让自己的发送窗口取为拥塞窗口和接收方的接受窗口中较小的一个。 TCP 的拥塞控制采用了四种算法，即：慢开始、拥塞避免、快重传和快恢复。在网络层也可以使路由器采用适当的分组丢弃策略（如：主动队列管理 AQM），以减少网络拥塞的发生。 慢开始： 慢开始算法的思路是当主机开始发送数据时，如果立即把大量数据字节注入到网络，那么可能会引起网络阻塞，因为现在还不知道网络的符合情况。经验表明，较好的方法是先探测一下，即由小到大逐渐增大发送窗口，也就是由小到大逐渐增大拥塞窗口数值。cwnd 初始值为 1，每经过一个传播轮次，cwnd 加倍。 拥塞避免： 拥塞避免算法的思路是让拥塞窗口 cwnd 缓慢增大，即每经过一个往返时间 RTT 就把发送方的 cwnd 加 1。 快重传与快恢复： 在 TCP/IP 中，快速重传和快恢复（fast retransmit and recovery，FRR）是一种拥塞控制算法，它能快速恢复丢失的数据包。 没有 FRR，如果数据包丢失了，TCP 将会使用定时器来要求传输暂停。在暂停的这段时间内，没有新的或复制的数据包被发送。有了 FRR，如果接收机接收到一个不按顺序的数据段，它会立即给发送机发送一个重复确认。如果发送机接收到三个重复确认，它会假定确认件指出的数据段丢失了，并立即重传这些丢失的数据段。 有了 FRR，就不会因为重传时要求的暂停被耽误。当有单独的数据包丢失时，快速重传和快恢复（FRR）能最有效地工作。当有多个数据信息包在某一段很短的时间内丢失时，它则不能很有效地工作。 24、什么是粘包？ 在进行 Java NIO 学习时，可能会发现：如果客户端连续不断的向服务端发送数据包时，服务端接收的数据会出现两个数据包粘在一起的情况。 TCP 是基于字节流的，虽然应用层和 TCP 传输层之间的数据交互是大小不等的数据块，但是 TCP 把这些数据块仅仅看成一连串无结构的字节流，没有边界； 从 TCP 的帧结构也可以看出，在 TCP 的首部没有表示数据长度的字段。 基于上面两点，在使用 TCP 传输数据时，才有粘包或者拆包现象发生的可能。一个数据包中包含了发送端发送的两个数据包的信息，这种现象即为粘包。 接收端收到了两个数据包，但是这两个数据包要么是不完整的，要么就是多出来一块，这种情况即发生了拆包和粘包。拆包和粘包的问题导致接收端在处理的时候会非常困难，因为无法区分一个完整的数据包。 25、TCP 黏包是怎么产生的？ 发送方产生粘包 采用 TCP 协议传输数据的客户端与服务器经常是保持一个长连接的状态（一次连接发一次数据不存在粘包），双方在连接不断开的情况下，可以一直传输数据。但当发送的数据包过于的小时，那么 TCP 协议默认的会启用 Nagle 算法，将这些较小的数据包进行合并发送（缓冲区数据发送是一个堆压的过程）；这个合并过程就是在发送缓冲区中进行的，也就是说数据发送出来它已经是粘包的状态了。 接收方产生粘包 接收方采用 TCP 协议接收数据时的过程是这样的：数据到接收方，从网络模型的下方传递至传输层，传输层的 TCP 协议处理是将其放置接收缓冲区，然后由应用层来主动获取（C 语言用 recv、read 等函数）；这时会出现一个问题，就是我们在程序中调用的读取数据函数不能及时的把缓冲区中的数据拿出来，而下一个数据又到来并有一部分放入的缓冲区末尾，等我们读取数据时就是一个粘包。（放数据的速度 > 应用层拿数据速度） 26、怎么解决拆包和粘包？ 分包机制一般有两个通用的解决方法： 特殊字符控制； 在包头首都添加数据包的长度。 当然,也可以借鉴数据链路层中组帧的方法: 字符计数法 字符填充的首位定界法 零比特填充的首尾标志法 如果使用 netty 的话，就有专门的编码器和解码器解决拆包和粘包问题了。 tips：UDP 没有粘包问题，但是有丢包和乱序。不完整的包是不会有的，收到的都是完全正确的包。传送的数据单位协议是 UDP 报文或用户数据报，发送的时候既不合并，也不拆分。 27、你对 HTTP 状态码有了解吗？ 1XX 信息 100 Continue ：表明到目前为止都很正常，客户端可以继续发送请求或者忽略这个响应。 2XX 成功 200 OK 204 No Content ：请求已经成功处理，但是返回的响应报文不包含实体的主体部分。一般在只需要从客户端往服务器发送信息，而不需要返回数据时使用。 206 Partial Content ：表示客户端进行了范围请求，响应报文包含由 Content-Range 指定范围的实体内容。 3XX 重定向 301 Moved Permanently ：永久性重定向； 302 Found ：临时性重定向； 303 See Other ：和 302 有着相同的功能，但是 303 明确要求客户端应该采用 GET 方法获取资源。 304 Not Modified ：如果请求报文首部包含一些条件，例如：If-Match，If-Modified-Since，If-None-Match，If-Range，If-Unmodified-Since，如果不满足条件，则服务器会返回 304 状态码。 307 Temporary Redirect ：临时重定向，与 302 的含义类似，但是 307 要求浏览器不会把重定向请求的 POST 方法改成 GET 方法。 4XX 客户端错误 400 Bad Request ：请求报文中存在语法错误。 401 Unauthorized ：该状态码表示发送的请求需要有认证信息（BASIC 认证、DIGEST 认证）。如果之前已进行过一次请求，则表示用户认证失败。 403 Forbidden ：请求被拒绝。 404 Not Found 5XX 服务器错误 500 Internal Server Error ：服务器正在执行请求时发生错误； 503 Service Unavailable ：服务器暂时处于超负载或正在进行停机维护，现在无法处理请求。 28、HTTP 状态码 301 和 302 代表的是什么？ 有什么区别？ 301，302 都是 HTTP 状态的编码，都代表着某个 URL 发生了转移。 区别： 301 redirect: 301 代表永久性转移（Permanently Moved） 302 redirect: 302 代表暂时性转移（Temporarily Moved） 29、forward 和 redirect 的区别？ Forward 和 Redirect 代表了两种请求转发方式：直接转发和间接转发。 直接转发方式（Forward）：客户端和浏览器只发出一次请求，Servlet、HTML、JSP 或其它信息资源，由第二个信息资源响应该请求，在请求对象 request 中，保存的对象对于每个信息资源是共享的。 间接转发方式（Redirect）：实际是两次 HTTP 请求，服务器端在响应第一次请求的时候，让浏览器再向另外一个 URL 发出请求，从而达到转发的目的。 举个通俗的例子：　 直接转发就相当于：“A 找 B 借钱，B 说没有，B 去找 C 借，借到借不到都会把消息传递给 A”； 间接转发就相当于：\"A 找 B 借钱，B 说没有，让 A 去找 C 借\"。 30、HTTP 方法有哪些？ 31、说下 GET 和 POST 的区别？ GET 和 POST 本质都是 HTTP 请求，只不过对它们的作用做了界定和适配，并且让他们适应各自的场景。 本质区别：GET 只是一次 HTTP请求，POST 先发请求头再发请求体，实际上是两次请求。 从功能上讲，GET 一般用来从服务器上获取资源，POST 一般用来更新服务器上的资源； 从 REST 服务角度上说，GET 是幂等的，即读取同一个资源，总是得到相同的数据，而 POST 不是幂等的，因为每次请求对资源的改变并不是相同的；进一步地，GET 不会改变服务器上的资源，而 POST 会对服务器资源进行改变； 从请求参数形式上看，GET 请求的数据会附在 URL 之后，即将请求数据放置在 HTTP 报文的 请求头 中，以 ? 分割 URL 和传输数据，参数之间以 & 相连。特别地，如果数据是英文字母/数字，原样发送；否则，会将其编码为 application/x-www-form-urlencoded MIME 字符串(如果是空格，转换为+，如果是中文/其他字符，则直接把字符串用 BASE64 加密，得出如：%E4%BD%A0%E5%A5%BD，其中 ％XX 中的 XX 为该符号以 16 进制表示的 ASCII)；而 POST 请求会把提交的数据则放置在是 HTTP 请求报文的 请求体 中； 就安全性而言，POST 的安全性要比 GET 的安全性高，因为 GET 请求提交的数据将明文出现在 URL 上，而且 POST 请求参数则被包装到请求体中，相对更安全； 从请求的大小看，GET 请求的长度受限于浏览器或服务器对 URL 长度的限制，允许发送的数据量比较小，而 POST 请求则是没有大小限制的。 32、在浏览器中输入 URL 地址到显示主页的过程？ 总体来说分为以下几个过程: DNS 解析 TCP 连接 发送 HTTP 请求 服务器处理请求并返回 HTTP 报文 渲染页面 TCP释放 33、DNS 的解析过程？ 主机向本地域名服务器的查询一般都是采用递归查询。所谓递归查询就是：如果主机所询问的本地域名服务器不知道被查询的域名的 IP 地址，那么本地域名服务器就以 DNS 客户的身份，向根域名服务器继续发出查询请求报文(即替主机继续查询)，而不是让主机自己进行下一步查询。因此，递归查询返回的查询结果或者是所要查询的 IP 地址，或者是报错，表示无法查询到所需的 IP 地址。 本地域名服务器向根域名服务器的查询的迭代查询。迭代查询的特点：当根域名服务器收到本地域名服务器发出的迭代查询请求报文时，要么给出所要查询的 IP 地址，要么告诉本地服务器：“你下一步应当向哪一个域名服务器进行查询”。然后让本地服务器进行后续的查询。根域名服务器通常是把自己知道的顶级域名服务器的 IP 地址告诉本地域名服务器，让本地域名服务器再向顶级域名服务器查询。顶级域名服务器在收到本地域名服务器的查询请求后，要么给出所要查询的 IP 地址，要么告诉本地服务器下一步应当向哪一个权限域名服务器进行查询。最后，本地域名服务器得到了所要解析的 IP 地址或报错，然后把这个结果返回给发起查询的主机。 34、谈谈你对域名缓存的了解？ 为了提高 DNS 查询效率，并减轻服务器的负荷和减少因特网上的 DNS 查询报文数量，在域名服务器中广泛使用了高速缓存，用来存放最近查询过的域名以及从何处获得域名映射信息的记录。 由于名字到地址的绑定并不经常改变，为保持高速缓存中的内容正确，域名服务器应为每项内容设置计时器并处理超过合理时间的项（例如：每个项目两天）。当域名服务器已从缓存中删去某项信息后又被请求查询该项信息，就必须重新到授权管理该项的域名服务器绑定信息。当权限服务器回答一个查询请求时，在响应中都指明绑定有效存在的时间值。增加此时间值可减少网络开销，而减少此时间值可提高域名解析的正确性。 不仅在本地域名服务器中需要高速缓存，在主机中也需要。许多主机在启动时从本地服务器下载名字和地址的全部数据库，维护存放自己最近使用的域名的高速缓存，并且只在从缓存中找不到名字时才使用域名服务器。维护本地域名服务器数据库的主机应当定期地检查域名服务器以获取新的映射信息，而且主机必须从缓存中删除无效的项。由于域名改动并不频繁，大多数网点不需花精力就能维护数据库的一致性。 35、 谈下你对 HTTP 长连接和短连接的理解? 分别应用于哪些场景？ 在 HTTP/1.0 中默认使用短连接。也就是说，客户端和服务器每进行一次 HTTP 操作，就建立一次连接，任务结束就中断连接。当客户端浏览器访问的某个 HTML 或其他类型的 Web 页中包含有其他的 Web 资源（如：JavaScript 文件、图像文件、CSS 文件等），每遇到这样一个 Web 资源，浏览器就会重新建立一个 HTTP 会话。 而从 HTTP/1.1 起，默认使用长连接，用以保持连接特性。使用长连接的 HTTP 协议，会在响应头加入这行代码： Connection:keep-alive 在使用长连接的情况下，当一个网页打开完成后，客户端和服务器之间用于传输 HTTP 数据的 TCP 连接不会关闭，客户端再次访问这个服务器时，会继续使用这一条已经建立的连接。 Keep-Alive 不会永久保持连接，它有一个保持时间，可以在不同的服务器软件（如：Apache）中设定这个时间。实现长连接需要客户端和服务端都支持长连接。 36、谈下 HTTP 1.0 和 1.1、1.2 的主要变化？ HTTP1.1 的主要变化： 缓存处理，在HTTP1.0中主要使用header里的If-Modified-Since,Expires来做为缓存判断的标准，HTTP1.1则引入了更多的缓存控制策略例如Entity tag，If-Unmodified-Since, If-Match, If-None-Match等更多可供选择的缓存头来控制缓存策略。 带宽优化及网络连接的使用，HTTP1.0中，存在一些浪费带宽的现象，例如客户端只是需要某个对象的一部分，而服务器却将整个对象送过来了，并且不支持断点续传功能，HTTP1.1则在请求头引入了range头域，它允许只请求资源的某个部分，即返回码是206（Partial Content），这样就方便了开发者自由的选择以便于充分利用带宽和连接。 错误通知的管理，在HTTP1.1中新增了24个错误状态响应码，如409（Conflict）表示请求的资源与资源的当前状态发生冲突；410（Gone）表示服务器上的某个资源被永久性的删除。 Host头处理，在HTTP1.0中认为每台服务器都绑定一个唯一的IP地址，因此，请求消息中的URL并没有传递主机名（hostname）。但随着虚拟主机技术的发展，在一台物理服务器上可以存在多个虚拟主机（Multi-homed Web Servers），并且它们共享一个IP地址。HTTP1.1的请求消息和响应消息都应支持Host头域，且请求消息中如果没有Host头域会报告一个错误（400 Bad Request）。 长连接，HTTP 1.1支持长连接（PersistentConnection）和请求的流水线（Pipelining）处理，在一个TCP连接上可以传送多个HTTP请求和响应，减少了建立和关闭连接的消耗和延迟，在HTTP1.1中默认开启Connection： keep-alive，一定程度上弥补了HTTP1.0每次请求都要创建连接的缺点。 HTTP2.0 的主要变化： HTTP2.0 支持多路复用，同一个连接可以并发处理多个请求，方法是把 HTTP数据包拆为多个帧，并发有序的发送，根据序号在另一端进行重组，而不需要一个个 HTTP请求顺序到达； HTTP2.0 支持服务端推送，就是服务端在 HTTP 请求到达后，除了返回数据之外，还推送了额外的内容给客户端； HTTP2.0 压缩了请求头，同时基本单位是二进制帧流，这样的数据占用空间更少； HTTP2.0 适用于 HTTPS 场景，因为其在 HTTP和 TCP 中间加了一层 SSL 层。 37.HTTPS 的工作过程？ 客户端发送自己支持的加密规则给服务器，代表告诉服务器要进行连接了； 服务器从中选出一套加密算法和 hash 算法以及自己的身份信息（地址等）以证书的形式发送给浏览器，证书中包含服务器信息，加密公钥，证书的办法机构； 客户端收到网站的证书之后要做下面的事情： 3.1 验证证书的合法性； 3.2 果验证通过证书，浏览器会生成一串随机数，并用证书中的公钥进行加密； 3.3 用约定好的 hash 算法计算握手消息，然后用生成的密钥进行加密，然后一起发送给服务器。 服务器接收到客户端传送来的信息，要做下面的事情： 4.1 用私钥解析出密码，用密码解析握手消息，验证 hash 值是否和浏览器发来的一致； 4.2 使用密钥加密消息； 如果计算法 hash 值一致，握手成功。 38、HTTP 和 HTTPS 的区别？ 开销：HTTPS 协议需要到 CA 申请证书，一般免费证书很少，需要交费； 资源消耗：HTTP 是超文本传输协议，信息是明文传输，HTTPS 则是具有安全性的 ssl 加密传输协议，需要消耗更多的 CPU 和内存资源； 端口不同：HTTP 和 HTTPS 使用的是完全不同的连接方式，用的端口也不一样，前者是 80，后者是 443； 安全性：HTTP 的连接很简单，是无状态的；HTTPS 协议是由 SSL+HTTP 协议构建的可进行加密传输、身份认证的网络协议，比 HTTP 协议安全。 39、HTTPS 的优缺点？ 优点： 使用 HTTPS 协议可认证用户和服务器，确保数据发送到正确的客户机和服务器； HTTPS 协议是由 SSL + HTTP 协议构建的可进行加密传输、身份认证的网络协议，要比 HTTP 协议安全，可防止数据在传输过程中不被窃取、改变，确保数据的完整性； HTTPS 是现行架构下最安全的解决方案，虽然不是绝对安全，但它大幅增加了中间人攻击的成本。 缺点： HTTPS 协议握手阶段比较费时，会使页面的加载时间延长近 50%，增加 10% 到 20% 的耗电； HTTPS 连接缓存不如 HTTP 高效，会增加数据开销和功耗，甚至已有的安全措施也会因此而受到影响； SSL 证书需要钱，功能越强大的证书费用越高，个人网站、小网站没有必要一般不会用； SSL 证书通常需要绑定 IP，不能在同一 IP 上绑定多个域名，IPv4 资源不可能支撑这个消耗； HTTPS 协议的加密范围也比较有限，在黑客攻击、拒绝服务攻击、服务器劫持等方面几乎起不到什么作用。最关键的，SSL 证书的信用链体系并不安全，特别是在某些国家可以控制 CA 根证书的情况下，中间人攻击一样可行。 40.什么是数字签名? 为了避免数据在传输过程中被替换，比如黑客修改了你的报文内容，但是你并不知道，所以我们让发送端做一个数字签名，把数据的摘要消息进行一个加密，比如 MD5，得到一个签名，和数据一起发送。然后接收端把数据摘要进行 MD5 加密，如果和签名一样，则说明数据确实是真的。 41、什么是数字证书？ 对称加密中，双方使用公钥进行解密。虽然数字签名可以保证数据不被替换，但是数据是由公钥加密的，如果公钥也被替换，则仍然可以伪造数据，因为用户不知道对方提供的公钥其实是假的。所以为了保证发送方的公钥是真的，CA 证书机构会负责颁发一个证书，里面的公钥保证是真的，用户请求服务器时，服务器将证书发给用户，这个证书是经由系统内置证书的备案的。 42、什么是对称加密和非对称加密？ 对称密钥加密是指加密和解密使用同一个密钥的方式，这种方式存在的最大问题就是密钥发送问题，即如何安全地将密钥发给对方。 非对称加密指使用一对非对称密钥，即：公钥和私钥，公钥可以随意发布，但私钥只有自己知道。发送密文的一方使用对方的公钥进行加密处理，对方接收到加密信息后，使用自己的私钥进行解密。 由于非对称加密的方式不需要发送用来解密的私钥，所以可以保证安全性。但是和对称加密比起来，它非常的慢，所以我们还是要用对称加密来传送消息，但对称加密所使用的密钥我们可以通过非对称加密的方式发送出去。 "},"数据结构/1.图论.html":{"url":"数据结构/1.图论.html","title":"1.图论","keywords":"","body":"一 图的表示与遍历1.邻接矩阵2.邻接表3.dfs遍历4.bfs遍历二 最短路算法1.Dijkstra 算法 单源有权最短路2.Floyd算法 多源最短路算法三 并查集1.定义2.模板四 拓扑排序1.定义2.模板:五 最小生成树0.定义1.Prim算法2.Kruskal 算法六 二分图1.定义2.二分图的判断3.最大匹配4.匈牙利算法一 图的表示与遍历 1.邻接矩阵 2.邻接表 邻接表模板: typedef pair ll; map> graph; //初始化 ..... // for (int i=0;i>a>>b>>value; graph[a].push_back({b,value}); graph[b].push_back({a,value}); } 3.dfs遍历 void DFS ( Vertex V ){ visited[ V ] = true; for ( V 的每个邻接点 W ) if( !visited[ W ]) DFS( W ); } 4.bfs遍历 void BFS( Vertex V ){ queue q; visited[V] = true; q.push(V); while(!q.empty()){ V = q.front(); q.pop(); for( V 的每个邻接点 W ){ if( !visited[ W ]){ visited[W] = true; q.push(W); } } } } 二 最短路算法 1.Dijkstra 算法 单源有权最短路 令 S = {源点s + 已经确定了最短路径的顶点 vi _ii} 对任一未收录的顶点 v，定义 dist[v] 为 s 到 v 的最短路径长度，但该路径仅经过 S 中的顶点。即路径 {s→(vi _ii∈S)→v} 的最小长度 若路径是按照递增（非递减）的顺序生成的，则 真正的最短路必须只经过 S 中的顶点 每次从未收录的顶点中选一个 dist 最小的收录 增加一个 v 进入 S，可能影响另外一个 w 的 dist 值 dist[w] = min{dist[w],dist[v] + 的权重} 伪代码: void Dijkstra( Vertex s ){ while(1){ V = 未收录顶点中dist最小值; if( 这样的V不存在 ) break; collected[V] = true; for( V 的每个邻接点 W ) if( collected[W] == false ) if(dist[V] + E ; path[W] = V; } } } 堆优化版算法模板:(heap中的自动排序:先对第一个从小到大排序,若第一个相等,对第二个从小到大排序) int dijkstra(vector>& gh, int N, int K) { const int INF = 0x3f3f3f3f; typedef pair PII; // first:距离; second: 几号点 vector vis(N + 1, false); // 是否已得到最短距离 vector dist(N+1, INF); // 距离起始点的最短距离 unordered_map> graph; // 邻接表；u->v,权重w priority_queue, greater> heap; // 小顶堆；维护到起始点的最短距离和点 for (auto &t: gh){ // 初始化邻接表 graph[t[0]].push_back({t[2],t[1]}); } heap.push({0, K}); dist[K] = 0; while(heap.size()){ auto t = heap.top(); heap.pop(); int ver = t.second, distance = t.first; if (vis[ver]) continue; // 之前更新过，是冗余备份 vis[ver] = true; for (auto &p: graph[ver]){ if (dist[p.second] > distance + p.first){ // 用t去更新其他点到起始点的最短距离 dist[p.second] = distance + p.first; heap.push({dist[p.second], p.second}); } } } int ans = *max_element(dist.begin()+1, dist.end()); return ans == INF ? -1: ans; } 2.Floyd算法 多源最短路算法 实际上是动态规划的思想: void Floyd(){ for( k = 0; k 算法模板: int floyd(vector>& gh, int N, int K) { const int INF = 0x3f3f3f3f; vector> dist(N+1,vector(N+1,INF)); for (int i = 0; i INF/2?-1:ans; } 三 并查集 1.定义 如下面这幅图，总共有 10 个节点，他们互不相连，分别用 0~9 标记： 现在我们的 Union-Find 算法主要需要实现这两个 API： class UF { /* 将 p 和 q 连接 */ public void union(int p, int q); /* 判断 p 和 q 是否连通 */ public boolean connected(int p, int q); /* 返回图中有多少个连通分量 */ public int count(); } 这里所说的「连通」是一种等价关系，也就是说具有如下三个性质： 1、自反性：节点p和p是连通的。 2、对称性：如果节点p和q连通，那么q和p也连通。 3、传递性：如果节点p和q连通，q和r连通，那么p和r也连通。 比如说之前那幅图，0～9 任意两个不同的点都不连通，调用connected都会返回 false，连通分量为 10 个。 如果现在调用union(0, 1)，那么 0 和 1 被连通，连通分量降为 9 个。 再调用union(1, 2)，这时 0,1,2 都被连通，调用connected(0, 2)也会返回 true，连通分量变为 8 个。 如果某两个节点被连通，则让其中的（任意）一个节点的根节点接到另一个节点的根节点上: 2.模板 class UnionFind{ public: int UnionNums; int *graph; /** * find */ int find(int k){ if(graph[k]==k)return k; else return k=find(graph[k]); } /** * union */ void Union(int i,int j){ int pI=find(i); int pK=find(j); if(pI!=pK){ graph[pK]=pI; UnionNums--; } } //初始化 UnionFind(int nums){ UnionNums=nums; graph=new int[nums+1]; for(int i=0;i四 拓扑排序 1.定义 一共有 n 门课要上，编号为0 ~ n-1。先决条件[1, 0]，代表必须先上课 0，才能上课 1 。给你 n 和一个先决条件表，请你判断能否完成所有课程 生活的例子 先穿内裤再穿裤子，先穿打底再穿外套，先穿衣服再戴帽子，约定俗成 内裤外穿、光着身子戴帽等，都会有点奇怪 约束我们的一条条 先后规则，能否转成一串顺序行为——衣服是一件件穿的 引入有向图 描述依赖关系 示例：n = 6，先决条件表：[ [3, 0], [3, 1], [4, 1], [4, 2], [5, 3], [5, 4] ] 0, 1, 2 没有先修课，可以直接选。其余的，都要先修 2 门课 我们用 有向图 描述这种 依赖关系 (做事的先后关系)： 把这样一个 有向无环图 变成 线性的排序 就叫 拓扑排序 有向图 中有 入度 和 出度 概念： 如果存在一条有向边 A --> B，则这条边给 A 增加了 1 个出度，给 B 增加了 1 个入度 所以顶点 0、1、2 的 入度为 0。 顶点 3、4、5 的 入度为 2 解决问题的关键: 0.顺序是什么? 1.对于什么来说是有入度的,什么是出度 2.每次只能选入读为0的课进行收录 3.到最后如果还有一个课的入读不为0,则说明存在环形图 2.模板: 以上面提到的课程表为例子: 你这个学期必须选修 numCourse 门课程，记为 0 到 numCourse-1 。 在选修某些课程之前需要一些先修课程。 例如，想要学习课程 0 ，你需要先完成课程 1 ，我们用一个匹配来表示他们：[0,1] 给定课程总量以及它们的先决条件，请你判断是否可能完成所有课程的学习？ class Solution { public: bool canFinish(int numCourses, vector>& prerequisites) { int len=numCourses; int in[len]; int out[len]; vector> ve; queue q; vector v; for(int i=0;i五 最小生成树 0.定义 连通网：在连通图中，若图的边具有一定的意义，每一条边都对应着一个数，称为权；权代表着连接连个顶点的代价，称这种连通图叫做连通网。 生成树：一个连通图的生成树是指一个连通子图，它含有图中全部n个顶点，但只有足以构成一棵树的n-1条边。一颗有n个顶点的生成树有且仅有n-1条边，如果生成树中再添加一条边，则必定成环。 最小生成树：在连通网的所有生成树中，所有边的代价和最小的生成树，称为最小生成树。 1.Prim算法 时间复杂度: Prim算法循环|V|-1∣V∣−1次, 使用线性扫描算法寻找最小值的时间复杂度为O(|V|^2+|E|), 使用堆优化版Prim算法的时间复杂度是O(|E|log|V|)O(∣E∣log∣V∣). 算法模板:(堆优化) typedef pair ll; int prim( vector> graph, vector dist,int n){ priority_queue,greater> q; vector vis(n+1, false); dist[1]=0; int sum=0; q.push({dist[1],1}); while( !q.empty() ){ int index=q.top().second; q.pop(); if(vis[index])continue; #加上这条边 sum+=dist[index]; vis[index]= true; #对其邻接边,收录 for (int i = 1; i >n>>m; int inf=10000000; vector dist(n+1,inf); vector> graph(n+1,vector(n+1,inf)); int a,b,value; for(int i=0;i>a>>b>>value; graph[a][b]=value; graph[b][a]=value; } cout 2.Kruskal 算法 Kruskal算法是用于生成无向带权连通图的最小生成树的算法, 在Kruskal算法中将图中每个顶点看做一个独立的集合, 首先将图中的所有边按照权值进行从小到大排序, 并按此顺序枚举每条边, 如果这条边的两个端点不属于同一个集合(连通分量), 那么将两个集合合并(并查集算法), 同时将这条边加入边集合E'E , 直到所有的顶点都属于同一个集合时, E'E 就是MST. Kruskal是一种贪心算法, 对于未选择的边权最小的边(u, v)(u,v), 如果加上这条边后图中出现了一个环, 则不符合树的性质, 则不选择该边. 算法模板: #include using namespace std; struct Edge{ int a,b,w; Edge(int a,int b ,int w):a(a),b(b),w(w){} bool operator g; vector fa; int find(int x){ return x==fa[x]?x:fa[x]=find(fa[x]); } int kruskal(int n){ //对边进行排序 sort(g.begin(),g.end()); int ans=0; int cnt=n; for(Edge e:g){ int fax= find(e.a); int fay= find(e.b); if(fax!=fay){ //计数 cnt--; //合并 fa[fax]=fay; //加上这条边的值 ans+=e.w; } } return cnt==1?ans:-1; } int main(){ int n,m; cin>>n>>m; int a,b,value; fa=vector(n+1,0); for (int i = 1; i >a>>b>>value; g.push_back(Edge(a,b,value)); } cout 六 二分图 1.定义 给定一个二分图G，在G的一个子图M中，M的边集{E}中的任意两条边都不依附于同一个顶点，则称M是一个匹配。极大匹配(Maximal Matching)是指在当前已完成的匹配下,无法再通过增加未完成匹配的边的方式来增加匹配的边数。最大匹配(maximum matching)是所有极大匹配当中边数最大的一个匹配。选择这样的边数最大的子集称为图的最大匹配问题。如果一个匹配中，图中的每个顶点都和图中某条边相关联，则称此匹配为完全匹配，也称作完备匹配。求二分图匹配可以用最大流(Maximal Flow)或者匈牙利算法(Hungarian Algorithm)。 完全匹配一定是极大匹配，但是极大匹配不一定是完全匹配。下图就是一个最大匹配。 2.二分图的判断 对于二分图的问题我们首先要判断一个图它是不是二分图。对于二分图的判断方法最常见的是染色法，顾名思义就是我们对每一个点进行染色操作，我们只用黑白两种颜色，问能不能使所有的点都染上了色，而且相邻两个点的颜色不同，如果可以那么这个图就是一个二分图，对于判断是否是一个二分图的方法可以用dfs和bfs两种方式去实现。下面我就上一个bfs的判断二分图的代码。 BFS判断二分图code： vector G[maxn]; // 存边 int col[maxn]; // 标记顶点颜色 int n,m; // 点和边的个数 bool bfs(){ queue q; q.push(1); // 放入第一个点 memset(col,0,sizeof(col)); col[1] = 1; // 先标记第一个点为1 while(!q.empty()){ int v = q.front(); q.pop(); for(int i=0;i 3.最大匹配 给定一个二分图G，在G的一个子图M中，M的边集中的任意两条边都不依附于同一个顶点，则称M是一个匹配。选择这样的边数最大的子集称为图的最大匹配问题（maximal matching problem)。 首先我们先了解两个概念 交替路：从一个未匹配的点出发，依次经过未匹配边、匹配边、未匹配边....这样的路叫做交替路。 增广路：从一个未匹配的点出发，走交替路，到达了一个未匹配过的点，这条路叫做增广路。 看下图，其中1、4、5、7是已经匹配的点，1->5,4->7是已经匹配的边，那么我们从8开始出发，8->4->7->1->5->2这条路就是一条增广路。 为什么我们要去找增广路呢？ 因为接下来要讲的匈牙利算法就是去寻找增广路而求出最大匹配数的(一句废话)，对于求二分图最大匹配的算法可以用网络流去跑一个最大流求解，还可以用二分图的常见算法匈牙利算法(Hungarian Algorithm)，这里我就只讲一下匈牙利算法，这个算法的核心就是去找未匹配的点，然后从这个点出发去寻找增广路，因为增广路有几个主要的特点： \\1. 增广路有奇数条边 。 \\2. 路径上的点一定是一个在X边，一个在Y边，交错出现。 \\3. 起点和终点都是目前还没有配对的点。 \\4. 未匹配边的数量比匹配边的数量多1。 重点是第4点，我们可以根据此特性，把这条增广路中的匹配边改为未匹配边，把未匹配边改为匹配边，这样我们就可以使总匹配边数+1了，根据上面的图得出下图，很显然匹配边多了一条。 4.匈牙利算法 \\二. 匈牙利算法** 下面我们讨论下匈牙利算法的内容： 给定一个图： 前面已经说了，我们讨论的基础是二部图，而上图就是一个二部图，我们从上图的左边开始讨论，\\我们的目标是尽可能给x中最多的点找到配对。** 注意，最大匹配是互相的，如果我们给X找到了最多的Y中的对应点，同样，Y中也不可能有更多的点得到匹配了。 刚开始，一个匹配都没有，我们随意选取一条边，（x1, y1）这条边，构建最初的匹配出来，结果如下，已经配对的边用粗线标出： 我们给x2添加一个匹配，如下图的（x2, y2）边。 目前来看，一切都很顺利，到这里，我们形成了匹配M，其有（x1, y1）, (x2, y2 ) 两条边。 \\3. 我们现在想给x3匹配一条边，发现它的另一端y1已经被\\x1**占用了，那x3就不高兴了，它就去找y1游说，让y1离开x1。 即将被迫分手的x1很委屈，好在它还有其他的选择，于是 x1 妥协了，准备去找自己看中的y2。 但很快，x1发现 y2 被x2 看中了，它就想啊，y1 抛弃了我，那我就让 y2 主动离开 \\x2 （很明显，这是个递归的过程）**。 x2 该怎么办呢？好在天无绝人之路，它去找了y5。 谢天谢地，y5 还没有名花有主，终于皆大欢喜。 匹配如下： 上面这个争论与妥协的过程中，我们把牵涉到的节点都拿出来：（x3, y1, x1, y2, x2, y5），很明显，这是一条路径P。 而在第二步中，我们已经形成了匹配M，而P呢？还记得增广路径么，我们发现，P原来是M的一条增广路径！ 上文已经说过，发现一条增广路径，就意味着一个更大匹配的出现，于是，我们将M中的配对点拆分开，重新组合，得到了一个更大匹配，M1, 其拥有（x3, y1）,(x1, y2), (x2, y5)三条边。 而这，就是匈牙利算法的精髓。 同样，x4 , x5 按顺序加入进来，最终会得到本图的最大匹配。 得到这个结果后，我们发现，其实也可以把y4 让给 x6 , 这样x5 就会空置，但并不影响最大匹配的大小。 vector G[maxn]; // 存边 int pre[maxn]; // 记录匹配点 bool vis[maxn]; // 标记是否匹配过 int n,m; // n个点 m条边 bool dfs(int x){ for(int i=0;i "},"数据结构/2.图论例题.html":{"url":"数据结构/2.图论例题.html","title":"2.图论例题","keywords":"","body":"1.图的遍历1.生化危机2.节点间通路2.最短路1. 哈利·波特的考试2. 被Gank的亚索3.并查集1.合并账户2.移出最多的同行或同列石头3.冗余连接4.被围绕的区域5.统计参与通信的服务器6.保证图可以遍历4.拓扑排序1.课程表2.矩阵中的最长递增路径5.最小生成树1.公路村村通1.图的遍历 1.生化危机 可用dfs或者bfs遍历即可: #include using namespace std; int main(){ int m,n,k; cin>>m>>n>>k; map> st; for (int i = 0; i {}; } //安全城市 vector isSafe(m, false); vector vis(m, false); for (int i = 0; i >temp; isSafe[temp]= true; } //公路 int a,b; for (int j = 0; j >a>>b; st[a].push_back(b); st[b].push_back(a); } //起止 int start,end; cin>>start>>end; if(!isSafe[end]){ cout q; q.push(start); while(!q.empty()){ int top=q.front(); q.pop(); vis[top]= true; if(top==end){ cout 2.节点间通路 考察图的遍历: //bfs版本: bool findWhetherExistsPath(int n, vector>& graph, int start, int target) { unordered_map> st; vector vis(n+1, false); for( auto k: graph){ st[k[0]].push_back(k[1]); } queue qu; qu.push(start); while ( !qu.empty() ){ int top=qu.front(); qu.pop(); vis[top]= true; if (top == target )return true; for (int i=0;i> st; vector flag; bool dfs(int cur,int target){ if(cur==target){ return true; } flag[cur]= true; for(int i=0;i>& graph, int start, int target) { flag=vector(n, false); for (int i = 0; i (); } for(auto k:graph){ st[k[0]].push_back(k[1]); } return dfs(start,target); } 2.最短路 1. 哈利·波特的考试 输入样例 6 11 3 4 70 1 2 1 5 4 50 2 6 50 5 6 60 1 3 70 4 6 60 3 6 80 5 1 100 2 4 60 5 2 80 输出样例: 4 70 题目分析: 简单理解就是: 每个节点到其他节点的最短的路径中,最长的那一条构建一个集合 从这个集合中寻找最短的一跳,作为答案 这里用的是floyd最短路算法 #include using namespace std; int inf=1000000; //寻找节点V到其他节点的最长路径 int find(vector> graph,int v){ int m=-100000; for (int i = 1; i >n>>m; vector> graph(n+1,vector(n+1,inf)); int a,b,value; for (int i = 0; i >a>>b>>value; graph[a][b]=value; graph[b][a]=value; } #floyd构建最短路 for (int k = 1; k 2. 被Gank的亚索 输入示例 5 6 1 5 1 2 5 2 5 5 1 3 3 3 5 7 1 4 1 4 5 9 输出示例 10 3 题目分析: 题目想让我们找最短路径的数目,需要注意的是,两个节点间读取数据时相同长度的路径不止一条(想一想LOL的地图),因此应该事先记录 其次,因为两节点间的路径不止一条,所以在计算答案的时候,应用*,而不是+ 堆优化版dijkstra最短路: #include using namespace std; int main(){ int n,m,s,e; cin>>n>>m>>s>>e; int inf=0x3f3f3f3f; typedef pair ll; //图 vector> graph(n+1,vector(n+1,0)); //堆 priority_queue,greater> st; //距离 vector dist(n+1,inf); //标志 vector vis(n+1, false); //路径数量 vector lowestNums(n+1,1); vector> roadNums(n+1,vector(n+1,0)); int a,b,value; for (int i = 0; i >a>>b>>value; if ( ! graph[a][b] || value 3.并查集 1.合并账户 并查集,问题在于,如何记录两个列表是否是同一个人的? 判断依据:有没有相同的邮箱 所以可以这样想 初始版本: 1.循环每一个列表的每一个邮箱,和别的列表比较,是否存在别的列表中 改进: 1.用一个set保存邮箱,如果邮箱没有出现过,就记录第一次出现时的父类 2.如果出现过,就合并两个父类 c++; class Solution { public: int graph[10000]={0}; int find(int i){ if(graph[i]==i)return graph[i]; else return graph[i]=find(graph[i]); } void Union(int i,int j){ int p1=find(i); int p2=find(j); if(p1!=p2){ graph[p2]=p1; } } //用于保存该邮箱是否有出现过 setisTurn; //用于保存该邮箱的父类 map emailFa; vector> accountsMerge(vector>& accounts) { int n=accounts.size(); for(int i=0;i> m; for(int i=0;i(); for(int j=1;j> result; for(auto acc:m){ vector ans; ans.push_back(accounts[acc.first][0]); for(auto s:acc.second){ ans.push_back(s); } result.push_back(ans); } return result; } }; 2.移出最多的同行或同列石头 并查集,关键要找出什么和什么并起来 这一题很明显是点和点之间并(条件是两个点的行数或列数相同),并且在合并过程中求出每个簇下点的数量, 最后结果就是每个簇的点的数量-1,相加起来 class Solution { public: int graph[10000]={0}; int size[10000]={0}; int find(int i){ if(graph[i]==i)return graph[i]; else return graph[i]=find(graph[i]); } void Union(int i,int j){ int p1=find(i); int p2=find(j); if(p1!=p2){ if(size[p1]>size[p2]){ graph[p2]=p1; size[p1]+=size[p2]; size[p2]=0; }else{ graph[p1]=p2; size[p2]+=size[p1]; size[p1]=0; } } } int removeStones(vector>& stones) { int n=stones.size(); for(int i=0;i0)cnt+=size[i]-1; } return cnt; } }; 3.冗余连接 这个问题意思是,现在有一个无向且有还的图,如何删掉一条边,使其成为无环图 ---->也即,并查集合并的时候,如果发现有祖先使相等的,就说明该条边造成了环,则记录该边 class Solution { public: int graph[10000]={0}; int size[10000]={0}; vectorresult; int find(int i){ if(graph[i]==i)return graph[i]; else return graph[i]=find(graph[i]); } void Union(int i,int j){ int p1=find(i); int p2=find(j); if(p1!=p2){ graph[p2]=p1; }else{ //说明发生了冲突,则把该边加入结果集 result.push_back(i); result.push_back(j); } } vector findRedundantConnection(vector>& edges) { int n=edges.size(); for(int i=0;i v; v.push_back(result[result.size()-2]); v.push_back(result[result.size()-1]); return v; } }; 4.被围绕的区域 如何运用并查集? 1.将二维转化为一维,即i*row+j 2.创造虚拟节点row*col,将边界的和虚拟节点并,若非边界,就和上下左右并即可 int graph[10000]={0}; int find(int i){ if(graph[i]==i)return graph[i]; else return graph[i]=find(graph[i]); } int cnt=0; void Union(int i,int j){ int p1=find(i); int p2=find(j); if(p1!=p2){ graph[p2]=p1; }else{ //说明改变无需存在就已经联通,让可重新操作的次数++; cnt++; } } bool connected(int i,int j){ return find(i)==find(j); } //可以将二维转化成一维 int turnNode(int i,int j,int cow){ return i*cow+j; } //初始化函数 void init(int n){ for(int i=0;i>& board) { int row=board.size(); if(row==0)return; int col=board[0].size(); init(row*col+1); //创建一个虚拟节点,用于连接边界的O int virtualNode=row*col; for(int i=0;i=0&&lx=0&&ly 5.统计参与通信的服务器 并查集,先算每一行,每一列,第一次出现的1的下标 接着,后面的电脑只需要并到前面记录的第一次出现的父类即可 用size代表该簇的大小 class Solution { public: int graph[1000000]={0}; int size[1000000]={0}; //查找 int find(int i){ if(graph[i]==i)return graph[i]; else return graph[i]=find(graph[i]); } //合并 void Union(int i,int j){ int p1=find(i); int p2=find(j); if(p1!=p2){ graph[p2]=p1; size[p1]+=size[p2]; size[p2]=0; } } bool connected(int i,int j){ return find(i)==find(j); } //可以将二维转化成一维 int node(int i,int j,int cow){ return i*cow+j; } //初始化函数 void init(int n){ for(int i=0;i>& grid) { int rows=grid.size(); int cols=grid[0].size(); // cout row; mapcol; for(int i=0;i1)cnt+=size[i]; } return cnt; } }; 6.保证图可以遍历 图中有三种类型的边 对于Alice而言,她可以走类型一和1和3, Bob可以走2和3 因此,我们可以先将类型为3的边合并, 再在3合并后的基础上,分别合并1和2,也即,分别构建属于Bob和Alice的并查集, 如果在合并过程中,发现两个点无法合并,就说明这条边可以删除 class Solution { public: int getRoot(vector& par,int x){ int root = x; while(par[root]!=root){ root = par[root]; } while(par[x]!=root){ int tmp = par[x]; par[x] = root; x = tmp; } return root; } bool merge(vector& par,int x,int y){ int _x = getRoot(par,x); int _y = getRoot(par,y); if(_x!=_y){ par[_x]=_y; return true; } return false; } int maxNumEdgesToRemove(int n, vector>& edges) { vectorpar1 = vector(n+1,0); vectorpar2; int ans = 0; int cnt1 = n,cnt2; for(int i =1;i 4.拓扑排序 1.课程表 模板题: class Solution { public: bool canFinish(int numCourses, vector>& prerequisites) { //代表入度 vector in(numCourses,0); //邻接表 vector> st(numCourses,vector{}); for (int i = 0; i q; for (int j = 0; j 2.矩阵中的最长递增路径 这是一道动态规划的题目,但用拓扑排序更方便 要求最长递增路径,比如从最有下角的两个1, 1->8 1->{6,2} 其实这里的1相当于第一层 接着2,6,8就是第二层 .. .. 所以,最长的路径其实就是算层数, 即,每次拓扑排序遍历的是一整层的数据,而不是一个个遍历 class Solution { public int longestIncreasingPath(int[][] matrix) { if (matrix==null||matrix.length==0){ return 0; } int[][] count = new int[matrix.length][matrix[0].length]; //统计每个点的入度用count数组保存，因为是递增，所以如果在上下左右，每发现一个比当前点小的数，当前点入度+1 for (int i = 0; i deque = new LinkedList<>(); //count数组中所有入度为0的点加入队列 for (int i = 0; i 0; size--) { int[] poll = deque.poll(); for (int[] d : direction) { if (longestIncreasingPathVerify(matrix, poll[0] + d[0], poll[1] + d[1]) && matrix[poll[0] + d[0]][poll[1] + d[1]] > matrix[poll[0]][poll[1]]) { if (--count[poll[0] + d[0]][poll[1] + d[1]] == 0) { deque.add(new int[]{poll[0] + d[0], poll[1] + d[1]}); } } } } } return ans; } private boolean longestIncreasingPathVerify(int[][] matrix, int i, int j) { return i >= 0 && j >= 0 && i 5.最小生成树 1.公路村村通 输入样例: 6 15 1 2 5 1 3 3 1 4 7 1 5 4 1 6 2 2 3 4 2 4 6 2 5 2 2 6 6 3 4 6 3 5 1 3 6 1 4 5 10 4 6 8 5 6 3 输出: 10 题目分析 就是根据所有的边,构建一颗最小生成树,使所需费用最低,这里用的是Kruskal 算法: #include using namespace std; //结构Edge 代表边 struct Edge{ int a,b,w; Edge(int a,int b ,int w):a(a),b(b),w(w){} bool operator g; vector fa; //并查集路径压缩查找 int find(int x){ return x==fa[x]?x:fa[x]=find(fa[x]); } //kruska算法 int kruska(int n){ sort(g.begin(),g.end()); int ans=0; int cnt=n; for(Edge e:g){ int fax= find(e.a); int fay= find(e.b); if(fax!=fay){ cnt--; fa[fax]=fay; ans+=e.w; } } return cnt==1?ans:-1; } int main(){ int n,m; cin>>n>>m; int a,b,value; fa=vector(n+1,0); for (int i = 1; i >a>>b>>value; g.push_back(Edge(a,b,value)); } cout"},"数据结构/3.散列表.html":{"url":"数据结构/3.散列表.html","title":"3.散列表","keywords":"","body":"散列查找1. 基本思想2. 基本工作3. 散列函数的构造4. 冲突处理方法5. 抽象数据类型定义散列查找 1. 基本思想 以关键字 key 为自变量，通过一个确定的函数 h（散列函数），计算出对应的函数值 h(key)，作为数据对象的存储地址 可能不同的关键字会映射到同一个散列地址上，即 h(keyi _i) = h(keyj _j)（当 keyi _ii ≠ keyj _jj），称为“冲突”——需要某种冲突解决策略 2. 基本工作 计算位置：构造散列函数确定关键词存储位置 解决冲突：应用某种策略解决多个关键词位置相同的问题 时间复杂度几乎为是常数：O(1) 3. 散列函数的构造 1. 考虑因素 计算简单，以便提高转换速度 关键词对应的地址空间分布均匀，以尽量减少冲突 2. 数字关键词 1. 直接定址法 取关键词的某个线性函数值为散列地址，即：h(key) = a x key + b （a、b为常数) 2. 除留余数法 散列函数为：h(key) = key mod p （p 一般取素数) 3. 数字分析法 分析数字关键字在各位上的变化情况，取比较随机的位作为散列地址 4. 折叠法 把关键词分割成位数相同的几个部分，然后叠加 5. 平方取中法 将关键词平方，取中间几位 3. 字符串关键字 1. ASCII码加和法 h(key) = (Σkey[i]) mod TableSize 2. 前3个字符移位法 h(key) = (key[0]×272 ^2 + key[1]×27 + key[2])mod TableSize 3. 移位法 例子（移位法） h(“abcde”) = 'a’x324 ^44 + 'b’x323 ^33 +'c’x322 ^22 + 'd’x32 + ‘e’ = ((('a’x32+‘b’)x32+‘c’)x32+‘d’)x32+‘e’ Index Hash( const char *Key,int TableSize){ unsigned int h = 0; // 散列值函数，初始化为 0 while ( *Key != '\\0' ) // 位移映射 h = ( h 4. 冲突处理方法 1. 常用策略 换个位置：开放地址法 同一位置的冲突对象组织在一起：链地址法 2. 开放定址法 一旦产生了冲突（该地址已有其它元素），就按某种规则去寻找另一空地址 若发生了第 i 次冲突，试探的下一个地址将增加 di ，基本公式是： hi _ii(key) = (h(key)+di _ii) mod TableSize （1 ≤ i ≤ TableSize） di _ii 决定了不同的解决冲突方案 1. 线性探测 以增量序列 1,2,…, (TableSize - 1) 循环试探下一个存储地址 2. 平方探测法 3. 双散列 4. 再散列 当散列表元素太多（即装填因子 α 太大）时，查找效率会下降 解决的方法是加倍扩大散列表，这个过程就叫\"再散列\"，扩大时，原有元素需要重新计算放置到新表中 3. 分离链接法 将相应位置上冲突的所有关键词存储在同一个单链表中 5. 抽象数据类型定义 数据类型：符号表（SymbolTable） 数据对象集：符号表是\"名字(Name)-属性(Attribute)\"对的集合 操作集：Table ∈ SymbolTable，Name ∈ NameType，Attr ∈ AttributeType 主要操作： SymbolTable InitalizeTable(int TableSize)：创建一个长度为 TableSize 的符号表 Boolean IsIn(SymbolTable Table,NameType Name)：查找特定的名字 Name 是否在 Table 中 AttributeType Find(SymbolTable Table,NameType Name)：获取 Table 中指定名字 Name 对应的属性 SymbolTable Modefy(SymbolTable Table,NameType Name,AttributeType Attr)：将 Table 中指定名字 Name 的属性修改为 Attr SymbolTable Insert(SymbolTable Table,NameType Name,AttributeType Attr)：向 Table 中插入一个新名字 Name 及其属性 Attr SymbolTable Delete(SymbolTable Table,NameType Name)：从 Table 中删除一个名字 Name 及其属性 1. 平方探测法实现 #include #include #include #define MAXTABLESIZE 100000 // 定义允许开辟的最大散列表长度 typedef int Index; typedef int ElementType; typedef Index Position; typedef enum{ // 分别对应：有合法元素、空、有已删除元素 Legitimate,Empty,Deleted } EntryType; // 定义单元状态类型 typedef struct HashEntry Cell; struct HashEntry{ // 哈希表存值单元 ElementType Data; // 存放元素 EntryType Info; // 单元状态 }; typedef struct HashTbl *HashTable; struct HashTbl{ // 哈希表结构体 int TableSize; // 哈希表大小 Cell *Cells; // 哈希表存值单元数组 }; using namespace std; int NextPrime(int N); // 查找素数 HashTable CreateTable( int TableSize); // 创建哈希表 Index Hash(int Key,int TableSize); // 哈希函数 // 查找素数 int NextPrime(int N){ int p = (N%2)?N+2:N+1; // 从大于 N 的下个奇数开始 int i; while(p 2;i--) if(!(p%i)) // p 不是素数 break; if(i==2) break; p += 2; // 继续试探下个奇数 } return p; } // 创建哈希表 HashTable CreateTable( int TableSize){ HashTable H; int i; H = (HashTable)malloc(sizeof(struct HashTbl)); // 保证哈希表最大长度是素数 H->TableSize = NextPrime(TableSize); // 初始化单元数组 H->Cells = (Cell *)malloc(sizeof(Cell)*H->TableSize); // 初始化单元数组状态 for(int i=0;iTableSize;i++) H->Cells[i].Info = Empty; return H; } // 平方探测查找 Position Find(HashTable H,ElementType Key){ Position CurrentPos,NewPos; int CNum = 0 ; // 记录冲突次数 CurrentPos = NewPos = Hash(Key,H->TableSize); // 如果当前单元状态不为空，且数值不等，则一直做 while(H->Cells[NewPos].Info != Empty && H->Cells[NewPos].Data != Key){ if(++CNum % 2 ){ // 冲突奇数次发生 NewPos = CurrentPos + (CNum+1)/2*(CNum+1)/2; // 如果越界，一直减直到再次进入边界 while(H->TableSize TableSize; } }else{ // 冲突偶数次发生 NewPos = CurrentPos - CNum/2*CNum/2; // 如果越界，一直加直到再次进入边界 while(NewPos TableSize; } } } return NewPos; } // 插入 bool Insert( HashTable H,ElementType Key,int i){ Position Pos = i; Pos = Find(H,Key); // 如果单元格状态不是\"存在合法元素\" if( H->Cells[Pos].Info != Legitimate){ H->Cells[Pos].Info = Legitimate; H->Cells[Pos].Data = Key; } return true; } // 除留余数法哈希函数 Index Hash(int Key,int TableSize){ return Key % TableSize; } void output(HashTable H){ for(int i=0;iTableSize;i++) coutCells[i].Data>N; for(int i=0;i>tmp; Insert(H,tmp,i); } output(H); return 0; } 2. 分离链接法实现 #include #include #include #define MAXTABLESIZE 100000 typedef int Index; typedef int ElementType; typedef struct LNode *PtrToLNode; struct LNode{ // 单链表 ElementType Data; PtrToLNode Next; }; typedef PtrToLNode Position; typedef PtrToLNode List; typedef struct TblNode *HashTable; // 散列表 struct TblNode{ int TableSize; // 表的最大长度 List Heads; // 指向链表头结点的数组 }; using namespace std; int NextPrime(int N){ int p = (N%2)?(N+2):(N+1); // 比 tablesize 大的奇数 int i; while(p 2;i--) if(!(p%i)) break; if(i==2) // 找到素数了 break; p += 2; // 下一个奇数 } return p; } // 创建哈希表 HashTable CreateTable( int TableSize){ HashTable H; H = (HashTable)malloc(sizeof(struct TblNode)); H->TableSize = NextPrime(TableSize); H->Heads = (List)malloc(sizeof(struct TblNode) * H->TableSize); for(int i=0;iTableSize;i++) H->Heads[i].Next = NULL; // 链表头：H->Heads[i] 是不存东西的 return H; } // 除留余数法哈希函数 Index Hash( int TableSize,ElementType Key){ return Key%TableSize; } // 查找 Position Find(HashTable H,ElementType Key){ Position p; Index pos; pos = Hash(H->TableSize,Key); p = H->Heads[pos].Next; //获得链表头 while(p && p->Data != Key) p = p->Next; return p; } // 插入 bool Insert(HashTable H,ElementType Key){ Position p,NewCell; Index pos; p = Find(H,Key); if(!p){ // 关键词未找到，可以插入 NewCell = (Position)malloc(sizeof(struct LNode)); NewCell->Data = Key; pos = Hash(H->TableSize,Key); // 初始散列表地址 // 将新增结点插到最前面 NewCell->Next = H->Heads[pos].Next; H->Heads[pos].Next = NewCell; return true; }else{ return false; } } void output(HashTable H){ for(int i=0;iTableSize;i++){ coutHeads[i].Next; while(p){ coutData; p = p->Next; } coutTableSize;i++){ P = H->Heads[i].Next; while( P ){ tmp = P->Next; free(P); P = tmp; } } free(H->Heads); free(H); } int main(){ HashTable H = CreateTable(9); int N; cin>>N; for(int i=0;i>tmp; Insert(H,tmp); } output(H); DestroyTable(H); return 0; } "},"数据结构/4.排序算法.html":{"url":"数据结构/4.排序算法.html","title":"4.排序算法","keywords":"","body":"1.快速排序2.归并排序3.选择排序4.堆排序5.希尔排序6.计数排序7.基数排序8.桶排序9.直接插入排序10.总结1.快速排序 基本思想是：通过一趟排序将要排序的数据分割成独立的两部分，其中一部分的所有数据都比另外一部分的所有数据都要小，然后再按此方法对这两部分数据分别进行快速排序，整个排序过程可以递归进行，以此达到整个数据变成有序序列。 时间复杂度: ) 。 void quick_sort( vector & arr, int start ,int end){ if ( start=pivot)right--; arr[left]=arr[right]; while(left 复杂度分析 快速排序涉及到递归调用，所以该算法的时间复杂度还需要从递归算法的复杂度开始说起； ​ 递归算法的时间复杂度公式：T[n] = aT[n/b] + f(n) ；对于递归算法的时间复杂度这里就不展开来说了； 快速排序最优的情况就是每一次取到的元素都刚好平分整个数组 ​ 此时的时间复杂度公式则为：T[n] = 2T[n/2] + f(n)；T[n/2]**为平分后的子数组的时间复杂度，f[n] 为平分这个数组时所花的时间； 所以最好情况下是nlog(n) 最差情况下时间复杂度 ​ 最差的情况就是每一次取到的元素就是数组中最小/最大的，这种情况其实就是冒泡排序了(每一次都排好一个元素的顺序) 这种情况时间复杂度就好计算了，就是冒泡排序的时间复杂度：T[n] = n * (n-1) = n^2 + n; \\综上所述：快速排序最差的情况下时间复杂度为：O( n^2 )** 2.归并排序 算法思想:归并排序是建立在归并操作上的一种有效的排序算法，该算法是采用分治法（Divide and Conquer）的一个非常典型的应用。将已有序的子序列合并，得到完全有序的序列；即先使每个子序列有序，再使子序列段间有序。若将两个有序表合并成一个有序表，称为二路归并。 时间复杂度: ) 。 //归并排序 void merge( vector& arr,int start,int mid,int end){ vector tempArr(end-start+1,0); int left = start; int right = mid+1; int index=0; while( left=arr[right]){ tempArr[index++]=arr[right++]; } } while(left& arr,int start,int end){ if(start 3.选择排序 选择排序是一种简单直观的排序算法。它的工作原理是每一次从待排序的数据元素中选出最小（或最大）的一个元素，存放在序列的起始位置，直到全部待排序的数据元素排完。 选择排序是不稳定的排序方法。 时间复杂度为： ) void select_sort( vector& arr,int start,int end ){ int swap=0; for (int i = 0; i 4.堆排序 时间复杂度为： ) 两个特性 结构性：用数组表示的完全二叉树 有序性：任一结点的关键字是其子树所有结点的最大值（或最小值） “最大堆(MaxHeap)”，也称\"大顶堆\"：最大值 最小堆(MinHeap)”，也称\"小顶堆\"：最小值 构建最大堆 建立堆的主要函数 存储结构体: struct node{ int *data; int size; }; 1 create() 初始化堆 typedef node *tree; tree create(int size){ tree t=new node; t->size=0; t->data=(int *)malloc(sizeof(int)*(size+10)); t->data[0]=-1000000;//设置哨兵值 return t; } 这里为什么要设置哨兵值呢?在下面分析 2 insert() 插入值 主要思路:往数组的最后一个位置+1插入值,然后和前面的父节点(因为是完全二叉树又是数组存储,其父节点可以直接用i/2表示)比较,如果比父节点大就和父节点替换位置 bool insert(tree t,int item){ int i=++t->size;//获取最后一个位置+1; for(;t->data[i/2]data[i]=t->data[i/2]; } t->data[i]=item;//最后在该位置插入值 return true; } 3delete() 删除根节点 思路:要删的肯定是根节点(最大堆就是删除一个最大值),接着取出数组中的最后一个元素,放在第一位,然后从第一位开始往后比较,比后面小,就和后面的子节点替换位置,同理,子节点用i2或者i2+1表示,所以判断的时候需要先比较子节点哪个比较大 这里要注意的是,if(child!=h->size)代表当前的节点是否还存在左右子节点,如果child==h->size,说明已经到了边界,那就没有左右子节点了 int Delete(tree h){ if(h->size==0){ coutdata[1]; int top=h->data[h->size--];//去除最后一个元素,放在根节点, //下面这个循环用来寻找放置top的位置 for(parent=1;parent*2size;parent=child){ child=parent*2; if(child!=h->size){//如果存在左右儿子的情况 if(h->data[child]data[child+1])child++;//找出左右儿子的较大者 } if(top>=h->data[child])break;//找到了 //否则让左右儿子较大的替换当前parent节点 else h->data[parent]=h->data[child]; } //最后放入 h->data[parent]=top; return maxData; } 构建堆的调整 将 N 个元素直接按顺序存入，再调整各结点的位置（简单说来，对于从最后一个有孩子结点的结点来说，其本身结点和孩子结点共同构成\"子最小堆\"，借助前面删除的想法，对每个\"子最小堆\"排序，当排序完成，整个最小堆也建立成功） #include using namespace std; #define maxdata 100000 struct node{ int * data; int size; int capacity; }; typedef node * tree; tree create(int maxsize){ tree h=new node; h->data=(int *)malloc(sizeof(int)*(maxsize+1)); h->size=0; h->capacity=maxsize; h->data[0]=maxdata; return h; } void LevelOrderTraversal(tree H){ int i; printf(\"层序遍历的结果是：\"); for(i = 1;isize;i++){ printf(\"%d \",H->data[i]); } printf(\"\\n\"); } void Heapify(tree h,int i){ int parent,child; int data=h->data[i]; for(parent=i;parent*2size;parent=child){ child=parent*2; if(child!=h->size){ if(h->data[child]data[child+1])child++; } if(h->data[child]data[parent]=h->data[child]; } h->data[parent]=data; } void adjust(tree h){ int i=h->size/2; for(;i>0;i--){ Heapify(h,i);//从最拥有子树的最小树开始--完全二叉树的特性,可以自己画个图验证一下 } } int main(){ tree h; h=create(100); int n; cin>>n; for(int i=0;i>h->data[++h->size]; } adjust(h); LevelOrderTraversal(h); } 5.希尔排序 在最坏的情况下时间复杂度为: O(n^2) 最好的情况下时间复杂度为： O(n) 平均情况下时间复杂度为：O(n^1.3) 希尔排序的实质就是分组插入排序，该方法又称缩小增量排序 该方法的基本思想是：先将整个待排元素序列分割成若干个子序列（由相隔某个“增量”的元素组成的）分别进行直接插入排序，然后依次缩减增量再进行排序，待整个序列中的元素基本有序（增量足够小）时，再对全体元素进行一次直接插入排序。因为直接插入排序在元素基本有序的情况下（接近最好情况），效率是很高的，因此希尔排序在时间效率上比前两种方法有较大提高。 以n=10的一个数组49, 38, 65, 97, 26, 13, 27, 49, 55, 4为例 第一次 gap = 10 / 2 = 5 49 38 65 97 26 13 27 49 55 4 1A 1B ​ 2A 2B ​ 3A 3B ​ 4A 4B ​ 5A 5B 1A,1B，2A,2B等为分组标记，数字相同的表示在同一组，大写字母表示是该组的第几个元素， 每次对同一组的数据进行直接插入排序。即分成了五组(49, 13) (38, 27) (65, 49) (97, 55) (26, 4)这样每组排序后就变成了(13, 49) (27, 38) (49, 65) (55, 97) (4, 26)，下同。 第二次 gap = 5 / 2 = 2 排序后 13 27 49 55 4 49 38 65 97 26 1A 1B 1C 1D 1E ​ 2A 2B 2C 2D 2E 第三次 gap = 2 / 2 = 1 4 26 13 27 38 49 49 55 97 65 1A 1B 1C 1D 1E 1F 1G 1H 1I 1J 第四次 gap = 1 / 2 = 0 排序完成得到数组： 4 13 26 27 38 49 49 55 65 97 下面给出严格按照定义来写的希尔排序 //4.希尔排序 void shell_sort( vector& arr , int n){ for( int gap=n/2; gap>0 ;gap/=2){ for( int i=gap ;i=0 && arr[j-gap] > sentinel ){ arr[j]= arr[j-gap]; j-=gap; } arr[j]= sentinel; } } } } 6.计数排序 有这样一道排序题：数组里有20个随机数，取值范围为从0到10，要求用最快的速度把这20个整数从小到大进行排序。 第一时间你可能会想使用快速排序，因为快排的时间复杂度只有O(nlogn)。但是这种方法还是不够快，有没有比O(nlogn)更快的排序方法呢？你可能会有疑问：O(nlogn)已经是最快的排序算法了，怎么可能还有更快的排序方法？ 让我们先来回顾一下经典的排序算法，无论是归并排序，冒泡排序还是快速排序等等，都是基于元素之间的比较来进行排序的。但是有一种特殊的排序算法叫计数排序，这种排序算法不是基于元素比较，而是利用数组下标来确定元素的正确位置。 在刚才的题目里，随即整数的取值范围是从0到10，那么这些整数的值肯定是在0到10这11个数里面。于是我们可以建立一个长度为11的数组，数组下标从0到10，元素初始值全为0，如下所示： 先假设20个随机整数的值是：9, 3, 5, 4, 9, 1, 2, 7, 8，1，3, 6, 5, 3, 4, 0, 10, 9, 7, 9 让我们先遍历这个无序的随机数组，每一个整数按照其值对号入座，对应数组下标的元素进行加1操作。 比如第一个整数是9，那么数组下标为9的元素加1： 第二个整数是3，那么数组下标为3的元素加1： 继续遍历数列并修改数组...... 最终，数列遍历完毕时，数组的状态如下： 数组中的每一个值，代表了数列中对应整数的出现次数。 有了这个统计结果，排序就很简单了，直接遍历数组，输出数组元素的下标值，元素的值是几，就输出几次： 0, 1, 1, 2, 3, 3, 3, 4, 4, 5, 5, 6, 7, 7, 8, 9, 9, 9, 9, 10 显然，这个输出的数列已经是有序的了。 这就是计数排序的基本过程，它适用于一定范围的整数排序。在取值范围不是很大的情况下，它的性能在某些情况甚至快过那些O(nlogn)的排序，例如快速排序、归并排序。 计数排序的时间复杂度是O(n+k) 代码实现如下： //5.计数排序 void counting_sort( vector & arr ,int n){ int minE = arr[0]; int maxE = minE; for(auto k:arr){ if(k > maxE)maxE=k; if(k counting(maxE - minE + 1, 0); for(auto k:arr){ counting[k - minE]+=1; } int index=0; for(int i=0;i 7.基数排序 　通过基数排序对数组{53, 3, 542, 748, 14, 214, 154, 63, 616}，它的示意图如下： 在上图中，首先将所有待比较树脂统一为统一位数长度，接着从最低位开始，依次进行排序。 \\1. 按照个位数进行排序。 \\2. 按照十位数进行排序。 \\3. 按照百位数进行排序。 排序后，数列就变成了一个有序序列。 基数排序代码 ;) /* * 获取数组a中最大值 * * 参数说明： * a -- 数组 * n -- 数组长度 */ int get_max(int a[], int n) { int i, max; max = a[0]; for (i = 1; i max) max = a[i]; return max; } /* * 对数组按照\"某个位数\"进行排序(桶排序) * * 参数说明： * a -- 数组 * n -- 数组长度 * exp -- 指数。对数组a按照该指数进行排序。 * * 例如，对于数组a={50, 3, 542, 745, 2014, 154, 63, 616}； * (01) 当exp=1表示按照\"个位\"对数组a进行排序 * (02) 当exp=10表示按照\"十位\"对数组a进行排序 * (03) 当exp=100表示按照\"百位\"对数组a进行排序 * ... */ void count_sort(int a[], int n, int exp) { int output[n]; // 存储\"被排序数据\"的临时数组 int i, buckets[10] = {0}; // 将数据出现的次数存储在buckets[]中 for (i = 0; i = 0; i--) { output[buckets[ (a[i]/exp)%10 ] - 1] = a[i]; buckets[ (a[i]/exp)%10 ]--; } // 将排序好的数据赋值给a[] for (i = 0; i 0; exp *= 10) count_sort(a, n, exp); } ;) radix_sort(a, n)的作用是对数组a进行排序。 首先通过get_max(a)获取数组a中的最大值。获取最大值的目的是计算出数组a的最大指数 获取到数组a中的最大指数之后，再从指数1开始，根据位数对数组a中的元素进行排序。排序的时候采用了桶排序。 count_sort(a, n, exp)的作用是对数组a按照指数exp进行排序。 下面简单介绍一下对数组{53, 3, 542, 748, 14, 214, 154, 63, 616}按个位数进行排序的流程。 (01) 个位的数值范围是[0,10)。因此，参见桶数组buckets[]，将数组按照个位数值添加到桶中。 (02) 接着是根据桶数组buckets[]来进行排序。假设将排序后的数组存在output[]中；找出output[]和buckets[]之间的联系就可以对数据进行排序了。 8.桶排序 一句话总结：划分多个范围相同的区间，每个子区间自排序，最后合并。 桶排序是计数排序的扩展版本，计数排序可以看成每个桶只存储相同元素，而桶排序每个桶存储一定范围的元素，通过映射函数，将待排序数组中的元素映射到各个对应的桶中，对每个桶中的元素进行排序，最后将非空桶中的元素逐个放入原序列中。 桶排序需要尽量保证元素分散均匀，否则当所有数据集中在同一个桶中时，桶排序失效。 三、核心代码 public static void bucketSort(int[] arr){ // 计算最大值与最小值 int max = Integer.MIN_VALUE; int min = Integer.MAX_VALUE; for(int i = 0; i > bucketArr = new ArrayList<>(bucketNum); for(int i = 0; i ()); } // 将每个元素放入桶 for(int i = 0; i 9.直接插入排序 将一个记录插入到已排好序的序列中，从而得到一个新的有序序列（将序列的第一个数据看成是一个有序的子序列，然后从第二个记录逐个向该有序的子序列进行有序的插入，直至整个序列有序） 重点：使用哨兵，用于临时存储和判断数组边界。 2 排序流程图 3算法实现 java import java.util.Arrays; public class Sort { public static void main(String[] args) { int arr[] = {2,1,5,3,6,4,9,8,7}; int temp; for (int i=1;i=0;j--){ if (j>0 && arr[j-1]>temp) { arr[j]=arr[j-1]; }else { arr[j]=temp; break; } } } } System.out.println(Arrays.toString(arr)); } } 4 运行结果 10.总结 时间复杂度 稳定性分析 首先，排序算法的稳定性大家应该都知道，通俗地讲就是能保证排序前2个相等的数其在序列的前后位置顺序和排序后它们两个的前后位置顺序相同。在简单形式化一下，如果Ai = Aj, Ai原来在位置前，排序后Ai还是要在Aj位置前。 其次，说一下稳定性的好处。排序算法如果是稳定的，那么从一个键上排序，然后再从另一个键上排序，第一个键排序的结果可以为第二个键排序所用。基数排序就 是这样，先按低位排序，逐次按高位排序，低位相同的元素其顺序再高位也相同时是不会改变的。另外，如果排序算法稳定，对基于比较的排序算法而言，元素交换 的次数可能会少一些(个人感觉，没有证实)。 回到主题，现在分析一下常见的排序算法的稳定性，每个都给出简单的理由。 (1)冒泡排序 ​ 冒泡排序就是把小的元素往前调或者把大的元素往后调。比较是相邻的两个元素比较，交换也发生在这两个元素之间。所以，如果两个元素相等，我想你是不会再无 聊地把他们俩交换一下的；如果两个相等的元素没有相邻，那么即使通过前面的两两交换把两个相邻起来，这时候也不会交换，所以相同元素的前后顺序并没有改 变，所以冒泡排序是一种稳定排序算法。 (2)选择排序 选择排序是给每个位置选择当前元素最小的，比如给第一个位置选择最小的，在剩余元素里面给第二个元素选择第二小的，依次类推，直到第n-1个元素，第n个 元素不用选择了，因为只剩下它一个最大的元素了。那么，在一趟选择，如果当前元素比一个元素小，而该小的元素又出现在一个和当前元素相等的元素后面，那么 交换后稳定性就被破坏了。比较拗口，举个例子，序列5 8 5 2 9， 我们知道第一遍选择第1个元素5会和2交换，那么原序列中2个5的相对前后顺序就被破坏了，所以选择排序不是一个稳定的排序算法。 (3)插入排序 插入排序是在一个已经有序的小序列的基础上，一次插入一个元素。当然，刚开始这个有序的小序列只有1个元素，就是第一个元素。比较是从有序序列的末尾开 始，也就是想要插入的元素和已经有序的最大者开始比起，如果比它大则直接插入在其后面，否则一直往前找直到找到它该插入的位置。如果碰见一个和插入元素相 等的，那么插入元素把想插入的元素放在相等元素的后面。所以，相等元素的前后顺序没有改变，从原无序序列出去的顺序就是排好序后的顺序，所以插入排序是稳 定的。 (4)快速排序 快速排序有两个方向，左边的i下标一直往右走，当a[i] a[center_index]。如果i和j都走不动了，i j。 交换a[j]和a[center_index]，完成一趟快速排序。在中枢元素和a[j]交换的时候，很有可能把前面的元素的稳定性打乱，比如序列为 5 3 3 4 3 8 9 10 11， 现在中枢元素5和3(第5个元素，下标从1开始计)交换就会把元素3的稳定性打乱，所以快速排序是一个不稳定的排序算法，不稳定发生在中枢元素和a[j] 交换的时刻。 (5)归并排序 归并排序是把序列递归地分成短序列，递归出口是短序列只有1个元素(认为直接有序)或者2个序列(1次比较和交换),然后把各个有序的段序列合并成一个有 序的长序列，不断合并直到原序列全部排好序。可以发现，在1个或2个元素时，1个元素不会交换，2个元素如果大小相等也没有人故意交换，这不会破坏稳定 性。那么，在短的有序序列合并的过程中，稳定是是否受到破坏？没有，合并过程中我们可以保证如果两个当前元素相等时，我们把处在前面的序列的元素保存在结 果序列的前面，这样就保证了稳定性。所以，归并排序也是稳定的排序算法。 (6)基数排序 基数排序是按照低位先排序，然后收集；再按照高位排序，然后再收集；依次类推，直到最高位。有时候有些属性是有优先级顺序的，先按低优先级排序，再按高优 先级排序，最后的次序就是高优先级高的在前，高优先级相同的低优先级高的在前。基数排序基于分别排序，分别收集，所以其是稳定的排序算法。 (7)希尔排序(shell) 希尔排序是按照不同步长对元素进行插入排序，当刚开始元素很无序的时候，步长最大，所以插入排序的元素个数很少，速度很快；当元素基本有序了，步长很小， 插入排序对于有序的序列效率很高。所以，希尔排序的时间复杂度会比o(n^2)好一些。由于多次插入排序，我们知道一次插入排序是稳定的，不会改变相同元 素的相对顺序，但在不同的插入排序过程中，相同的元素可能在各自的插入排序中移动，最后其稳定性就会被打乱，所以shell排序是不稳定的。 (8)堆排序 我们知道堆的结构是节点i的孩子为2i和2i+1节点，大顶堆要求父节点大于等于其2个子节点，小顶堆要求父节点小于等于其2个子节点。在一个长为n 的序列，堆排序的过程是从第n/2开始和其子节点共3个值选择最大(大顶堆)或者最小(小顶堆),这3个元素之间的选择当然不会破坏稳定性。但当为n /2-1, n/2-2, ...1这些个父节点选择元素时，就会破坏稳定性。有可能第n/2个父节点交换把后面一个元素交换过去了，而第n/2-1个父节点把后面一个相同的元素没 有交换，那么这2个相同的元素之间的稳定性就被破坏了。所以，堆排序不是稳定的排序算法。 综上，得出结论: 选择排序、快速排序、希尔排序、堆排序不是稳定的排序算法，而冒泡排序、插入排序、归并排序和基数排序是稳定的排序算法。 "},"数据结构/5.排序列题.html":{"url":"数据结构/5.排序列题.html","title":"5.排序算法例题","keywords":"","body":"1.小孩的游戏2.topk问题3.输油管道问题1.小孩的游戏 输入列子: 3 99 23 99 输出: 999923 假设给了字符串99,23为什么要把99放在23之前呢? 因为99+23>23+99 所以就以这种方式排序即可: 也即排序的依据是a+b>b+a #include using namespace std; int main(){ int n; cin>>n; vector arr; string temp; for(int i=0;i>temp; arr.push_back(temp); } sort(arr.begin(),arr.end(),[](string s1,string s2){return s1+s2>s2+s1;}); string ans; for(auto k:arr){ ans+=k; } cout 2.topk问题 1.1 快速排序(o(N)) 快排切分时间复杂度分析： 因为我们是要找下标为k的元素，第一次切分的时候需要遍历整个数组 (0 ~ n) 找到了下标是 j 的元素，假如 k 比 j 小的话，那么我们下次切分只要遍历数组 (0~k-1)的元素就行啦，反之如果 k 比 j 大的话，那下次切分只要遍历数组 (k+1～n) 的元素就行啦，总之可以看作每次调用 partition 遍历的元素数目都是上一次遍历的 1/2，因此时间复杂度是 N + N/2 + N/4 + ... + N/N = 2N, 因此时间复杂度是 O(N) class Solution { public int[] getLeastNumbers(int[] arr, int k) { if (k == 0 || arr.length == 0) { return new int[0]; } // 最后一个参数表示我们要找的是下标为k-1的数 return quickSearch(arr, 0, arr.length - 1, k - 1); } private int[] quickSearch(int[] nums, int lo, int hi, int k) { // 每快排切分1次，找到排序后下标为j的元素，如果j恰好等于k就返回j以及j左边所有的数； int j = partition(nums, lo, hi); if (j == k) { return Arrays.copyOf(nums, j + 1); } // 否则根据下标j与k的大小关系来决定继续切分左段还是右段。 return j > k? quickSearch(nums, lo, j - 1, k): quickSearch(nums, j + 1, hi, k); } // 快排切分，返回下标j，使得比nums[j]小的数都在j的左边，比nums[j]大的数都在j的右边。 private int partition(int[] nums, int lo, int hi) { int v = nums[lo]; int i = lo, j = hi + 1; while (true) { while (++i = lo && nums[j] > v); if (i >= j) { break; } int t = nums[j]; nums[j] = nums[i]; nums[i] = t; } nums[lo] = nums[j]; nums[j] = v; return j; } } 1.2 堆排序(nlogk) 本题是求前 K 小，因此用一个容量为 K 的大根堆，每次 poll 出最大的数，那堆中保留的就是前 K 小啦（注意不是小根堆！小根堆的话需要把全部的元素都入堆，那是 O(NlogN)，就不是 O(NlogK)啦～～） 这个方法比快排慢 3.输油管道问题 就是找中位数 就是topk变成n/2 "},"数据结构/6.树论.html":{"url":"数据结构/6.树论.html","title":"6.树论","keywords":"","body":"一 ADT二叉搜索树1.定义2.主要函数3.代码实现二 字典树(前缀树)1.定义2.主要原理3.代码实现三 AVL平衡二叉树1.定义2.主要函数3.代码实现五 线段树1.什么是线段树？2.线段树的基本内容3.线段树的基本操作4.区间求和模板代码5.线段树离散化六 线索二叉树七 Haffman树1.Haffman编码一 ADT二叉搜索树 1.定义 二叉搜索树（BST）也称二叉排序树或二叉查找树 二叉搜索树：一棵二叉树，可以为空；如果不为空，满足以下性质： 非空左子树的所有键值小于其根结点的键值 非空右子树的所有键值大于其根结点的键值 左、右子树都是二叉搜索树 列如: 2.主要函数 1. 查找 查找从根结点开始，如果树为空，返回 NULL 若搜索树不为空，则根结点键值和 X 进行比较，并进行不同处理： 若 X 小于根结点键值，在左子树中继续查找 若 X 大于根结点键值，在右子树中继续查找 如 X 等于根节点键值，查找结束，返回指向此结点的指针 2. 查找最大和最小元素 最大元素一定是在树的最右分支的端结点上 最小元素一定是在树的最左分支的端结点上 3. 删除 删除的三种情况： 要删除的是叶结点：直接删除，并将其父结点指针置为 NULL 要删除的结点只有一个孩子结点：将其父结点的指针指向要删除结点的孩子结点 要删除的结点有左、右两棵子树：用右子树的最小元素或左子树的最大元素替代被删除结点 3.代码实现 #include #include using namespace std; typedef int ElementType; typedef struct TreeNode *BinTree; struct TreeNode{ ElementType Data; BinTree Left; BinTree Right; }; // 查找递归实现 BinTree Find(ElementType X,BinTree BST){ if(!BST) // 如果根结点为空，返回 NULL return NULL; if(X Data) // 比根结点小，去左子树查找 return Find(X,BST->Left); else if(BST->Data Right); else if(BST->Data == X) // 找到了 return BST; } // 查找非递归实现 BinTree IterFind(ElementType X,BinTree BST){ while(BST){ if(X Data) BST = BST->Left; else if(BST->Data Right; else if(BST->Data == X) // 找到了 return BST; } return NULL; } // 查找最小值的递归实现 BinTree FindMin(BinTree BST){ if(!BST) // 如果为空了，返回 NULL return NULL; else if(BST->Left) // 还存在左子树，沿左分支继续查找 return FindMin(BST->Left); else // 找到了 return BST; } // 查找最大值的非递归实现 BinTree FindMax(BinTree BST){ if(BST) // 如果不空 while(BST->Right) // 只要右子树还存在 BST = BST->Right; return BST; } // 插入 BinTree Insert(ElementType X,BinTree BST){ if(!BST){ // 如果为空，初始化该结点 BST = (BinTree)malloc(sizeof(struct TreeNode)); BST->Data = X; BST->Left = NULL; BST->Right = NULL; }else{ // 不为空 if(X Data) // 如果小，挂在左边 BST->Left = Insert(X,BST->Left); else if(BST->Data Right = Insert(X,BST->Right); // 如果相等，什么都不用做 } return BST; } // 删除 BinTree Delete(ElementType X,BinTree BST){ BinTree tmp; if(!BST) coutData) // X 比当前结点值小，在左子树继续查找删除 BST->Left = Delete(X,BST->Left); else if(BST->Data Right = Delete(X,BST->Right); else{ // 找到被删除结点 if(BST->Left && BST->Right){ // 被删除结点有俩孩子结点 tmp = FindMin(BST->Right); // 找到右子树中值最小的 BST->Data = tmp->Data; // 用找到的值覆盖当前结点 BST->Right = Delete(tmp->Data,BST->Right); // 把前面找到的右子树最小值结点删除 }else{ // 被删除结点只有一个孩子结点或没有孩子结点 tmp = BST; if(!BST->Left && !BST->Right) // 没有孩子结点 BST = NULL; else if(BST->Left && !BST->Right) // 只有左孩子结点 BST = BST->Left; else if(!BST->Left && BST->Right) // 只有右孩子结点 BST = BST->Right; free(tmp); } } return BST; } // 中序遍历 void InOrderTraversal(BinTree BT){ if(BT){ InOrderTraversal(BT->Left); // 进入左子树 coutData; // 打印根 InOrderTraversal(BT->Right); // 进入右子树 } } int main(){ BinTree BST = NULL; BST = Insert(5,BST); BST = Insert(7,BST); BST = Insert(3,BST); BST = Insert(1,BST); BST = Insert(2,BST); BST = Insert(4,BST); BST = Insert(6,BST); BST = Insert(8,BST); BST = Insert(9,BST); /* 5 /\\ 3 7 /\\ /\\ 1 4 6 8 \\ \\ 2 9 */ coutDataDataLeft->DataRight->Data 二 字典树(前缀树) 1.定义 Trie又被称为前缀树、字典树，所以当然是一棵树。上面这棵Trie树包含的字符串集合是{in, inn, int, tea, ten, to}。每个节点的编号是我们为了描述方便加上去的。树中的每一条边上都标识有一个字符。这些字符可以是任意一个字符集中的字符。比如对于都是小写字母的字符串，字符集就是’a’-‘z’；对于都是数字的字符串，字符集就是’0’-‘9’；对于二进制字符串，字符集就是0和1。 比如上图中3号节点对应的路径0123上的字符串是inn，8号节点对应的路径0568上的字符串是ten。终结点与集合中的字符串是一一对应的。 2.主要原理 下面我们来讲一下对于给定的字符串集合{W1, W2, W3, … WN}如何创建对应的Trie树。其实上Trie树的创建是从只有根节点开始，通过依次将W1, W2, W3, … WN插入Trie中实现的。所以关键就是之前提到的Trie的插入操作。 具体来说，Trie一般支持两个操作： \\1. Trie.insert(W)：第一个操作是插入操作，就是将一个字符串W加入到集合中。 \\2. Trie.search(S)：第二个操作是查询操作，就是查询一个字符串S是不是在集合中。 假设我们要插入字符串”in”。我们一开始位于根，也就是0号节点，我们用P=0表示。我们先看P是不是有一条标识着i的连向子节点的边。没有这条边，于是我们就新建一个节点，也就是1号节点，然后把1号节点设置为P也就是0号节点的子节点，并且将边标识为i。最后我们移动到1号节点，也就是令P=1。 这样我们就把”in”的i字符插入到Trie中了。然后我们再插入字符n，也是先找P也就是1号节点有没有标记为n的边。还是没有，于是再新建一个节点2，设置为P也就是1号节点的子节点，并且把边标识为n。最后再移动到P=2。这样我们就把n也插入了。由于n是”in”的最后一个字符，所以我们还需要将P=2这个节点标记为终结点。 现在我们再插入字符串”inn”。过程也是一样的，从P=0开始找标识为i的边，这次找到1号节点。于是我们就不用创建新节点了，直接移动到1号节点，也就是令P=1。再插入字符n，也是有2号节点存在，所以移动到2号节点，P=2。最后再插入字符n这时P没有标识为n的边了，所以新建3号节点作为2号节点的子节点，边标识为n，同时将3号节点标记为终结点： 将后面的字符串int tea ten to都插入之后，就得到了我们一开始给出的Trie： 3.代码实现 定义类 Trie class Trie { private: bool isEnd; Trie* next[26]; public: //方法将在下文实现... }; 插入 描述：向 Trie 中插入一个单词 word 实现：这个操作和构建链表很像。首先从根结点的子结点开始与 word 第一个字符进行匹配，一直匹配到前缀链上没有对应的字符，这时开始不断开辟新的结点，直到插入完 word 的最后一个字符，同时还要将最后一个结点isEnd = true;，表示它是一个单词的末尾。 void insert(string word) { Trie* node = this; for (char c : word) { if (node->next[c-'a'] == NULL) { node->next[c-'a'] = new Trie(); } node = node->next[c-'a']; } node->isEnd = true; } 查找 描述：查找 Trie 中是否存在单词 word 实现：从根结点的子结点开始，一直向下匹配即可，如果出现结点值为空就返回 false，如果匹配到了最后一个字符，那我们只需判断 node->isEnd即可。 bool search(string word) { Trie* node = this; for (char c : word) { node = node->next[c - 'a']; if (node == NULL) { return false; } } return node->isEnd; } 前缀匹配 描述：判断 Trie 中是或有以 prefix 为前缀的单词 实现：和 search 操作类似，只是不需要判断最后一个字符结点的isEnd，因为既然能匹配到最后一个字符，那后面一定有单词是以它为前缀的。 C++ bool startsWith(string prefix) { Trie* node = this; for (char c : prefix) { node = node->next[c-'a']; if (node == NULL) { return false; } } return true; } 到这我们就已经实现了 对 Trie 的一些基本操作，这样我们对 Trie 就有了进一步的理解。完整代码我贴在了文末。 总结 通过以上介绍和代码实现我们可以总结出 Trie 的几点性质： Trie 的形状和单词的插入或删除顺序无关，也就是说对于任意给定的一组单词，Trie 的形状都是唯一的。 查找或插入一个长度为 L 的单词，访问 next 数组的次数最多为 L+1，和 Trie 中包含多少个单词无关。 Trie 的每个结点中都保留着一个字母表，这是很耗费空间的。如果 Trie 的高度为 n，字母表的大小为 m，最坏的情况是 Trie 中还不存在前缀相同的单词，那空间复杂度就为 O(m^n) 最好情况下,时间复杂度为O(m) 最后，关于 Trie 的应用场景，希望你能记住 8 个字：一次建树，多次查询。(慢慢领悟叭~~) 全部代码 C++ class Trie { private: bool isEnd; Trie* next[26]; public: Trie() { isEnd = false; memset(next, 0, sizeof(next)); } void insert(string word) { Trie* node = this; for (char c : word) { if (node->next[c-'a'] == NULL) { node->next[c-'a'] = new Trie(); } node = node->next[c-'a']; } node->isEnd = true; } bool search(string word) { Trie* node = this; for (char c : word) { node = node->next[c - 'a']; if (node == NULL) { return false; } } return node->isEnd; } bool startsWith(string prefix) { Trie* node = this; for (char c : prefix) { node = node->next[c-'a']; if (node == NULL) { return false; } } return true; } }; 三 AVL平衡二叉树 1.定义 2.主要函数 1.RR单旋 当\"插入结点\"(BR)是\"被破坏平衡结点\"(A)右子树的右子树时，即 RR 插入时，采用 RR 旋转调整 其基本思路是把 B 的左子树腾出来挂到 A 的右子树上，返回 B 作为当前子树的根 AVLTree RRRotation(AVLTree A){ AVLTree B = A->right; // B 为 A 的右子树 A->right = B->left; // B 的左子树挂在 A 的右子树上 B->left = A; // A 挂在 B 的左子树上 return B; // 此时 B 为根结点了 } 2.LL单旋 当\"插入结点\"(BL)是\"被破坏平衡结点\"(A)左子树的左子树时，即 LL 插入，采用 RR 旋转调整 其基本思路是把 B 的右子树腾出来挂到 A 的左子树上，返回 B 作为当前子树的根 AVLTree LLRotation(AVLTree A){ // 此时根节点是 A AVLTree B = A->left; // B 为 A 的左子树 A->left = B->right; // B 的右子树挂在 A 的左子树上 B->right = A; // A 挂在 B 的右子树上 return B; // 此时 B 为根结点了 } 3.RL双旋 当\"插入结点\"(CL 或者 CR)是\"被破坏平衡结点\"(A)右子树的左子树时，即 RL 插入，采用 RL 旋转调整 基本思想是先将 B 作为根结点进行 LL 单旋转化为 RR 插入，再将 A 作为根结点进行 RR单旋（先 LL 再 RR） AVLTree RLRotation(AVLTree A){ // 先 LL 单旋 A->right = LLRotation(A->right); // 再 RR 单旋 return RRRotation(A); } 4.LR双旋转 基本思想是先将 B 作为根结点进行 RR 单旋转化为 LL 插入，再将 A 作为根结点进行 LL 单旋（先 RR 再 LL） AVLTree LRRotation(AVLTree A){ // 先 RR 单旋 A->left = RRRotation(A->left); // 再 LL 单旋 return LLRotation(A); } 3.代码实现 #include #include typedef struct AVLNode *AVLTree; struct AVLNode{ int data; // 存值 AVLTree left; // 左子树 AVLTree right; // 右子树 int height; // 树高 }; using namespace std; // 返回最大值 int Max(int a,int b){ return a>b?a:b; } // 返回树高，空树返回 -1 int getHeight(AVLTree A){ return A==NULL?-1:A->height; } // LL单旋 // 把 B 的右子树腾出来挂给 A 的左子树，再将 A 挂到 B 的右子树上去 AVLTree LLRotation(AVLTree A){ // 此时根节点是 A AVLTree B = A->left; // B 为 A 的左子树 A->left = B->right; // B 的右子树挂在 A 的左子树上 B->right = A; // A 挂在 B 的右子树上 A->height = Max(getHeight(A->left),getHeight(A->right)) + 1; B->height = Max(getHeight(B->left),A->height) + 1; return B; // 此时 B 为根结点了 } // RR单旋 AVLTree RRRotation(AVLTree A){ // 此时根节点是 A AVLTree B = A->right; A->right = B->left; B->left = A; A->height = Max(getHeight(A->left),getHeight(A->right)) + 1; B->height = Max(getHeight(B->left),A->height) + 1; return B; // 此时 B 为根结点了 } // LR双旋 AVLTree LRRotation(AVLTree A){ // 先 RR 单旋 A->left = RRRotation(A->left); // 再 LL 单旋 return LLRotation(A); } // RL双旋 AVLTree RLRotation(AVLTree A){ // 先 LL 单旋 A->right = LLRotation(A->right); // 再 RR 单旋 return RRRotation(A); } AVLTree Insert(AVLTree T,int x){ if(!T){ // 如果该结点为空，初始化结点 T = (AVLTree)malloc(sizeof(struct AVLNode)); T->data = x; T->left = NULL; T->right = NULL; T->height = 0; }else{ // 否则不为空， if(x data){ // 左子树 T->left = Insert(T->left,x); if(getHeight(T->left)-getHeight(T->right)==2){ // 如果左子树和右子树高度差为 2 if(x left->data) // LL 单旋 T = LLRotation(T); else if(T->left->data data right = Insert(T->right,x); if(getHeight(T->right)-getHeight(T->left)==2){ if(x right->data) // RL 双旋 T = RLRotation(T); else if(T->right->data height = Max(getHeight(T->left),getHeight(T->right)) + 1; return T; } int main(){ AVLTree T=NULL; int n; cin>>n; for(int i=0;i>tmp; T = Insert(T,tmp); } coutdata; return 0; } Sample Input : 7 88 70 61 96 120 90 65 Sample Output : 88 五 线段树 1.什么是线段树？ 线段树是怎样的树形结构? 　　线段树是一种二叉搜索树，什么叫做二叉搜索树，首先满足二叉树，每个结点度小于等于二，即每个结点最多有两颗子树，何为搜索，我们要知道，线段树的每个结点都存储了一个区间，也可以理解成一个线段，而搜索，就是在这些线段上进行搜索操作得到你想要的答案。 线段树能够解决什么样的问题。 　　线段树的适用范围很广，可以在线维护修改以及查询区间上的最值，求和。更可以扩充到二维线段树（矩阵树）和三维线段树（空间树）。对于一维线段树来说，每次更新以及查询的时间复杂度为O(logN)。 线段树和其他RMQ算法的区别 　　常用的解决RMQ问题有ST算法，二者预处理时间都是O(NlogN)，而且ST算法的单次查询操作是O(1)，看起来比线段树好多了，但二者的区别在于线段树支持在线更新值，而ST算法不支持在线操作。 　　这里也存在一个误区，刚学线段树的时候就以为线段树和树状数组差不多，用来处理RMQ问题和求和问题，但其实线段树的功能远远不止这些，我们要熟练的理解线段这个概念才能更加深层次的理解线段树。 2.线段树的基本内容 　　现在请各位不要带着线段树只是为了解决区间问题的数据结构，事实上，是线段树多用于解决区间问题，并不是线段树只能解决区间问题，首先，我们得先明白几件事情。 　　每个结点存什么，结点下标是什么，如何建树。 　　下面我以一个简单的区间最大值来阐述上面的三个概念。 　　对于A[1:6] = {1,8,6,4,3,5}来说，线段树如上所示，红色代表每个结点存储的区间，蓝色代表该区间最值。 　　可以发现，每个叶子结点的值就是数组的值，每个非叶子结点的度都为二，且左右两个孩子分别存储父亲一半的区间。每个父亲的存储的值也就是两个孩子存储的值的最大值。 　　上面的每条结论应该都容易看出来。那么结点到底是如何存储区间的呢，以及如何快速找到非叶子结点的孩子以及非根节点的父亲呢，这里也就是理解线段树的重点以及难点所在，如同树状数组你理解了lowbit就能很快理解树状数组一样，线段树你只要理解了结点与结点之间的关系便能很快理解线段树的基本知识。 　　对于一个区间[l,r]来说，最重要的数据当然就是区间的左右端点l和r，但是大部分的情况我们并不会去存储这两个数值，而是通过递归的传参方式进行传递。这种方式用指针好实现，定义两个左右子树递归即可，但是指针表示过于繁琐，而且不方便各种操作，大部分的线段树都是使用数组进行表示，那这里怎么快速使用下标找到左右子树呢。 　　对于上述线段树，我们增加绿色数字为每个结点的下标 　　则每个结点下标如上所示，这里你可能会问，为什么最下一排的下标直接从9跳到了12，道理也很简单，中间其实是有两个空间的呀！！虽然没有使用，但是他已经开了两个空间，这也是为什么无优化的线段树建树需要22k（2k-1 n的空间防止RE。 　　仔细观察每个父亲和孩子下标的关系，有发现什么联系吗？不难发现，每个左子树的下标都是偶数，右子树的下标都是奇数且为左子树下标+1，而且不难发现以下规律 l = fa*2 （左子树下标为父亲下标的两倍） r = fa*2+1（右子树下标为父亲下标的两倍+1） 　　具体证明也很简单，把线段树看成一个完全二叉树（空结点也当作使用）对于任意一个结点k来说，它所在此二叉树的log2（k） 层，则此层共有2log2(k)个结点，同样对于k的左子树那层来说有2log2(k)+1个结点，则结点k和左子树间隔了22log2(k)-k + 2(k-2log2(k))个结点，然后这就很简单就得到k+22log2(k)-k + 2(k-2log2(k)) = 2*k的关系了吧，右子树也就等于左子树结点+1。 　　是不是觉得其实很简单，而且因为左子树都是偶数，所以我们常用位运算来寻找左右子树 k k 　整理一下思绪，现在已经明白了数组如何存在线段树，结点间的关系，以及使用递归的方式建立线段树，那么具体如何建立线段树，我们来看代码，代码中不清楚的地方都有详细的注释说明。 1 const int maxn = 1000; 2 int a[maxn],t[maxn>1); //m则为中间点，左儿子的结点区间为[l,m],右儿子的结点区间为[m+1,r] 14 build(k 　　现在再来看代码，是不是觉得清晰很多了，使用递归的方法建立线段树，确实清晰易懂，各位看到这里也请自己试着实现一下递归建树，若是哪里有卡点再来看一下代码找到哪里出了问题。那线段树有没有非递归的方式建树呢，答案是有，但是非递归的建树方式会使得线段树的查询等操作和递归建树方式完全不一样，由简至难，后面我们再说非递归方式的实现。 　　到现在你应该可以建立一颗线段树了，而且知道每个结点存储的区间和值，如果上述操作还不能实现或是有哪里想不明白，建议再翻回去看一看所讲的内容。不要急于看完，理解才更重要。 3.线段树的基本操作 　　基本操作有哪些，你应该也能想出来，在线的二叉搜索树，所拥有的操作当然有，更新和询问两种。 1.点更新 　　如何实现点更新，我们先不急看代码，还是对于上面那个线段树，假使我把a[3]+7，则更新后的线段树应该变成 　　更新了a[3]后，则每个包含此值的结点都需要更新，那么有多少个结点需要更新呢？根据二叉树的性质，不难发现是log(k)个结点，这也正是为什么每次更新的时间复杂度为O(logN)，那应该如何实现呢，我们发现，无论你更新哪个叶子节点，最终都是会到根结点的，而把这个往上推的过程逆过来就是从根结点开始，找到左子树还是右子树包含需要更新的叶子节点，往下更新即可，所以我们还是可以使用递归的方法实现线段树的点更新 1 //递归方式更新 updata(p,v,1,n,1); 2 void updata(int p,int v,int l,int r,int k){ //p为下标，v为要加上的值，l，r为结点区间，k为结点下标 3 if(l == r) //左端点等于右端点，即为叶子结点，直接加上v即可 4 a[k] += v,t[k] += v; //原数组和线段树数组都得到更新 5 else{ 6 int m = l + ((r-l)>>1); //m则为中间点，左儿子的结点区间为[l,m],右儿子的结点区间为[m+1,r] 7 if(p 　　看完代码是不是很清晰，这里也建议自己再次手动实现一遍理解递归的思路。 2.区间查询 　　说完了单点更新肯定就要来说区间查询了，我们知道线段树的每个结点存储的都是一段区间的信息 ，如果我们刚好要查询这个区间，那么则直接返回这个结点的信息即可，比如对于上面线段树，如果我直接查询[1,6]这个区间的最值，那么直接返回根节点信息返回13即可，但是一般我们不会凑巧刚好查询那些区间，比如现在我要查询[2,5]区间的最值，这时候该怎么办呢，我们来看看哪些区间是[2,5]的真子集， 　　一共有5个区间，而且我们可以发现[4,5]这个区间已经包含了两个子树的信息，所以我们需要查询的区间只有三个，分别是[2,2],[3,3],[4,5]，到这里你能通过更新的思路想出来查询的思路吗? 我们还是从根节点开始往下递归，如果当前结点是要查询的区间的真子集，则返回这个结点的信息且不需要再往下递归了，这样从根节点往下递归，时间复杂度也是O(logN)。那么代码则为 1 //递归方式区间查询 query(L,R,1,n,1); 2 int query(int L,int R,int l,int r,int k){ //[L,R]即为要查询的区间，l，r为结点区间，k为结点下标 3 if(L >1); //m则为中间点，左儿子的结点区间为[l,m],右儿子的结点区间为[m+1,r] 8 if(L m) //如果右子树和需要查询的区间交集非空，注意这里不是else if，因为查询区间可能同时和左右区间都有交集 11 res = max(res, query(L,R,m+1,r,k　　如果你能理解建树和更新的过程，那么这里的区间查询也不会太难理解。还是建议再次手动实现。 3.区间更新 　　树状数组中的区间更新我们用了差分的思想，而线段树的区间更新相对于树状数组就稍微复杂一点，这里我们引进了一个新东西，Lazy_tag，字面意思就是懒惰标记的意思，实际上它的功能也就是偷懒= =，因为对于一个区间[L,R]来说，我们可能每次都更新区间中的没个值，那样的话更新的复杂度将会是O(NlogN)，这太高了，所以引进了Lazy_tag，这个标记一般用于处理线段树的区间更新。 　　线段树在进行区间更新的时候，为了提高更新的效率，所以每次更新只更新到更新区间完全覆盖线段树结点区间为止，这样就会导致被更新结点的子孙结点的区间得不到需要更新的信息，所以在被更新结点上打上一个标记，称为lazy-tag，等到下次访问这个结点的子结点时再将这个标记传递给子结点，所以也可以叫延迟标记。 　　也就是说递归更新的过程，更新到结点区间为需要更新的区间的真子集不再往下更新，下次若是遇到需要用这下面的结点的信息，再去更新这些结点，所以这样的话使得区间更新的操作和区间查询类似，复杂度为O(logN)。 1 void Pushdown(int k){ //更新子树的lazy值，这里是RMQ的函数，要实现区间和等则需要修改函数内容 2 if(lazy[k]){ //如果有lazy标记 3 lazy[k>1); 20 if(L 　　注意看Pushdown这个函数，也就是当需要查询某个结点的子树时，需要用到这个函数，函数功能就是更新子树的lazy值，可以理解为平时先把事情放着，等到哪天要检查的时候，就临时再去做，而且做也不是一次性做完，检查哪一部分它就只做这一部分。是不是感受到了什么是Lazy_tag，实至名归= =。 　　值得注意的是，使用了Lazy_tag后，我们再进行区间查询也需要改变。区间查询的代码则变为 1 //递归方式区间查询 query(L,R,1,n,1); 2 int query(int L,int R,int l,int r,int k){ //[L,R]即为要查询的区间，l，r为结点区间，k为结点下标 3 if(L >1); //m则为中间点，左儿子的结点区间为[l,m],右儿子的结点区间为[m+1,r] 9 if(L m) //如果右子树和需要查询的区间交集非空，注意这里不是else if，因为查询区间可能同时和左右区间都有交集 12 res = max(res, query(L,R,m+1,r,k　　其实变动也不大，就是多了一个临时更新子树的值的过程。 4.区间求和模板代码 class NumArray { public: #define ls k t; vector lazy; int n; vector nums; NumArray(vector& nums) {; n=nums.size(); if(n==0)return; t=vector(n*4,0); lazy=vector(n*4,0); this->nums=nums; build(1,0,n-1); } //push up void pushUp(int k){ t[k]=t[ls]+t[rs]; } //递归方式建树 void build(int k,int l,int r){ //k为当前需要建立的结点，l为当前需要建立区间的左端点，r则为右端点 if(l == r) //左端点等于右端点，即为叶子节点，直接赋值即可 t[k] = nums[l]; else{ int m = l + ((r-l)>>1); //m则为中间点，左儿子的结点区间为[l,m],右儿子的结点区间为[m+1,r] build(ls,l,m); //递归构造左儿子结点 build(rs,m+1,r); //递归构造右儿子结点 pushUp(k); //更新父节点 } } //点更新 //递归方式更新 update_point(p,v,1,n,1); void update_point(int p, int v, int l, int r, int k){ //p为下标，v为要加上的值，l，r为结点区间，k为结点下标 if(l == r) //左端点等于右端点，即为叶子结点，直接加上v即可 t[k] += v; //线段树数组都得到更新 else{ //先删除标记 pushdown(k); int m = l + ((r-l)>>1); //m则为中间点，左儿子的结点区间为[l,m],右儿子的结点区间为[m+1,r] if(p =i&&r=i) { update_range(i,j,v,l,mid,ls); } else if(mid>1); //m则为中间点，左儿子的结点区间为[l,m],右儿子的结点区间为[m+1,r] if(L m) //如果右子树和需要查询的区间交集非空，注意这里不是else if，因为查询区间可能同时和左右区间都有交集 res += range_sum(L,R,m+1,r,rs); return res; //返回当前结点得到的信息 } } }; 5.线段树离散化 建立一棵[1,1QW]的线段树，其根系是非常庞大的，TLE和MLE是铁定的了。所以必须离散化。 通俗点说，离散化就是压缩区间，使原有的长区间映射到新的短区间，但是区间压缩前后的覆盖关系不变。举个例子： 有一条1到10的数轴（长度为9），给定4个区间[2,4] [3,6] [8,10] [6,9]，覆盖关系就是后者覆盖前者，每个区间染色依次为 1 2 3 4。 现在我们抽取这4个区间的8个端点，2 4 3 6 8 10 6 9 然后删除相同的端点，这里相同的端点为6，则剩下2 4 3 6 8 10 9 对其升序排序，得2 3 4 6 8 9 10 然后建立映射 2 3 4 6 8 9 10 ↓ ↓ ↓ ↓ ↓ ↓ ↓ 1 2 3 4 5 6 7 那么新的4个区间为 [1,3] [2,4] [5,7] [4,6]，覆盖关系没有被改变。新数轴为1到7，即原数轴的长度从9压缩到6，显然构造[1,7]的线段树比构造[1,10]的线段树更省空间，搜索也更快，但是求解的结果却是一致的。 离散化时有一点必须要注意的，就是必须先剔除相同端点后再排序，这样可以减少参与排序元素的个数，节省时间。 六 线索二叉树 为什么要对二叉树进行线索化？ 对二叉树进行遍历是以一定的规则将二叉树的节点排列成一个线性序列，这些线性序列有且仅有一个直接前驱和直接后继，但是以二叉链表进行存储的时候，只能找到节点的左右孩子信息，不能直接得到节点在任一序列的前驱和后继信息，前驱和后继信息只能在遍历的动态过程中才能得到。 如何获取到前驱和后继的信息呢？ 在有n个节点的二叉链表中必定存在n+1个空链域，利用这些空链域存放节点的前驱和后继信息，所以在每个节点上增加两个指针域分别指向节点在任一序列中的前驱和后继信息。 节点结构： png) 以上述节点结构构成的二叉链表作为二叉链表的存储结构，叫做线索化链表； 指向节点前驱和后继的指针称为线索； 加上线索的二叉树称为线索二叉树； 对二叉树以某种次序遍历使其成为线索二叉树的过程称为线索化。 线索化的总思路：如果访问到当前节点的时候，线索化其左子树并记录下上一个访问的节点；再对上一个访问的节点的右子树进行线索化，直到所有节点都访问完。 //prev需要传引用进去，才能记录下上次访问的节点，否则节点的右子树的线索化不能完成 void _PrevOrder_Th(Node* root, Node*& prev) { if (root == NULL) { return; } if (root->_LeftNode == NULL) { root->_LeftNode = prev; root->LeftTag = THREAD; } if (prev&&prev->_RightNode == NULL) { prev->_RightNode = root; prev->RightTag = THREAD; } prev = root; if (root->LeftTag == LINK) { _PrevOrder_Th(root->_LeftNode, prev); } if (root->RightTag == LINK) { _PrevOrder_Th(root->_RightNode, prev); } } 前序线索化的遍历： 遇到先对其进行进行访问，再对其左子树进行遍历访问，直到找到最左的那个节点；再根据线索化的指向对其右子树进行遍历访问。 void PreOrder() { Node* cur = _root; while (cur) { while (cur->LeftTag == LINK) { cout _data_LeftNode; } cout _data_RightNode; } cout 中序线索化： void _InOrder_Th(Node* root, Node*& prev) { if (root == NULL) { return; } else { _InOrder_Th(root->_LeftNode, prev); if (root->_LeftNode == NULL) { root->_LeftNode = prev; root->LeftTag = THREAD; } if (prev&&prev->_RightNode == NULL) { prev->_RightNode = root; prev->RightTag = THREAD; } prev = root; _InOrder_Th(root->_RightNode, prev); } } 中序线索化的遍历： void InOrder() { Node* cur = _root; while (cur) { //找到最左的点 while (cur->LeftTag == LINK) { cur = cur->_LeftNode; } cout _data RightTag == THREAD) { cur = cur->_RightNode; cout _data _RightNode; } cout 七 Haffman树 1.Haffman编码 哈夫曼编码，主要目的是根据使用频率来最大化节省字符（编码）的存储空间。 简易的理解就是，假如我有A,B,C,D,E五个字符，出现的频率（即权值）分别为5,4,3,2,1,那么我们第一步先取两个最小权值作为左右子树构造一个新树，即取1，2构成新树，其结点为1+2=3，如图： 虚线为新生成的结点，第二步再把新生成的权值为3的结点放到剩下的集合中，所以集合变成{5,4,3,3}，再根据第二步，取最小的两个权值构成新树，如图： 再依次建立哈夫曼树，如下图： 其中各个权值替换对应的字符即为下图： 所以各字符对应的编码为：A->11,B->10,C->00,D->011,E->010 霍夫曼编码是一种无前缀编码。解码时不会混淆。其主要应用在数据压缩，加密解密等场合。 如果考虑到进一步节省存储空间，就应该将出现概率大（占比多）的字符用尽量少的0-1进行编码，也就是更靠近根（节点少），这也就是最优二叉树-哈夫曼树。 "},"数据结构/7.树论例题.html":{"url":"数据结构/7.树论例题.html","title":"7.树论例题.md","keywords":"","body":"一般树题目集1.橙子树2.不同的二叉搜索树树的递归类型0.二叉树展开为链表⭐1.验证二叉搜索树2.路径总和 II3.路径总和 III3.二叉树中的最大路径和4.二叉树的最近公共祖先5.打家劫舍 III6.好叶子节点对的数量二叉搜索树列题1.地鼠安家线状树列题1 区域和检索 2.区间和的个数3.天际线问题4.翻转对树状数组列题字典树列题1.模板题目2.添加与搜索单词 3.单词搜索 II4.数组中两个数的最大异或值5.回文对一般树题目集 1.橙子树 5 2 1 2 1 2 3 2 3 4 2 4 5 1 输出: 7 题目分析 算法选择 这里选择用dijkstra求最大路径长度 代码实现 #include using namespace std; typedef pair ll; const int MAXN=1e5+10; vector graph[MAXN]; vector dist(MAXN,0); vector vis(MAXN,0); priority_queue,less> qu; //优先队列 int sumL=0;//总的边权 int maxL=0; //离源点最大长度 void add(int a,int b,int v){ graph[a].push_back({b,v}); graph[b].push_back({a,v}); sumL+=v; } void dijkstra(int start) { qu.push({0,start}); while(!qu.empty()) { int cur=qu.top().second; qu.pop(); if (vis[cur])continue; vis[cur]=true; maxL=max(maxL,dist[cur]); //对每一个邻接边 for(int i=0;idist[child.first]) { dist[child.first]=dist[cur]+child.second; qu.push({dist[child.first],child.first}); } } } } } int main() { int n,start; cin>>n>>start; int a,b,v; for(int i=0;i>a>>b>>v; add(a,b,v); } dijkstra(start); cout 2.不同的二叉搜索树 题目分析 本题主要考察dp,bst的性质 以i为根节点,则其左子树为[0,i-1],右子树为[i+1,n] 所以num[i]=dp[i-1]+dp[n-i] (i=0,1,2,....n); dp边界是dp[0]=1 代码: class Solution { public: int numTrees(int n) { vector dp(n+1,0); dp[0]=1; for (int i = 1; i 树的递归类型 0.二叉树展开为链表⭐ 题目分析 递归的常见思路: 相信一个函数,能提供将以root为根的树,展开为链表 于是我们可以借助此函数得到左右两根链表 最后变化一下即可 一定要把递归想象成一个抽象的过程,就当作我们已经有两根链表了,要做的无非是遍历到左边那根链表的最右边那个节点,然后把右边那根链表接上即可 class Solution { public: typedef TreeNode * node; node dfs(node root){ if(!root)return NULL; node left=dfs(root->left); node right=dfs(root->right); root->right=left; //遍历到最右边一个节点 node temp=root; while(temp->right){ temp=temp->right; } temp->right=right; root->left=NULL; return root; } void flatten(TreeNode* root) { dfs(root); } }; 1.验证二叉搜索树 对于BST,无非左子树比当前小,右子树比当前大,逃不过这个性质,关于BST的都可以往这个方向套 所以,很容易得出递归的思路: /** * Definition for a binary tree node. * struct TreeNode { * int val; * TreeNode *left; * TreeNode *right; * TreeNode(int x) : val(x), left(NULL), right(NULL) {} * }; */ class Solution { public: typedef TreeNode * node; bool dfs(node root,long lower,long upper){ if(!root)return true; int val=root->val; if(val=upper)return false; return dfs(root->left,lower,val)&&dfs(root->right,val,upper); } bool isValidBST(TreeNode* root) { return dfs(root,LONG_MIN,LONG_MAX); } }; 2.路径总和 II 题目分析 这是一道很常见的dfs回朔类型的题目 struct TreeNode{ int val; TreeNode * left; TreeNode * right; TreeNode() : val(0), left(nullptr), right(nullptr) {} TreeNode(int x) : val(x), left(nullptr), right(nullptr) {} }; typedef TreeNode * node; vector> result; void dfs(node root,int cur,vector arr) { if(!root)return; int val=root->val; cur-=val; arr.push_back(val); if (!root->left&&!root->right&&cur==0) { result.push_back(arr); //注意回朔 arr.pop_back(); return; } dfs(root->left,cur,arr); dfs(root->right,cur,arr); //注意回朔 arr.pop_back(); } vector> pathSum(TreeNode* root, int sum) { dfs (root,sum,vector{}); return result; } 3.路径总和 III 题目分析 这个题目就是对每个顶点,有两种情况: 继承上一个顶点,查看当前的值是否和剩下的target相同 传递至左右顶点,在左右孩子中寻找路径 另外,dfs含义: 以当前根为顶点,寻找目标为sum的路径 dfs函数需要对每个顶点都调用一次 /** * Definition for a binary tree node. * struct TreeNode { * int val; * TreeNode *left; * TreeNode *right; * TreeNode() : val(0), left(nullptr), right(nullptr) {} * TreeNode(int x) : val(x), left(nullptr), right(nullptr) {} * TreeNode(int x, TreeNode *left, TreeNode *right) : val(x), left(left), right(right) {} * }; */ class Solution { public: typedef TreeNode * node; int dfs(node root,int sum){ if(!root)return 0; int ans=0; int val=root->val; //当前点 if(val==sum){ ans++; } //左右 ans+=dfs(root->left,sum-val); ans+=dfs(root->right,sum-val); return ans; } int pathSum(TreeNode* root, int sum) { if(!root)return 0; return dfs(root,sum)+pathSum(root->left,sum)+pathSum(root->right,sum); } }; 3.二叉树中的最大路径和 题目分析 同样根据第一题的思路: 相信一个函数dfs,它可以返回以root为根节点的最大路径 也即我们就可以得到左右两颗树各自的最大路径,那对于当前节点root,其最大路径就是left+cur+right,并更新答案 但是返回的,是左右最大的路径+cur,而不是返回最大路径 /** * Definition for a binary tree node. * struct TreeNode { * int val; * TreeNode *left; * TreeNode *right; * TreeNode() : val(0), left(nullptr), right(nullptr) {} * TreeNode(int x) : val(x), left(nullptr), right(nullptr) {} * TreeNode(int x, TreeNode *left, TreeNode *right) : val(x), left(left), right(right) {} * }; */ class Solution { public: typedef TreeNode * node; int answer=INT_MIN; int dfs(node root) { if (!root)return 0; int left=dfs(root->left); //如果左右小于0,则直接抛弃左右即可,也即赋值为0 if (leftright); if(rightval; //跟新答案 answer=max(answer,left+right+val); //返回左右最大+cur return max(left,right)+val; } int maxPathSum(TreeNode* root) { dfs(root); return answer; } }; 4.二叉树的最近公共祖先 这种题目主要分清楚几种情况即可 TreeNode* lowestCommonAncestor(TreeNode* root, TreeNode* p, TreeNode* q) { if (!root)return NULL; //如果当前节点和p,q某一个相等,说明当前节点就是要找的点 if (root==p||root==q)return root; //从左右中寻找 node left=lowestCommonAncestor(root->left,p,q); node right=lowestCommonAncestor(root->right,p,q); //都不为空,说明p,q分散在左右中,当前为父节点 if (left&&right)return root; else return left?left:right; } 5.打家劫舍 III 题目分析 这是一道递归+dp思想的题目 状态无非两个: 如果抢当前的节点,则左右孩子不能碰 不抢当前点,则可以碰左右孩子 接着,递归的思想,相信一个函数,其可以返回以一个root为根的点所能抢的最大佳值 /** * Definition for a binary tree node. * struct TreeNode { * int val; * TreeNode *left; * TreeNode *right; * TreeNode(int x) : val(x), left(NULL), right(NULL) {} * }; */ class Solution { public: typedef TreeNode * node; //备忘录 map dict; int dfs(node root) { if (!root) return 0; if(dict.find(root)!=dict.end()) return dict[root]; //如果抢当前点,则左右孩子点不能抢 int do_it= root->val; if(root->left)do_it+=dfs(root->left->left)+dfs(root->left->right); if(root->right)do_it+=dfs(root->right->left)+dfs(root->right->right); //不抢当前点,只能抢左右孩子点 int no_do=dfs(root->left)+dfs(root->right); int result= max(do_it,no_do); dict[root]=result; return result; } int rob(TreeNode* root) { return dfs(root); } }; 6.好叶子节点对的数量 题目分析 基本思路如下: 找到当前节点的所有左右叶子节点距离当前节点的距离 对两个数组进行两两匹配,看是否在distance范围内 struct TreeNode{ int val; TreeNode *left; TreeNode * right; TreeNode(int x):val(x),left(NULL),right(NULL){ } }; typedef TreeNode * node; int ans=0; vector dfs(node root,int top){ vector result{}; if(!root)return result; //如果当前是叶子节点,就返回当前距离1 if(!root->left&&!root->right) { result.push_back(1); return result; } //获取左右的叶子数组 vector left=dfs(root->left,top); vector right=dfs(root->right,top);; if(left.size()>0&&right.size()>0) { //两两匹配 for (auto l:left) { for (auto r:right) { if(l+rtop)continue; result.push_back(l+1); } for(auto r:right){ if(r+1>top)continue; result.push_back(r+1); } return result; } int countPairs(TreeNode* root, int distance) { dfs(root,distance); return ans; } 二叉搜索树列题 1.地鼠安家 输入示例 5 -1 1 -1 1 2 3 -1 3 -1 2 4 5 -1 5 -1 输出: 3 题目分析 考点:BST,BST中序遍历递增 先构建一棵树,利用中序遍历判断该序列是否递增即可 线状树列题 1 区域和检索 模板题: //线段树模板 class SegmentTree{ public: #define ls k tree; vector lazy; SegmentTree(int size) { tree=vector(size*4,0); lazy=vector(size*4,0); } SegmentTree() { } void build(int k,int l,int r,vectornums) { if(l==r) tree[k]=nums[l]; else { int m=l+(r-l)/2; build(ls,l,m,nums); build(rs,m+1,r,nums); pushUp(k); } } void pushUp(int k) { tree[k]=tree[ls]+tree[rs]; } void update(int l,int r,int k,int p,int v ) { if(l==r) tree[k]=v; else { int m = l+(r-l)/2; if(pm) update(m+1,r,rs,p,v); pushUp(k); } } int query(int l,int r,int L,int R,int k) { if(l>=L&&rm) right=query(m+1,r,L,R,rs); return left+right; } } }; class NumArray { public: SegmentTree tree; int num; NumArray(vector& nums) { int size=nums.size(); if(size==0)return; this->num=size-1; this->tree=SegmentTree(size); tree.build(1,0,num,nums); } void update(int i, int val) { tree.update(0,num,1,i,val); } int sumRange(int i, int j) { return tree.query(0,num,i,j,1); } }; 2.区间和的个数 题目解析 给出一个数组，要求在这个数组中找出任意一段子区间的和，位于 [lower,upper] 之间。 这一题可以用暴力解法，2 层循环，遍历所有子区间，求和并判断是否位于 [lower,upper] 之间，时间复杂度 O(n^2) )。 这一题当然还有更优的解法，用线段树或者树状数组，将时间复杂度降为 O(n log n) 。题目中要求 lower ≤ sum(i,j) ≤ upper，sum(i,j) = prefixSum(j) - prefixSum(i-1)，那么 lower + prefixSum(i-1) ≤ prefixSum(j) ≤ upper + prefixSum(i-1)。所以利用前缀和将区间和转换成了前缀和在线段树中 query 的问题，只不过线段树中父节点中存的不是子节点的和，而应该是子节点出现的次数。第二个转换，由于前缀和会很大，所以需要离散化。例如 prefixSum = [-3,-2,-1,0]，用前缀和下标进行离散化，所以线段树中左右区间变成了 0-3 。 利用 prefixSum 下标离散化： 还需要注意一些小细节，prefixSum 计算完以后需要去重，去重以后并排序，方便构造线段树的有效区间。如果不去重，线段树中可能出现非法区间(left > right)或者重叠区间。最后一步往线段树中倒序插入 prefixSum 的时候，用的是非去重的，插入 prefixSum[j] 代表 sum(i,j) 中的 j，例如往线段树中插入 prefixSum[5]，代表当前树中加入了 j = 5 的情况。query 操作实质是在做区间匹配，例如当前 i 循环到 i = 3，累计往线段树中插入了 prefixSum[5]，prefixSum[4]，prefixSum[3]，那么 query 操作实质是在判断：lower ≤ sum(i=3,j=3) ≤ upper，lower ≤ sum(i=3,j=4) ≤ upper，lower ≤ sum(i=3,j=5) ≤ upper，这 3 个等式是否成立，有几个成立就返回几个，即是最终要求得的结果的一部分。 举个例子，nums = [-3,1,2,-2,2,-1]，prefixSum = [-3,-2,0,-2,0,-1]，去重以后并排序得到 sum = [-3,-2,-1,0]。离散化构造线段树，这里出于演示的方便，下图中就不画出离散后的线段树了，用非离散的线段树展示： 倒序插入 len(prefixSum)-1 = prefixSum[5] = -1： 这时候查找区间变为了 [-3 + prefixSum[5-1], -1 + prefixSum[5-1]] = [-3,-1]，即判断 -3 ≤ sum(5,5) ≤ -1，满足等式的有几种情况，这里明显只有一种情况，即 j = 5，也满足等式，所以这一步 res = 1。 倒序插入 len(prefixSum)-2 = prefixSum[4] = 0： 这时候查找区间变为了 [-3 + prefixSum[4-1], -1 + prefixSum[4-1]] = [-5,-3]，即判断 -5 ≤ sum(4, 4,5) ≤ -3，满足等式的有几种情况，这里有两种情况，即 j = 4 或者 j = 5，都不满足等式，所以这一步 res = 0。 倒序插入 len(prefixSum)-3 = prefixSum[3] = -2： 这时候查找区间变为了 [-3 + prefixSum[3-1], -1 + prefixSum[3-1]] = [-3,-1]，即判断 -3 ≤ sum(3, 3,4,5) ≤ -1，满足等式的有几种情况，这里有三种情况，即 j = 3 、j = 4 或者 j = 5，满足等式的有 j = 3 和 j = 5，即 -3 ≤ sum(3, 3) ≤ -1 和 -3 ≤ sum(3, 5) ≤ -1。所以这一步 res = 2。 倒序插入 len(prefixSum)-4 = prefixSum[2] = 0： 这时候查找区间变为了 [-3 + prefixSum[2-1], -1 + prefixSum[2-1]] = [-5,-3]，即判断 -5 ≤ sum(2, 2,3,4,5) ≤ -3，满足等式的有几种情况，这里有四种情况，即 j = 2、 j = 3 、j = 4 或者 j = 5，都不满足等式。所以这一步 res = 0。 倒序插入 len(prefixSum)-5 = prefixSum[1] = -2： 这时候查找区间变为了 [-3 + prefixSum[1-1], -1 + prefixSum[1-1]] = [-6,-4]，即判断 -6 ≤ sum(1, 1,2,3,4,5) ≤ -4，满足等式的有几种情况，这里有五种情况，即 j = 1、 j = 2、 j = 3 、j = 4 或者 j = 5，都不满足等式。所以这一步 res = 0。 倒序插入 len(prefixSum)-6 = prefixSum[0] = -3： 这时候查找区间变为了 [-3 + prefixSum[0-1], -1 + prefixSum[0-1]] = [-3,-1]，注意 prefixSum[-1] = 0，即判断 -3 ≤ sum(0, 0,1,2,3,4,5) ≤ -1，满足等式的有几种情况，这里有六种情况，即 j = 0、j = 1、j = 2、 j = 3 、j = 4 或者 j = 5，满足等式的有 j = 0、j = 1、 j = 3 和 j = 5，即 -3 ≤ sum(0, 0) ≤ -1 、 -3 ≤ sum(0, 1) ≤ -1、-3 ≤ sum(0, 3) ≤ -1 和 -3 ≤ sum(0, 5) ≤ -1。所以这一步 res = 4。最后的答案就是把每一步的结果都累加，res = 1 + 0 + 2 + 0 + 0 + 4 = 7。 可能上面看完了还是云里雾里,没关系,通俗点讲就是: 先对线段树的区间离散化,v-lower,v-upper代表的就是区间中的每个点,这么做的目的是为了方便根据前缀和公式: lower ≤ sum(i,j) ≤ upper，sum(i,j) = prefixSum(j) - prefixSum(i-1), 对这个公式进行变化: prefixSum(j)-upper 也即,v-lower,v-upper也当作边,是为了根据上面这个公式,求出共有多少的点符合在某个区间内 最后,遍历每个prefixSum,即可查询其之前能有多少个点满足 prefixSum(j)-upper 也即,从线段树中查询区间[ prefixSum(j)-upper,prefixSum(i)-lower]的数量,就是满足的点的个数 最后,把该点插入线段树 class Solution { public: int arr[1000000]; void up(int rt){ arr[rt]=arr[rtm)update(x,v,m+1,r,rtm)ans+=query(x,y,m+1,r,rt& nums, int lower, int upper) { int n=nums.size(); //前缀和 long long sum[n+1]; sum[0]=0; for(int i=0;i mset; for(int i=0;i hash; int id=0; for(auto x:mset){ hash[x]=id; id++; } int ans=0; int m=id-1;//由于此时我们使用的是离散化后的数组，所以元素数目为所有可能值的数目 for(int i=0;i 3.天际线问题 分析 如果学过线段树的话,这题一看是区间问题,马上带上线段树模板: 维护区间最大值,需要离散化,高度变化,则为关键点: https://leetcode-cn.com/problems/the-skyline-problem/submissions/ class Solution { void pushUp(int p) { tree[p]=max(tree[p*2],tree[p*2+1]); } void pushDown(int p) { tree[2 * p] = max(tree[2 * p], lazy[p]); tree[2 * p + 1] = max(tree[2 * p + 1], lazy[p]); lazy[2 * p] = max(lazy[2 * p], lazy[p]) ; lazy[2 * p + 1] = max(lazy[2 * p + 1], lazy[p]); lazy[p] = 0; } void update(int l, int r, int curl, int curr, int p, int v) { if (curl >= l && curr 0 ) { pushDown(p); } if (l mid) update(l, r, mid + 1, curr, 2 * p + 1, v); pushUp(p); } int getsum(int l, int r, int curl, int curr, int p) { if (curl >= l && curr 0 ) { pushDown(p); } if (l mid) right = getsum(l, r, mid + 1, curr, 2 * p + 1); return max(left, right); } public: vector tree; vector lazy; vector> getSkyline(vector>& buildings) { set st; //1.离散化 unordered_map hash; unordered_map hashShift; for (auto building : buildings) { st.insert(building[0]); st.insert(building[1]); } int k = 1; for (auto it : st) { hash[it] = k; hashShift[k] = it; k++; } int n = k - 1; tree=vector(n*4,0); lazy=vector(n*4,0); //构建线段树 for (auto building: buildings) { update(hash[building[0]], hash[building[1]]-1, 1, n, 1, building[2]); //注意-1 } //对每条边,进行查询 vector> res; int before = 0; for (int i = 1; i {hashShift[i], height}); } before = height; } return res; } }; 4.翻转对 题目分析 本体类似区间合的个数 从这两题的对比可以看出,线段树维护的是区间,我们需要知道我们要查询的区间是从哪到哪 比如区间和个数那题,区间的范围是: [ prefixSum(j)-upper,prefixSum(i)-lower] 这题,区间的范围是: pre>cur*2 也即比当前大两倍的数,在代码中体现为: int lower=hash[nums[i]*2]+1; int upper=hash.size()-1; ans+=query(1,0,num,lower,upper); 代码: class Solution { public: #define ls k tree; vector lazy; int reversePairs(vector& nums) { int size=nums.size(); if(size==0)return 0; //离散化 map hash; set s; for (auto k:nums) { s.insert(k); s.insert((long long)k*2); } int id=0; for (auto k:s) { hash[k]=id++; } //查询,更新线段树 int num=id-1; tree=vector(num*4,0); int ans=0; for(int i=0;imid) update(rs,mid+1,r,p); pushUp(k); } } int query(int k,int l,int r,int L,int R) { if (l>=L&&rmid) ans+=query(rs,mid+1,r,L,R); return ans; } } }; 树状数组列题 字典树列题 1.模板题目 定义类 Trie class Trie { private: bool isEnd; Trie* next[26]; public: //方法将在下文实现... }; 插入 描述：向 Trie 中插入一个单词 word 实现：这个操作和构建链表很像。首先从根结点的子结点开始与 word 第一个字符进行匹配，一直匹配到前缀链上没有对应的字符，这时开始不断开辟新的结点，直到插入完 word 的最后一个字符，同时还要将最后一个结点isEnd = true;，表示它是一个单词的末尾。 void insert(string word) { Trie* node = this; for (char c : word) { if (node->next[c-'a'] == NULL) { node->next[c-'a'] = new Trie(); } node = node->next[c-'a']; } node->isEnd = true; } 查找 描述：查找 Trie 中是否存在单词 word 实现：从根结点的子结点开始，一直向下匹配即可，如果出现结点值为空就返回 false，如果匹配到了最后一个字符，那我们只需判断 node->isEnd即可。 bool search(string word) { Trie* node = this; for (char c : word) { node = node->next[c - 'a']; if (node == NULL) { return false; } } return node->isEnd; } 前缀匹配 描述：判断 Trie 中是或有以 prefix 为前缀的单词 实现：和 search 操作类似，只是不需要判断最后一个字符结点的isEnd，因为既然能匹配到最后一个字符，那后面一定有单词是以它为前缀的。 C++ bool startsWith(string prefix) { Trie* node = this; for (char c : prefix) { node = node->next[c-'a']; if (node == NULL) { return false; } } return true; } 到这我们就已经实现了 对 Trie 的一些基本操作，这样我们对 Trie 就有了进一步的理解。完整代码我贴在了文末。 总结 通过以上介绍和代码实现我们可以总结出 Trie 的几点性质： Trie 的形状和单词的插入或删除顺序无关，也就是说对于任意给定的一组单词，Trie 的形状都是唯一的。 查找或插入一个长度为 L 的单词，访问 next 数组的次数最多为 L+1，和 Trie 中包含多少个单词无关。 Trie 的每个结点中都保留着一个字母表，这是很耗费空间的。如果 Trie 的高度为 n，字母表的大小为 m，最坏的情况是 Trie 中还不存在前缀相同的单词，那空间复杂度就为 O(m^n) 最好情况下,时间复杂度为O(m) 最后，关于 Trie 的应用场景，希望你能记住 8 个字：一次建树，多次查询。(慢慢领悟叭~~) 全部代码 C++ class Trie { private: bool isEnd; Trie* next[26]; public: Trie() { isEnd = false; memset(next, 0, sizeof(next)); } void insert(string word) { Trie* node = this; for (char c : word) { if (node->next[c-'a'] == NULL) { node->next[c-'a'] = new Trie(); } node = node->next[c-'a']; } node->isEnd = true; } bool search(string word) { Trie* node = this; for (char c : word) { node = node->next[c - 'a']; if (node == NULL) { return false; } } return node->isEnd; } bool startsWith(string prefix) { Trie* node = this; for (char c : prefix) { node = node->next[c-'a']; if (node == NULL) { return false; } } return true; } }; 2.添加与搜索单词 这题和模板题的区别只是在search中,因为.可以替代每个单词,所以需要用到回朔搜索法 class WordDictionary { public: typedef WordDictionary Trie; bool isEnd; Trie * next[26]; /** Initialize your data structure here. */ WordDictionary() { isEnd= false; memset(next, 0, sizeof(next)); } /** Adds a word into the data structure. */ void addWord(string word) { Trie * node=this; for (auto k:word) { int index= k-'a'; if(node->next[index]==NULL) { node->next[index]=new Trie(); } node=node->next[index]; } node->isEnd= true; } /** Returns if the word is in the data structure. A word could contain the dot character '.' to represent any one letter. */ bool search(string word) { return dfs(word,this); } //回朔函数 bool dfs(string word,Trie * root) { for(int i=0;inext[i]!=NULL) { //成功就直接返回 if(dfs(st,root->next[i]))return true; } } return false; } else { //如果不是.,就接着迭代即可,无需回朔 int cur=word[i]-'a'; if(root->next[cur]==NULL)return false; root=root->next[cur]; } } return root->isEnd; } }; /** * Your WordDictionary object will be instantiated and called as such: * WordDictionary* obj = new WordDictionary(); * obj->addWord(word); * bool param_2 = obj->search(word); */ 3.单词搜索 II 根据字典树的数据结构,就可以发现其很适合和dfs等算法进行搭配 本题就是典型的dfs+字典树 首先根据words构建字典树,剩下的就是dfs的模板代码了 class Solution { class Trie{ public: bool isEnd; Trie * next[26]; string word; Trie(){ isEnd= false; memset(next,0,sizeof(next)); word=\"\"; } }; typedef Trie * node; public: vectorresult; int row; int col; vector> graph; vector> vis; int direction [4][2]={1,0,0,1,-1,0,0,-1}; vector findWords(vector>& board, vector& words) { //初始化 row=board.size(); if(row==0) return result; col=board[0].size(); this->graph=board; vis=vector>(row,vector(col,0)); //利用word构建字典树 node root=new Trie(); for (auto word:words) { insert(root,word); } //dfs查询 for (int i=0;inext[index]==NULL)return; root=root->next[index]; //边界 if(root->isEnd) { result.push_back(root->word); //注意要设置为false,否则会有重复现象 root->isEnd=false; //注意这里不能直接return,因为可能还会有以此为前缀的别的单词存在,需要继续搜索 } //回朔 for(auto k:direction) { int x=i+k[0]; int y=j+k[1]; if(isOk(x,y)) { char cur=graph[x][y]; vis[x][y]= true; dfs(x,y,root); vis[x][y]= false; } } } bool isOk(int i,int j) { return i>=0&&j>=0&&inext[index]==NULL) { root->next[index]=new Trie(); } root=root->next[index]; } root->isEnd= true; root->word=word; } }; 4.数组中两个数的最大异或值 1.Hash按位与法 异或运算的性质 解决这个问题，我们首先需要利用异或运算的一个性质： 如果 a ^ b = c 成立，那么a ^ c = b 与 b ^ c = a 均成立。 即 如果有三个数，满足其中两个数的异或值等于另一个值，那么这三个数的顺序可以任意调换。 （说明：利用这条性质，可以不使用第 3 个变量而交换两个变量的值。） 如何应用到本题？ 这道题找最大值的思路是这样的：因为两两异或可以得到一个值，在所有的两两异或得到的值中，一定有一个最大值，我们推测这个最大值应该是什么样的？即根据“最大值”的存在性解题（一定存在）。在这里要强调一下： 我们只用关心这个最大的异或值需要满足什么性质，进而推出这个最大值是什么，而不必关心这个异或值是由哪两个数得来的。 （上面这句话很重要，如果读者一开始看不明白下面的思考，不妨多看几遍我上面写的这句话。） 于是有如下思考： 1、二进制下，我们希望一个数尽可能大，即希望越高位上越能够出现“1”，这样这个数就是所求的最大数，这是贪心算法的思想。 2、于是，我们可以从最高位开始，到最低位，首先假设高位是 “1”，把这 n 个数全部遍历一遍，看看这一位是不是真的可以是“1”，否则这一位就得是“0”，判断的依据是上面“异或运算的性质”，即下面的第 3 点； 3、如果 a ^ b = max 成立 ，max 表示当前得到的“最大值”，那么一定有 max ^ b = a 成立。我们可以先假设当前数位上的值为 “1”，再把当前得到的数与这个 n 个数的 前缀（因为是从高位到低位看，所以称为“前缀”）进行异或运算，放在一个哈希表中，再依次把所有 前缀 与这个假设的“最大值”进行异或以后得到的结果放到哈希表里查询一下，如果查得到，就说明这个数位上可以是“1”，否则就只能是 0（看起来很晕，可以看代码理解）。 一种极端的情况是，这 n 个数在某一个数位上全部是 0 ，那么任意两个数异或以后都只能是 0，那么假设当前数位是 1 这件事情就不成立。 举个列子: import java.util.HashSet; import java.util.Set; public class Solution { // 先确定高位，再确定低位（有点贪心算法的意思），才能保证这道题的最大性质 // 一位接着一位去确定这个数位的大小 // 利用性质： a ^ b = c ，则 a ^ c = b，且 b ^ c = a public int findMaximumXOR(int[] nums) { int res = 0; int mask = 0; for (int i = 30; i >= 0; i--) { // 注意点1：注意保留前缀的方法，mask 是这样得来的 // 用异或也是可以的 mask = mask ^ (1 set = new HashSet<>(); for (int num : nums) { // 注意点2：这里使用 & ，保留前缀的意思（从高位到低位） set.add(num & mask); } // 这里先假定第 n 位为 1 ，前 n-1 位 res 为之前迭代求得 int temp = res | (1 2.字典树 字典树的思路就很明确,同样把数字从高位开始插入树中构建字典树, 遍历每个数,利用贪心的算法: 如果当前位为0,查找1 如果当前位为1,查找0 : class Trie{ public: Trie* next[2]; Trie() { memset(next, 0, sizeof(next)); } }; class Solution { Trie* root = new Trie(); public: int findMaximumXOR(vector& nums) { // 将数按照二进制形式全部存入字典树里面 for(int num : nums) { Trie* node = root; for(int i = 30; i >= 0; i--) { int bt = num >> i & 1; if(node->next[bt] == nullptr) { node->next[bt] = new Trie(); } node = node->next[bt]; } } // 找最大^值 int res = 0; for(int num : nums) { Trie* node = root; int sum = 0; for(int i = 30; i >= 0; i--) { int bt = num >> i & 1; // 如果bt==1则贪心的去找0异或 否则找1异或 if(bt == 1) { sum += node->next[0] != nullptr ? 1 next[0] != nullptr ? node->next[0] : node->next[1]; } else { sum += node->next[1] != nullptr ? 1 next[1] != nullptr ? node->next[1] : node->next[0]; } } res = max(res, sum); } return res; } }; 5.回文对 "},"数据结构/9.模板.html":{"url":"数据结构/9.模板.html","title":"数据结构模板","keywords":"","body":"0 常用STL API1.vector2.map & set3.queue & stack4.priority_queue5.stl排序一 树论模板1.搜索树2.字典树3.AVL平衡二叉树4.线段树二 图论模板1.最短路2.最小生成树3.并查集4.拓扑排序三 排序模板1.快排排序2.归并排序3.选择排序4.堆排序5.希尔排序6.基数排序7.桶排序四 散列表1.平方探测法0 常用STL API 1.vector push_back() size() clear() at() front() back() pop_back() 2.map & set map: empty() find()!=ma.end() set: insert() 3.queue & stack queue: push() front() pop() stack(): push() top() pop() 4.priority_queue class node{ public: int i; int j; bool operator (const node n) const{ return i>n.i; } node(int i,int j):i(i),j(j){} }; int main(){ priority_queue,greater> qu; qu.push(node(5,3)); qu.push(node(4,6)); while( !qu.empty() ){ node t=qu.top(); qu.pop(); cout5.stl排序 lamda: vector qu; qu.push_back(node(5,3)); qu.push_back(node(4,6)); sort(qu.begin(),qu.end(),[](node &x,node &y){ return x.i一 树论模板 1.搜索树 typedef int ElementType; typedef struct TreeNode *BinTree; struct TreeNode{ ElementType Data; BinTree Left; BinTree Right; }; // 查找递归实现 BinTree Find(ElementType X,BinTree BST){ if(!BST) // 如果根结点为空，返回 NULL return NULL; if(X Data) // 比根结点小，去左子树查找 return Find(X,BST->Left); else if(BST->Data Right); else if(BST->Data == X) // 找到了 return BST; } // 查找非递归实现 BinTree IterFind(ElementType X,BinTree BST){ while(BST){ if(X Data) BST = BST->Left; else if(BST->Data Right; else if(BST->Data == X) // 找到了 return BST; } return NULL; } // 查找最小值的递归实现 BinTree FindMin(BinTree BST){ if(!BST) // 如果为空了，返回 NULL return NULL; else if(BST->Left) // 还存在左子树，沿左分支继续查找 return FindMin(BST->Left); else // 找到了 return BST; } // 查找最大值的非递归实现 BinTree FindMax(BinTree BST){ if(BST) // 如果不空 while(BST->Right) // 只要右子树还存在 BST = BST->Right; return BST; } // 插入 BinTree Insert(ElementType X,BinTree BST){ if(!BST){ // 如果为空，初始化该结点 BST = (BinTree)malloc(sizeof(struct TreeNode)); BST->Data = X; BST->Left = NULL; BST->Right = NULL; }else{ // 不为空 if(X Data) // 如果小，挂在左边 BST->Left = Insert(X,BST->Left); else if(BST->Data Right = Insert(X,BST->Right); // 如果相等，什么都不用做 } return BST; } // 删除 BinTree Delete(ElementType X,BinTree BST){ BinTree tmp; if(!BST) coutData) // X 比当前结点值小，在左子树继续查找删除 BST->Left = Delete(X,BST->Left); else if(BST->Data Right = Delete(X,BST->Right); else{ // 找到被删除结点 if(BST->Left && BST->Right){ // 被删除结点有俩孩子结点 tmp = FindMin(BST->Right); // 找到右子树中值最小的 BST->Data = tmp->Data; // 用找到的值覆盖当前结点 BST->Right = Delete(tmp->Data,BST->Right); // 把前面找到的右子树最小值结点删除 }else{ // 被删除结点只有一个孩子结点或没有孩子结点 tmp = BST; if(!BST->Left && !BST->Right) // 没有孩子结点 BST = NULL; else if(BST->Left && !BST->Right) // 只有左孩子结点 BST = BST->Left; else if(!BST->Left && BST->Right) // 只有右孩子结点 BST = BST->Right; free(tmp); } } return BST; } 2.字典树 #include #include #include using namespace std; const int MAX_NODE = 1000000 + 10; const int CHARSET = 26; int trie[MAX_NODE][CHARSET] = {0}; int color[MAX_NODE] = {0}; //代表当前节点个数 int k = 1; void insert(char *w){ int len = strlen(w); //p代表当前在哪个节点 int p = 0; for(int i=0; i 3.AVL平衡二叉树 #include #include typedef struct AVLNode *AVLTree; struct AVLNode{ int data; // 存值 AVLTree left; // 左子树 AVLTree right; // 右子树 int height; // 树高 }; using namespace std; // 返回最大值 int Max(int a,int b){ return a>b?a:b; } // 返回树高，空树返回 -1 int getHeight(AVLTree A){ return A==NULL?-1:A->height; } // LL单旋 // 把 B 的右子树腾出来挂给 A 的左子树，再将 A 挂到 B 的右子树上去 AVLTree LLRotation(AVLTree A){ // 此时根节点是 A AVLTree B = A->left; // B 为 A 的左子树 A->left = B->right; // B 的右子树挂在 A 的左子树上 B->right = A; // A 挂在 B 的右子树上 A->height = Max(getHeight(A->left),getHeight(A->right)) + 1; B->height = Max(getHeight(B->left),A->height) + 1; return B; // 此时 B 为根结点了 } // RR单旋 AVLTree RRRotation(AVLTree A){ // 此时根节点是 A AVLTree B = A->right; A->right = B->left; B->left = A; A->height = Max(getHeight(A->left),getHeight(A->right)) + 1; B->height = Max(getHeight(B->left),A->height) + 1; return B; // 此时 B 为根结点了 } // LR双旋 AVLTree LRRotation(AVLTree A){ // 先 RR 单旋 A->left = RRRotation(A->left); // 再 LL 单旋 return LLRotation(A); } // RL双旋 AVLTree RLRotation(AVLTree A){ // 先 LL 单旋 A->right = LLRotation(A->right); // 再 RR 单旋 return RRRotation(A); } AVLTree Insert(AVLTree T,int x){ if(!T){ // 如果该结点为空，初始化结点 T = (AVLTree)malloc(sizeof(struct AVLNode)); T->data = x; T->left = NULL; T->right = NULL; T->height = 0; }else{ // 否则不为空， if(x data){ // 左子树 T->left = Insert(T->left,x); if(getHeight(T->left)-getHeight(T->right)==2){ // 如果左子树和右子树高度差为 2 if(x left->data) // LL 单旋 T = LLRotation(T); else if(T->left->data data right = Insert(T->right,x); if(getHeight(T->right)-getHeight(T->left)==2){ if(x right->data) // RL 双旋 T = RLRotation(T); else if(T->right->data height = Max(getHeight(T->left),getHeight(T->right)) + 1; return T; } int main(){ AVLTree T=NULL; int n; cin>>n; for(int i=0;i>tmp; T = Insert(T,tmp); } coutdata; return 0; } 4.线段树 class NumArray { public: #define ls k t; vector lazy; int n; vector nums; NumArray(vector& nums) {; n=nums.size(); if(n==0)return; t=vector(n*4,0); lazy=vector(n*4,0); this->nums=nums; build(1,0,n-1); } //push up ,push up函数是关键,这里是区间和,如果是区间最大值,修改pushUp即可 void pushUp(int k){ t[k]=t[ls]+t[rs]; } //递归方式建树 void build(int k,int l,int r){ //k为当前需要建立的结点，l为当前需要建立区间的左端点，r则为右端点 if(l == r) //左端点等于右端点，即为叶子节点，直接赋值即可 t[k] = nums[l]; else{ int m = l + ((r-l)>>1); //m则为中间点，左儿子的结点区间为[l,m],右儿子的结点区间为[m+1,r] build(ls,l,m); //递归构造左儿子结点 build(rs,m+1,r); //递归构造右儿子结点 pushUp(k); //更新父节点 } } //点更新 //递归方式更新 update_point(p,v,1,n,1); void update_point(int p, int v, int l, int r, int k){ //p为下标，v为要加上的值，l，r为结点区间，k为结点下标 if(l == r) //左端点等于右端点，即为叶子结点，直接加上v即可 t[k] += v; //线段树数组都得到更新 else{ //先删除标记 pushdown(k); int m = l + ((r-l)>>1); //m则为中间点，左儿子的结点区间为[l,m],右儿子的结点区间为[m+1,r] if(p =i&&r=i) { update_range(i,j,v,l,mid,ls); } else if(mid>1); //m则为中间点，左儿子的结点区间为[l,m],右儿子的结点区间为[m+1,r] if(L m) //如果右子树和需要查询的区间交集非空，注意这里不是else if，因为查询区间可能同时和左右区间都有交集 res += range_sum(L,R,m+1,r,rs); return res; //返回当前结点得到的信息 } } }; 二 图论模板 1.最短路 1.Dijkstra int dijkstra(vector>& gh, int N, int K) { const int INF = 0x3f3f3f3f; typedef pair PII; // first:距离; second: 几号点 vector vis(N + 1, false); // 是否已得到最短距离 vector dist(N+1, INF); // 距离起始点的最短距离 unordered_map> graph; // 邻接表；u->v,权重w priority_queue, greater> heap; // 小顶堆；维护到起始点的最短距离和点 for (auto &t: gh){ // 初始化邻接表 graph[t[0]].push_back({t[2],t[1]}); } heap.push({0, K}); dist[K] = 0; while(heap.size()){ auto t = heap.top(); heap.pop(); int ver = t.second, distance = t.first; if (vis[ver]) continue; // 之前更新过，是冗余备份 vis[ver] = true; for (auto &p: graph[ver]){ if (dist[p.second] > distance + p.first){ // 用t去更新其他点到起始点的最短距离 dist[p.second] = distance + p.first; heap.push({dist[p.second], p.second}); } } } int ans = *max_element(dist.begin()+1, dist.end()); return ans == INF ? -1: ans; } 2.floyd int floyd(vector>& gh, int N, int K) { const int INF = 0x3f3f3f3f; vector> dist(N+1,vector(N+1,INF)); for (int i = 0; i INF/2?-1:ans; } 2.最小生成树 1.Prim typedef pair ll; int prim( vector> graph, vector dist,int n){ priority_queue,greater> q; vector vis(n+1, false); dist[1]=0; int sum=0; q.push({dist[1],1}); while( !q.empty() ){ int index=q.top().second; q.pop(); if(vis[index])continue; #加上这条边 sum+=dist[index]; vis[index]= true; #对其邻接边,收录 for (int i = 1; i >n>>m; int inf=10000000; vector dist(n+1,inf); vector> graph(n+1,vector(n+1,inf)); int a,b,value; for(int i=0;i>a>>b>>value; graph[a][b]=value; graph[b][a]=value; } cout 2.Kruskal #include using namespace std; struct Edge{ int a,b,w; Edge(int a,int b ,int w):a(a),b(b),w(w){} bool operator g; vector fa; int find(int x){ return x==fa[x]?x:fa[x]=find(fa[x]); } int kruskal(int n){ //对边进行排序 sort(g.begin(),g.end()); int ans=0; int cnt=n; for(Edge e:g){ int fax= find(e.a); int fay= find(e.b); if(fax!=fay){ //计数 cnt--; //合并 fa[fax]=fay; //加上这条边的值 ans+=e.w; } } return cnt==1?ans:-1; } int main(){ int n,m; cin>>n>>m; int a,b,value; fa=vector(n+1,0); for (int i = 1; i >a>>b>>value; g.push_back(Edge(a,b,value)); } cout 3.并查集 class UnionFind{ public: int UnionNums; int *graph; /** * find */ int find(int k){ if(graph[k]==k)return k; else return k=find(graph[k]); } /** * union */ void Union(int i,int j){ int pI=find(i); int pK=find(j); if(pI!=pK){ graph[pK]=pI; UnionNums--; } } //初始化 UnionFind(int nums){ UnionNums=nums; graph=new int[nums+1]; for(int i=0;i 4.拓扑排序 class Solution { public: bool canFinish(int numCourses, vector>& prerequisites) { int len=numCourses; int in[len]; int out[len]; vector> ve; queue q; vector v; for(int i=0;i 三 排序模板 1.快排排序 void quick_sort( vector & arr, int start ,int end){ if ( start=pivot)right--; arr[left]=arr[right]; while(left 2.归并排序 //归并排序 void merge( vector& arr,int start,int mid,int end){ vector tempArr(end-start+1,0); int left = start; int right = mid+1; int index=0; while( left=arr[right]){ tempArr[index++]=arr[right++]; } } while(left& arr,int start,int end){ if(start 3.选择排序 void select_sort( vector& arr,int start,int end ){ int swap=0; for (int i = 0; i 4.堆排序 #include using namespace std; #define maxdata 100000 struct node{ int * data; int size; int capacity; }; typedef node * tree; tree create(int maxsize){ tree h=new node; h->data=(int *)malloc(sizeof(int)*(maxsize+1)); h->size=0; h->capacity=maxsize; h->data[0]=maxdata; return h; } void LevelOrderTraversal(tree H){ int i; printf(\"层序遍历的结果是：\"); for(i = 1;isize;i++){ printf(\"%d \",H->data[i]); } printf(\"\\n\"); } void Heapify(tree h,int i){ int parent,child; int data=h->data[i]; for(parent=i;parent*2size;parent=child){ child=parent*2; if(child!=h->size){ if(h->data[child]data[child+1])child++; } if(h->data[child]data[parent]=h->data[child]; } h->data[parent]=data; } void adjust(tree h){ int i=h->size/2; for(;i>0;i--){ Heapify(h,i);//从最拥有子树的最小树开始--完全二叉树的特性,可以自己画个图验证一下 } } int main(){ tree h; h=create(100); int n; cin>>n; for(int i=0;i>h->data[++h->size]; } adjust(h); LevelOrderTraversal(h); } 5.希尔排序 //4.希尔排序 void shell_sort( vector& arr , int n){ for( int gap=n/2; gap>0 ;gap/=2){ for( int i=gap ;i=0 && arr[j-gap] > sentinel ){ arr[j]= arr[j-gap]; j-=gap; } arr[j]= sentinel; } } } } 6.基数排序 /* * 获取数组a中最大值 * * 参数说明： * a -- 数组 * n -- 数组长度 */ int get_max(int a[], int n) { int i, max; max = a[0]; for (i = 1; i max) max = a[i]; return max; } /* * 对数组按照\"某个位数\"进行排序(桶排序) * * 参数说明： * a -- 数组 * n -- 数组长度 * exp -- 指数。对数组a按照该指数进行排序。 * * 例如，对于数组a={50, 3, 542, 745, 2014, 154, 63, 616}； * (01) 当exp=1表示按照\"个位\"对数组a进行排序 * (02) 当exp=10表示按照\"十位\"对数组a进行排序 * (03) 当exp=100表示按照\"百位\"对数组a进行排序 * ... */ void count_sort(int a[], int n, int exp) { int output[n]; // 存储\"被排序数据\"的临时数组 int i, buckets[10] = {0}; // 将数据出现的次数存储在buckets[]中 for (i = 0; i = 0; i--) { output[buckets[ (a[i]/exp)%10 ] - 1] = a[i]; buckets[ (a[i]/exp)%10 ]--; } // 将排序好的数据赋值给a[] for (i = 0; i 0; exp *= 10) count_sort(a, n, exp); } 7.桶排序 public static void bucketSort(int[] arr){ // 计算最大值与最小值 int max = Integer.MIN_VALUE; int min = Integer.MAX_VALUE; for(int i = 0; i > bucketArr = new ArrayList<>(bucketNum); for(int i = 0; i ()); } // 将每个元素放入桶 for(int i = 0; i 四 散列表 1.平方探测法 #define MAXTABLESIZE 100000 // 定义允许开辟的最大散列表长度 typedef int Index; typedef int ElementType; typedef Index Position; typedef enum{ // 分别对应：有合法元素、空、有已删除元素 Legitimate,Empty,Deleted } EntryType; // 定义单元状态类型 typedef struct HashEntry Cell; struct HashEntry{ // 哈希表存值单元 ElementType Data; // 存放元素 EntryType Info; // 单元状态 }; typedef struct HashTbl *HashTable; struct HashTbl{ // 哈希表结构体 int TableSize; // 哈希表大小 Cell *Cells; // 哈希表存值单元数组 }; using namespace std; int NextPrime(int N); // 查找素数 HashTable CreateTable( int TableSize); // 创建哈希表 Index Hash(int Key,int TableSize); // 哈希函数 // 查找素数 int NextPrime(int N){ int p = (N%2)?N+2:N+1; // 从大于 N 的下个奇数开始 int i; while(p 2;i--) if(!(p%i)) // p 不是素数 break; if(i==2) break; p += 2; // 继续试探下个奇数 } return p; } // 创建哈希表 HashTable CreateTable( int TableSize){ HashTable H; int i; H = (HashTable)malloc(sizeof(struct HashTbl)); // 保证哈希表最大长度是素数 H->TableSize = NextPrime(TableSize); // 初始化单元数组 H->Cells = (Cell *)malloc(sizeof(Cell)*H->TableSize); // 初始化单元数组状态 for(int i=0;iTableSize;i++) H->Cells[i].Info = Empty; return H; } // 平方探测查找 Position Find(HashTable H,ElementType Key){ Position CurrentPos,NewPos; int CNum = 0 ; // 记录冲突次数 CurrentPos = NewPos = Hash(Key,H->TableSize); // 如果当前单元状态不为空，且数值不等，则一直做 while(H->Cells[NewPos].Info != Empty && H->Cells[NewPos].Data != Key){ if(++CNum % 2 ){ // 冲突奇数次发生 NewPos = CurrentPos + (CNum+1)/2*(CNum+1)/2; // 如果越界，一直减直到再次进入边界 while(H->TableSize TableSize; } }else{ // 冲突偶数次发生 NewPos = CurrentPos - CNum/2*CNum/2; // 如果越界，一直加直到再次进入边界 while(NewPos TableSize; } } } return NewPos; } // 插入 bool Insert( HashTable H,ElementType Key,int i){ Position Pos = i; Pos = Find(H,Key); // 如果单元格状态不是\"存在合法元素\" if( H->Cells[Pos].Info != Legitimate){ H->Cells[Pos].Info = Legitimate; H->Cells[Pos].Data = Key; } return true; } // 除留余数法哈希函数 Index Hash(int Key,int TableSize){ return Key % TableSize; } "},"算法/1.时间复杂度.html":{"url":"算法/1.时间复杂度.html","title":"1.时间复杂度","keywords":"","body":"1.算法的效率2.时间复杂度3.大O表示法4.复杂度的比较1.算法的效率 虽然计算机能快速的完成运算处理，但实际上，它也需要根据输入数据的大小和算法效率来消耗一定的处理器资源。要想编写出能高效运行的程序，我们就需要考虑到算法的效率。 算法的效率主要由以下两个复杂度来评估： 时间复杂度：评估执行程序所需的时间。可以估算出程序对处理器的使用程度。 空间复杂度：评估执行程序所需的存储空间。可以估算出程序对计算机内存的使用程度。 设计算法时，一般是要先考虑系统环境，然后权衡时间复杂度和空间复杂度，选取一个平衡点。不过，时间复杂度要比空间复杂度更容易产生问题，因此算法研究的主要也是时间复杂度，不特别说明的情况下，复杂度就是指时间复杂度。 2.时间复杂度 时间频度 一个算法执行所耗费的时间，从理论上是不能算出来的，必须上机运行测试才能知道。但我们不可能也没有必要对每个算法都上机测试，只需知道哪个算法花费的时间多，哪个算法花费的时间少就可以了。并且一个算法花费的时间与算法中语句的执行次数成正比例，哪个算法中语句执行次数多，它花费时间就多。一个算法中的语句执行次数称为语句频度或时间频度。记为T(n)。 时间复杂度 前面提到的时间频度T(n)中，n称为问题的规模，当n不断变化时，时间频度T(n)也会不断变化。但有时我们想知道它变化时呈现什么规律，为此我们引入时间复杂度的概念。一般情况下，算法中基本操作重复执行的次数是问题规模n的某个函数，用T(n)表示，若有某个辅助函数f(n)，使得当n趋近于无穷大时，T（n)/f(n)的极限值为不等于零的常数，则称f(n)是T(n)的同数量级函数，记作T(n)=O(f(n))，它称为算法的渐进时间复杂度，简称时间复杂度。 3.大O表示法 像前面用O( )来体现算法时间复杂度的记法，我们称之为大O表示法。 算法复杂度可以从最理想情况、平均情况和最坏情况三个角度来评估，由于平均情况大多和最坏情况持平，而且评估最坏情况也可以避免后顾之忧，因此一般情况下，我们设计算法时都要直接估算最坏情况的复杂度。 大O表示法O(f(n)中的f(n)的值可以为1、n、logn、n²等，因此我们可以将O(1)、O(n)、O(logn)、O(n²)分别可以称为常数阶、线性阶、对数阶和平方阶，那么如何推导出f(n)的值呢？我们接着来看推导大O阶的方法。 推导大O阶 推导大O阶，我们可以按照如下的规则来进行推导，得到的结果就是大O表示法： 1.用常数1来取代运行时间中所有加法常数。 2.修改后的运行次数函数中，只保留最高阶项 3.如果最高阶项存在且不是1，则去除与这个项相乘的常数。 常数阶 先举了例子，如下所示。 int sum = 0,n = 100; //执行一次 sum = (1+n)*n/2; //执行一次 System.out.println (sum); //执行一次 123 上面算法的运行的次数的函数为f(n)=3，根据推导大O阶的规则1，我们需要将常数3改为1，则这个算法的时间复杂度为O(1)。如果sum = （1+n）*n/2这条语句再执行10遍，因为这与问题大小n的值并没有关系，所以这个算法的时间复杂度仍旧是O(1)，我们可以称之为常数阶。 线性阶 线性阶主要要分析循环结构的运行情况，如下所示。 for(int i=0;i上面算法循环体中的代码执行了n次，因此时间复杂度为O(n)。 对数阶 接着看如下代码： int number=1; while(number可以看出上面的代码，随着number每次乘以2后，都会越来越接近n，当number不小于n时就会退出循环。假设循环的次数为X，则由2^x=n得出x=log₂n，因此得出这个算法的时间复杂度为O(logn)。 平方阶 下面的代码是循环嵌套： for(int i=0;i内层循环的时间复杂度在讲到线性阶时就已经得知是O(n)，现在经过外层循环n次，那么这段算法的时间复杂度则为O(n²)。 接下来我们来算一下下面算法的时间复杂度： for(int i=0;i需要注意的是内循环中int j=i，而不是int j=0。当i=0时，内循环执行了n次；i=1时内循环执行了n-1次，当i=n-1时执行了1次，我们可以推算出总的执行次数为： n+(n-1)+(n-2)+(n-3)+……+1 =(n+1)+[(n-1)+2]+[(n-2)+3]+[(n-3)+4]+…… =(n+1)+(n+1)+(n+1)+(n+1)+…… =(n+1)n/2 =n(n+1)/2 =n²/2+n/2 根据此前讲过的推导大O阶的规则的第二条：只保留最高阶，因此保留n²/2。根据第三条去掉和这个项的常数，则去掉1/2,最终这段代码的时间复杂度为O(n²)。 其他常见复杂度 除了常数阶、线性阶、平方阶、对数阶，还有如下时间复杂度： f(n)=nlogn时，时间复杂度为O(nlogn)，可以称为nlogn阶。 f(n)=n³时，时间复杂度为O(n³)，可以称为立方阶。 f(n)=2ⁿ时，时间复杂度为O(2ⁿ)，可以称为指数阶。 f(n)=n!时，时间复杂度为O(n!)，可以称为阶乘阶。 f(n)=(√n时，时间复杂度为O(√n)，可以称为平方根阶。 4.复杂度的比较 下面将算法中常见的f(n)值根据几种典型的数量级来列成一张表，根据这种表，我们来看看各种算法复杂度的差异。 n logn √n nlogn n² 2ⁿ n! 5 2 2 10 25 32 120 10 3 3 30 100 1024 3628800 50 5 7 250 2500 约10^15 约3.0*10^64 100 6 10 600 10000 约10^30 约9.3*10^157 1000 9 31 9000 1000 000 约10^300 约4.0*10^2567 从上表可以看出，O(n)、O(logn)、O(√n )、O(nlogn )随着n的增加，复杂度提升不大，因此这些复杂度属于效率高的算法，反观O(2ⁿ)和O(n!)当n增加到50时，复杂度就突破十位数了，这种效率极差的复杂度最好不要出现在程序中，因此在动手编程时要评估所写算法的最坏情况的复杂度。 下面给出一个更加直观的图： 其中x轴代表n值，y轴代表T(n)值（时间复杂度）。T(n)值随着n的值的变化而变化，其中可以看出O(n!)和O(2ⁿ)随着n值的增大，它们的T(n)值上升幅度非常大，而O(logn)、O(n)、O(nlogn)随着n值的增大，T(n)值上升幅度则很小。 常用的时间复杂度按照耗费的时间从小到大依次是： O(1)b "},"算法/2.回溯算法.html":{"url":"算法/2.回溯算法.html","title":"2.回朔算法","keywords":"","body":"一 算法框架二 问题举例1.全排列问题2.N皇后问题三 分类题型1.排列,组合类型2.解数独:返回一次类型一 算法框架 解决一个回溯问题，实际上就是一个决策树的遍历过程。你只需要思考 3 个问题： 1、路径：也就是已经做出的选择。 2、选择列表：也就是你当前可以做的选择。 3、结束条件：也就是到达决策树底层，无法再做选择的条件。 如果你不理解这三个词语的解释，没关系，我们后面会用「全排列」和「N 皇后问题」这两个经典的回溯算法问题来帮你理解这些词语是什么意思，现在你先留着印象。 代码方面，回溯算法的框架： result = [] def backtrack(路径, 选择列表): if 满足结束条件: result.add(路径) return for 选择 in 选择列表: 做选择 backtrack(路径, 选择列表) 撤销选择 其核心就是 for 循环里面的递归，在递归调用之前「做选择」，在递归调用之后「撤销选择」，特别简单。 什么叫做选择和撤销选择呢，这个框架的底层原理是什么呢？下面我们就通过「全排列」这个问题来解开之前的疑惑，详细探究一下其中的奥妙！ 二 问题举例 1.全排列问题 我们在高中的时候就做过排列组合的数学题，我们也知道 n 个不重复的数，全排列共有 n! 个。 PS：为了简单清晰起见，我们这次讨论的全排列问题不包含重复的数字。 那么我们当时是怎么穷举全排列的呢？比方说给三个数 [1,2,3]，你肯定不会无规律地乱穷举，一般是这样： 先固定第一位为 1，然后第二位可以是 2，那么第三位只能是 3；然后可以把第二位变成 3，第三位就只能是 2 了；然后就只能变化第一位，变成 2，然后再穷举后两位…… 其实这就是回溯算法，我们高中无师自通就会用，或者有的同学直接画出如下这棵回溯树： 只要从根遍历这棵树，记录路径上的数字，其实就是所有的全排列。我们不妨把这棵树称为回溯算法的「决策树」。 为啥说这是决策树呢，因为你在每个节点上其实都在做决策 现在可以解答开头的几个名词：**[2] 就是「路径」，记录你已经做过的选择；**[1,3] 就是「选择列表」，表示你当前可以做出的选择；「结束条件」就是遍历到树的底层，在这里就是选择列表为空的时候。 如果明白了这几个名词，可以把「路径」和「选择」列表作为决策树上每个节点的属性，比如下图列出了几个节点的属性： 我们定义的 backtrack 函数其实就像一个指针，在这棵树上游走，同时要正确维护每个节点的属性，每当走到树的底层，其「路径」就是一个全排列。 回想我们刚才说的，「路径」和「选择」是每个节点的属性，函数在树上游走要正确维护节点的属性，那么就要在这两个特殊时间点搞点动作： 现在，你是否理解了回溯算法的这段核心框架？ for 选择 in 选择列表: # 做选择 将该选择从选择列表移除 路径.add(选择) backtrack(路径, 选择列表) # 撤销选择 路径.remove(选择) 将该选择再加入选择列表 我们只要在递归之前做出选择，在递归之后撤销刚才的选择，就能正确得到每个节点的选择列表和路径。 下面，直接看全排列代码： vector> result; vector vis; void dfs(vector nums,vector track,int cur) { //边界 if(cur==nums.size()) { result.push_back(track); return; } if(cur>=nums.size())return; //选择列表 for(int i=0;i> permute(vector& nums) { int size=nums.size(); vis=vector(size,false); dfs(nums,vector{},0); return result; } 2.N皇后问题 这个问题很经典了，简单解释一下：给你一个 N×N 的棋盘，让你放置 N 个皇后，使得它们不能互相攻击。 PS：皇后可以攻击同一行、同一列、左上左下右上右下四个方向的任意单位。 这个问题本质上跟全排列问题差不多，决策树的每一层表示棋盘上的每一行；每个节点可以做出的选择是，在该行的任意一列放置一个皇后。 直接套用框架: int size; vector> result; bool isValid(int row,int col,vector colIndex) { for(int i=0;i board,vector colIndex,int row) { //边界 if(row == size) { result.push_back(board); } if(row >= size) return; //选择列表 for (int i = 0; i > solveNQueens(int n) { size=n; vector board(n,string(n,'.')); dfs(board,vector(n,-1),0); return result; } 三 分类题型 1.排列,组合类型 1.全排列 II 我们先画出递归树: class Solution { public: vector> result; vector flag; vector num; void dfs( vector arr,int cur){ if(cur==num.size()){ result.push_back(arr); } if(cur>=num.size())return; for (int i = 0; i 0 && num[i]==num[i-1]&& ! flag[i-1])continue; if(!flag[i]){ flag[i]= true; arr.push_back(num[i]); dfs(arr,cur+1); arr.pop_back(); flag[i]= false; } } } vector> permuteUnique(vector& nums) { num=nums; sort(num.begin(),num.end()); int size=nums.size(); flag=vector(size, false); dfs({},0); return result; } }; 2.子集 该回朔算法的递归树: 据此,我们需要一个指针cur,选择列表只能从cur开始往后 class Solution { public: vector> result; void dfs(vectornums,vector temp,int cur){ result.push_back(temp); for(int i=cur;i> subsets(vector& nums) { if(nums.size()==0)return result; dfs(nums,vector{},0); return result; } }; 3.组合 这题和子集的区别在于需要判断边界而已 vector> result; void dfs(vector nums,int cur,int n,int k) { if(nums.size()==k) { result.push_back(nums); } if(nums.size()>=k) return; for(int i=cur;i> combine(int n, int k) { dfs(vector{},1,n,k); return result; } 2.解数独:返回一次类型 这题看似很难,但其实和前面的回朔没有太大的区别 我们先整理以下思路: 我们想要在盘中的(i,j)这个点填数字num,并且num要有要求: 同一行不能重复 也即row(i,num)==0 同一列不能重复 也即col(j,num)==0 同一个33的格子不能重复 也即 box(j/3+(i/3)3)==0 最后,我们只需要一个解即可,所以在做出决策的回朔时需要这样判断: if(dfs(xxxx)) return true; 最后代码: class Solution { public: int row[10][10]={0}; int col[10][10]={0}; int box[10][10]={0}; int m,n; bool isOk(int i,int j,int val){ return i>=0&&i=0&&j>& board,int i,int j){ //边界就是换行即可 if(j==n){ j=0; i++; if(i==n)return true; } if(board[i][j]=='.'){ for (int k = 1; k >& board) { m=n=9; //先预设值 for (int i = 0; i "},"jvm/1.jvm内存结构.html":{"url":"jvm/1.jvm内存结构.html","title":"1.jvm内存结构","keywords":"","body":"JVM 内存结构程序计数器（PC 寄存器）程序计数器的定义程序计数器的作用程序计数器的特点Java 虚拟机栈（Java 栈）Java 虚拟机栈的定义压栈出栈过程Java 虚拟机栈的特点本地方法栈（C 栈）本地方法栈的定义栈帧变化过程堆堆的定义堆的特点方法区方法区的定义方法区的特点运行时常量池直接内存（堆外内存）操作直接内存直接内存与堆内存比较JVM 内存结构 Java 虚拟机的内存空间分为 5 个部分： 程序计数器 Java 虚拟机栈 本地方法栈 堆 方法区 JDK 1.8 同 JDK 1.7 比，最大的差别就是：元数据区取代了永久代。元空间的本质和永久代类似，都是对 JVM 规范中方法区的实现。不过元空间与永久代之间最大的区别在于：元数据空间并不在虚拟机中，而是使用本地内存。 程序计数器（PC 寄存器） 程序计数器的定义 程序计数器是一块较小的内存空间，是当前线程正在执行的那条字节码指令的地址。若当前线程正在执行的是一个本地方法，那么此时程序计数器为Undefined。 程序计数器的作用 字节码解释器通过改变程序计数器来依次读取指令，从而实现代码的流程控制。 在多线程情况下，程序计数器记录的是当前线程执行的位置，从而当线程切换回来时，就知道上次线程执行到哪了。 程序计数器的特点 是一块较小的内存空间。 线程私有，每条线程都有自己的程序计数器。 生命周期：随着线程的创建而创建，随着线程的结束而销毁。 是唯一一个不会出现OutOfMemoryError的内存区域。 Java 虚拟机栈（Java 栈） Java 虚拟机栈的定义 Java 虚拟机栈是描述 Java 方法运行过程的内存模型。 Java 虚拟机栈会为每一个即将运行的 Java 方法创建一块叫做“栈帧”的区域，用于存放该方法运行过程中的一些信息，如： 局部变量表 操作数栈 动态链接 方法出口信息 ...... 压栈出栈过程 当方法运行过程中需要创建局部变量时，就将局部变量的值存入栈帧中的局部变量表中。 Java 虚拟机栈的栈顶的栈帧是当前正在执行的活动栈，也就是当前正在执行的方法，PC 寄存器也会指向这个地址。只有这个活动的栈帧的本地变量可以被操作数栈使用，当在这个栈帧中调用另一个方法，与之对应的栈帧又会被创建，新创建的栈帧压入栈顶，变为当前的活动栈帧。 方法结束后，当前栈帧被移出，栈帧的返回值变成新的活动栈帧中操作数栈的一个操作数。如果没有返回值，那么新的活动栈帧中操作数栈的操作数没有变化。 由于 Java 虚拟机栈是与线程对应的，数据不是线程共享的，因此不用关心数据一致性问题，也不会存在同步锁的问题。 Java 虚拟机栈的特点 局部变量表随着栈帧的创建而创建，它的大小在编译时确定，创建时只需分配事先规定的大小即可。在方法运行过程中，局部变量表的大小不会发生改变。 Java 虚拟机栈会出现两种异常：StackOverFlowError 和 OutOfMemoryError。 StackOverFlowError 若 Java 虚拟机栈的大小不允许动态扩展，那么当线程请求栈的深度超过当前 Java 虚拟机栈的最大深度时，抛出 StackOverFlowError 异常。 OutOfMemoryError 若允许动态扩展，那么当线程请求栈时内存用完了，无法再动态扩展时，抛出 OutOfMemoryError 异常。 Java 虚拟机栈也是线程私有，随着线程创建而创建，随着线程的结束而销毁。 出现 StackOverFlowError 时，内存空间可能还有很多。 本地方法栈（C 栈） 本地方法栈的定义 本地方法栈是为 JVM 运行 Native 方法准备的空间，由于很多 Native 方法都是用 C 语言实现的，所以它通常又叫 C 栈。它与 Java 虚拟机栈实现的功能类似，只不过本地方法栈是描述本地方法运行过程的内存模型。 栈帧变化过程 本地方法被执行时，在本地方法栈也会创建一块栈帧，用于存放该方法的局部变量表、操作数栈、动态链接、方法出口信息等。 方法执行结束后，相应的栈帧也会出栈，并释放内存空间。也会抛出 StackOverFlowError 和 OutOfMemoryError 异常。 如果 Java 虚拟机本身不支持 Native 方法，或是本身不依赖于传统栈，那么可以不提供本地方法栈。如果支持本地方法栈，那么这个栈一般会在线程创建的时候按线程分配。 堆 堆的定义 堆是用来存放对象的内存空间，几乎所有的对象都存储在堆中。 堆的特点 线程共享，整个 Java 虚拟机只有一个堆，所有的线程都访问同一个堆。而程序计数器、Java 虚拟机栈、本地方法栈都是一个线程对应一个。 在虚拟机启动时创建。 是垃圾回收的主要场所。 进一步可分为：新生代(Eden 区 From Survior To Survivor)、老年代。 不同的区域存放不同生命周期的对象，这样可以根据不同的区域使用不同的垃圾回收算法，更具有针对性。 堆的大小既可以固定也可以扩展，但对于主流的虚拟机，堆的大小是可扩展的，因此当线程请求分配内存，但堆已满，且内存已无法再扩展时，就抛出 OutOfMemoryError 异常。 Java 堆所使用的内存不需要保证是连续的。而由于堆是被所有线程共享的，所以对它的访问需要注意同步问题，方法和对应的属性都需要保证一致性。 方法区 方法区的定义 Java 虚拟机规范中定义方法区是堆的一个逻辑部分。方法区存放以下信息： 已经被虚拟机加载的类信息 常量 静态变量 即时编译器编译后的代码 方法区的特点 线程共享。 方法区是堆的一个逻辑部分，因此和堆一样，都是线程共享的。整个虚拟机中只有一个方法区。 永久代。 方法区中的信息一般需要长期存在，而且它又是堆的逻辑分区，因此用堆的划分方法，把方法区称为“永久代”。 内存回收效率低。 方法区中的信息一般需要长期存在，回收一遍之后可能只有少量信息无效。主要回收目标是：对常量池的回收；对类型的卸载。 Java 虚拟机规范对方法区的要求比较宽松。 和堆一样，允许固定大小，也允许动态扩展，还允许不实现垃圾回收。 运行时常量池 方法区中存放：类信息、常量、静态变量、即时编译器编译后的代码。常量就存放在运行时常量池中。 当类被 Java 虚拟机加载后， .class 文件中的常量就存放在方法区的运行时常量池中。而且在运行期间，可以向常量池中添加新的常量。如 String 类的 intern() 方法就能在运行期间向常量池中添加字符串常量。 直接内存（堆外内存） 直接内存是除 Java 虚拟机之外的内存，但也可能被 Java 使用。 操作直接内存 在 NIO 中引入了一种基于通道和缓冲的 IO 方式。它可以通过调用本地方法直接分配 Java 虚拟机之外的内存，然后通过一个存储在堆中的DirectByteBuffer对象直接操作该内存，而无须先将外部内存中的数据复制到堆中再进行操作，从而提高了数据操作的效率。 直接内存的大小不受 Java 虚拟机控制，但既然是内存，当内存不足时就会抛出 OutOfMemoryError 异常。 直接内存与堆内存比较 直接内存申请空间耗费更高的性能 直接内存读取 IO 的性能要优于普通的堆内存。 直接内存作用链： 本地 IO -> 直接内存 -> 本地 IO 堆内存作用链：本地 IO -> 直接内存 -> 非直接内存 -> 直接内存 -> 本地 IO 服务器管理员在配置虚拟机参数时，会根据实际内存设置-Xmx等参数信息，但经常忽略直接内存，使得各个内存区域总和大于物理内存限制，从而导致动态扩展时出现OutOfMemoryError异常。 "},"jvm/2.HotSpot虚拟机对象探秘.html":{"url":"jvm/2.HotSpot虚拟机对象探秘.html","title":"2.HotSpot虚拟机对象探秘.md","keywords":"","body":"HotSpot 虚拟机对象探秘对象的内存布局对象头实例数据对齐填充对象的创建过程类加载检查为新生对象分配内存初始化对象的访问方式句柄访问方式直接指针访问方式HotSpot 虚拟机对象探秘 对象的内存布局 在 HotSpot 虚拟机中，对象的内存布局分为以下 3 块区域： 对象头（Header） 实例数据（Instance Data） 对齐填充（Padding） 对象头 对象头记录了对象在运行过程中所需要使用的一些数据： 哈希码 GC 分代年龄 锁状态标志 线程持有的锁 偏向线程 ID 偏向时间戳 -->(与偏向锁等相关的知识) 对象头可能包含类型指针，通过该指针能确定对象属于哪个类。如果对象是一个数组，那么对象头还会包括数组长度。 实例数据 实例数据部分就是成员变量的值，其中包括父类成员变量和本类成员变量。 对齐填充 用于确保对象的总长度为 8 字节的整数倍。 HotSpot VM 的自动内存管理系统要求对象的大小必须是 8 字节的整数倍。而对象头部分正好是 8 字节的倍数（1 倍或 2 倍），因此，当对象实例数据部分没有对齐时，就需要通过对齐填充来补全。 对齐填充并不是必然存在，也没有特别的含义，它仅仅起着占位符的作用。 对象的创建过程 类加载检查 虚拟机在解析.class文件时，若遇到一条 new 指令，首先它会去检查常量池中是否有这个类的符号引用，并且检查这个符号引用所代表的类是否已被加载、解析和初始化过。如果没有，那么必须先执行相应的类加载过程。 为新生对象分配内存 对象所需内存的大小在类加载完成后便可完全确定，接下来从堆中划分一块对应大小的内存空间给新的对象。分配堆中内存有两种方式： 指针碰撞 如果 Java 堆中内存绝对规整（说明采用的是“复制算法”或“标记整理法”），空闲内存和已使用内存中间放着一个指针作为分界点指示器，那么分配内存时只需要把指针向空闲内存挪动一段与对象大小一样的距离，这种分配方式称为“指针碰撞”。 空闲列表 如果 Java 堆中内存并不规整，已使用的内存和空闲内存交错（说明采用的是标记-清除法，有碎片），此时没法简单进行指针碰撞， VM 必须维护一个列表，记录其中哪些内存块空闲可用。分配之时从空闲列表中找到一块足够大的内存空间划分给对象实例。这种方式称为“空闲列表”。 初始化 分配完内存后，为对象中的成员变量赋上初始值，设置对象头信息，调用对象的构造函数方法进行初始化。 至此，整个对象的创建过程就完成了。 对象的访问方式 所有对象的存储空间都是在堆中分配的，但是这个对象的引用却是在堆栈中分配的。也就是说在建立一个对象时两个地方都分配内存，在堆中分配的内存实际建立这个对象，而在堆栈中分配的内存只是一个指向这个堆对象的指针（引用）而已。 那么根据引用存放的地址类型的不同，对象有不同的访问方式。 句柄访问方式 堆中需要有一块叫做“句柄池”的内存空间，句柄中包含了对象实例数据与类型数据各自的具体地址信息。 引用类型的变量存放的是该对象的句柄地址（reference）。访问对象时，首先需要通过引用类型的变量找到该对象的句柄，然后根据句柄中对象的地址找到对象。 直接指针访问方式 引用类型的变量直接存放对象的地址，从而不需要句柄池，通过引用能够直接访问对象。但对象所在的内存空间需要额外的策略存储对象所属的类信息的地址。 需要说明的是，HotSpot 采用第二种方式，即直接指针方式来访问对象，只需要一次寻址操作，所以在性能上比句柄访问方式快一倍。但像上面所说，它需要额外的策略来存储对象在方法区中类信息的地址。 "},"jvm/3.对象存亡与垃圾收集算法.html":{"url":"jvm/3.对象存亡与垃圾收集算法.html","title":"3.对象存亡与垃圾收集算法.md","keywords":"","body":"垃圾收集策略与算法1.判定对象是否存活引用计数法可达性分析法2.引用的种类强引用（Strong Reference）软引用（Soft Reference）弱引用（Weak Reference）虚引用（Phantom Reference）3.回收堆中无效对象判定 finalize() 是否有必要执行对象重生或死亡4.回收方法区内存判定废弃常量判定无用的类5.垃圾收集算法分代收集理论标记-清除算法标志-复制算法（新生代）标记-整理算法（老年代）分代收集算法垃圾收集策略与算法 程序计数器、虚拟机栈、本地方法栈随线程而生，也随线程而灭；栈帧随着方法的开始而入栈，随着方法的结束而出栈。这几个区域的内存分配和回收都具有确定性，在这几个区域内不需要过多考虑回收的问题，因为方法结束或者线程结束时，内存自然就跟随着回收了。 而对于 Java 堆和方法区，我们只有在程序运行期间才能知道会创建哪些对象，这部分内存的分配和回收都是动态的，垃圾收集器所关注的正是这部分内存。 1.判定对象是否存活 在堆里几乎存放了java世界中所有的对象试列,垃圾收集器在对堆进行垃圾回收前,需要先判断哪些对象已经死亡 引用计数法 在对象头维护着一个 counter 计数器，对象被引用一次则计数器 +1；若引用失效则计数器 -1。当计数器为 0 时，就认为该对象无效了。 引用计数算法的实现简单，判定效率也很高，在大部分情况下它都是一个不错的算法。但是主流的 Java 虚拟机里没有选用引用计数算法来管理内存，主要是因为它很难解决对象之间循环引用的问题。 举个栗子 &#x1F449; 对象 objA 和 objB 都有字段 instance，令 objA.instance = objB 并且 objB.instance = objA，由于它们互相引用着对方，导致它们的引用计数都不为 0，于是引用计数算法无法通知 GC 收集器回收它们。 可达性分析法 这个算法的思想是通过一些列称为\"GC Roots\"的跟对象作为起始节点集,从这些节点开始,根据引用关系向下搜索, 搜索过程称为\"引用链\".如果某个对象没有和roots有任何引用链相连,用图论的话讲就是两节点间没有通路,就说明该对象不能再被使用 所有和 GC Roots 直接或间接关联的对象都是有效对象，和 GC Roots 没有关联的对象就是无效对象。 如上图,Object 5就是不能再使用的对象 GC Roots 是指： Java 虚拟机栈（栈帧中的本地变量表）中引用的对象 本地方法栈中引用的对象 方法区中常量引用的对象 方法区中类静态属性引用的对象 java 虚拟机内部的引用,如基本数据类型对应的class 对象,系统类加载器等. GC Roots 并不包括堆中对象所引用的对象，这样就不会有循环引用的问题。 2.引用的种类 无论是使用引用计数法,还是可达性分析算法,判定对象是否存活都与“引用”有关。在 JDK 1.2 以前，Java 中的引用定义很传统,，一个对象只有被引用或者没有被引用两种状态，我们希望能描述这一类对象：当内存空间还足够时，则保留在内存中；如果内存空间在进行垃圾收集后还是非常紧张，则可以抛弃这些对象。很多系统的缓存功能都符合这样的应用场景。 在 JDK 1.2 之后，Java 对引用的概念进行了扩充，将引用分为了以下四种。不同的引用类型，主要体现的是对象不同的可达性状态reachable和垃圾收集的影响。 强引用（Strong Reference） 类似 \"Object obj = new Object()\" 这类的引用，就是强引用，只要强引用存在，垃圾收集器永远不会回收被引用的对象。但是，如果我们错误地保持了强引用，比如：赋值给了 static 变量，那么对象在很长一段时间内不会被回收，会产生内存泄漏。 软引用（Soft Reference） 软引用用来描述一些还有用,但非必须的对象.只被软引用关联着的对象,在系统发出oom之前,会讲这些对象列进回收范围之内进行二次回收.如果这次回收还没有足够的内存,系统再抛出oom异常. 弱引用（Weak Reference） 弱引用的强度比软引用更弱一些。当 JVM 进行垃圾回收时，无论内存是否充足，都会回收只被弱引用关联的对象。 虚引用（Phantom Reference） 虚引用也称幽灵引用或者幻影引用，它是最弱的一种引用关系。一个对象是否有虚引用的存在，完全不会对其生存时间构成影响,也无法通过虚引用来取得一个对象实列。为一个对象设置虚引用的目的,只是为了能在这个对象被收集器回收时收到一个系统通知.它仅仅是提供了一种确保对象被 finalize 以后，做某些事情的机制，比如，通常用来做所谓的 Post-Mortem 清理机制。 3.回收堆中无效对象 对于可达性分析中不可达的对象，也并不是没有存活的可能。 判定 finalize() 是否有必要执行 JVM 会判断此对象是否有必要执行 finalize() 方法，如果对象没有覆盖 finalize() 方法，或者 finalize() 方法已经被虚拟机调用过，那么视为“没有必要执行”。那么对象基本上就真的被回收了。 如果对象被判定为有必要执行 finalize() 方法，那么对象会被放入一个 F-Queue 队列中，虚拟机会以较低的优先级执行这些 finalize()方法，但不会确保所有的 finalize() 方法都会执行结束。如果 finalize() 方法出现耗时操作，虚拟机就直接停止指向该方法，将对象清除。 对象重生或死亡 如果在执行 finalize() 方法时，将 this 赋给了某一个引用，那么该对象就重生了。如果没有，那么就会被垃圾收集器清除。 任何一个对象的 finalize() 方法只会被系统自动调用一次，如果对象面临下一次回收，它的 finalize() 方法不会被再次执行，想继续在 finalize() 中自救就失效了。 4.回收方法区内存 方法区中存放生命周期较长的类信息、常量、静态变量，每次垃圾收集只有少量的垃圾被清除。方法区中主要清除两种垃圾： 废弃常量 无用的类 判定废弃常量 只要常量池中的常量不被任何变量或对象引用，那么这些常量就会被清除掉。比如，一个字符串 \"bingo\" 进入了常量池，但是当前系统没有任何一个 String 对象引用常量池中的 \"bingo\" 常量，也没有其它地方引用这个字面量，必要的话，\"bingo\"常量会被清理出常量池。 判定无用的类 判定一个类是否是“无用的类”，条件较为苛刻。 该类的所有对象都已经被清除 加载该类的 ClassLoader 已经被回收 该类的 java.lang.Class 对象没有在任何地方被引用，无法在任何地方通过反射访问该类的方法。 一个类被虚拟机加载进方法区，那么在堆中就会有一个代表该类的对象：java.lang.Class。这个对象在类被加载进方法区时创建，在方法区该类被删除时清除。 5.垃圾收集算法 学会了如何判定无效对象、无用类、废弃常量之后，剩余工作就是回收这些垃圾。常见的垃圾收集算法有以下几个： 分代收集理论 分代收集建立在两个假说之上: 弱分代假说:绝大多数对象都是朝生夕灭的 强分代假说:熬过越多次垃圾收集过程的对象就越难以消亡 这两个假说共同奠定了多款常用的垃圾收集器的一致的设计原则:收集器应该讲JAVA堆划分出不同的区域,然后将回收的对象依据其年龄分配到不同的区域中进行存储. java堆划分出不同的区域之后,垃圾收集器才可以每次只回收其中一个或几个的区域--因而才有了\"minor GC\",\"major Gc\",\"Full GC\"等,同时,可根据不同区域的对象特征,运用不同的垃圾收集算法以达到更好的效果 --因而发展出了\"标志-复制算法\",\"标志--清除\",\"标志-整理\"等多种垃圾回收算法. 设计者一般至少会把java堆划分为新生代和老生代.顾名思义,在新生代中,每次回收都会有大量的对象死去,而每次 回收后存活的对象,将会着步晋升到老生代中存储 部分收集(Partial GC): 新生代收集(Minor GC/YONG GC) 老生代收集(Major GC) 混合收集(Mixed GC):指收集整个新生代以及部分的老生代 Full GC: 收集整个Java 堆 和方法区 标记-清除算法 标记的过程是：遍历所有的 GC Roots，然后将所有 GC Roots 可达的对象标记为存活的对象。 清除的过程将遍历堆中所有的对象，将没有标记的对象全部清除掉。与此同时，清除那些被标记过的对象的标记，以便下次的垃圾回收。 这种方法有两个不足： 效率问题：标记和清除两个过程的效率都不高。 空间问题：标记清除之后会产生大量不连续的内存碎片，碎片太多可能导致以后需要分配较大对象时，无法找到足够的连续内存而不得不提前触发另一次垃圾收集动作。 标记清除算法是最基础的算法,后续算法大都是对该算法的改进版本 标志-复制算法（新生代） 为了解决效率问题，“复制”收集算法出现了。它将可用内存按容量划分为大小相等的两块，每次只使用其中的一块。当这一块内存用完，需要进行垃圾收集时，就将存活者的对象复制到另一块上面，然后将第一块内存全部清除。这种算法有优有劣： 优点：不会有内存碎片的问题。 缺点：内存缩小为原来的一半，浪费空间。 为了解决空间利用率问题，可以将内存分为三块： Eden、From Survivor、To Survivor，比例是 8:1:1，每次使用 Eden 和其中一块 Survivor。回收时，将 Eden 和 Survivor 中还存活的对象一次性复制到另外一块 Survivor 空间上，最后清理掉 Eden 和刚才使用的 Survivor 空间。这样只有 10% 的内存被浪费。 但是我们无法保证每次回收都只有不多于 10% 的对象存活，当 Survivor 空间不够，需要依赖其他内存（指老年代）进行分配担保。 分配担保 为对象分配内存空间时，如果 Eden+Survivor 中空闲区域无法装下该对象，会触发 MinorGC 进行垃圾收集。但如果 Minor GC 过后依然有超过 10% 的对象存活，这样存活的对象直接通过分配担保机制进入老年代，然后再将新对象存入 Eden 区。 标记-整理算法（老年代） 标记：它的第一个阶段与标记/清除算法是一模一样的，均是遍历 GC Roots，然后将存活的对象标记。 整理：移动所有存活的对象,让对象往一侧进行移动，且按照内存地址次序依次排列，然后将末端内存地址以后的内存全部回收。因此，第二阶段才称为整理阶段。 这是一种老年代的垃圾收集算法。老年代的对象一般寿命比较长，因此每次垃圾回收会有大量对象存活，如果采用复制算法，每次需要复制大量存活的对象，效率很低。 问题在于,移动存活对象并更新所有引用这些对象的地方将会是一种极为符重的操作,而且这种对象移动操作必须全程暂停用户应用进程才能进行,像这种停顿被称为\"stop the world\" 于是出现了一种和稀泥的方式: 绝大部分时间使用标记-清除算法,暂时容忍内存碎片的存在,当内存空间的碎片化程度已经影响了对象的内存分配时,再采用一次标志-整理算法,以获得规整的内存空间. 分代收集算法 根据对象存活周期的不同，将内存划分为几块。一般是把 Java 堆分为新生代和老年代，针对各个年代的特点采用最适当的收集算法。 新生代：复制算法 老年代：标记-清除算法、标记-整理算法 "},"jvm/4.HotSpot的算法细节与垃圾收集器.html":{"url":"jvm/4.HotSpot的算法细节与垃圾收集器.html","title":"4.HotSpot的算法细节与垃圾收集器.md","keywords":"","body":"HotSpot的算法细节实现根节点枚举安全点安全区域记忆集与卡表写屏障并发的可达性分析HotSpot 垃圾收集器Serial收集器ParNew收集器Parallel Scavenge收集器Serial Old收集器Parallel Old收集器CMS收集器Garbage First收集器G1对比CMSHotSpot的算法细节实现 根节点枚举 我们以可达性分析算法中从GC Roots集合找引用链这个操作作为介绍虚拟机高效实现的第一个例子。固定可作为GC Roots的节点主要在全局性的引用(例如常量或类静态属性)与执行上下文(例如栈帧中的本地变量表)中，尽管目标明确，但查找过程要做到高效并非一件容易的事情，现在Java应用越做越庞大，光是方法区的大小就常有数百上千兆，里面的类、常量等更是恒河沙数，若要逐个检 查以这里为起源的引用肯定得消耗不少时间。 迄今为止，所有收集器在根节点枚举这一步骤时都是必须暂停用户线程的，因此毫无疑问根节点枚举与之前提及的整理内存碎片一样会面临相似的“Stop The World”的困扰。现在可达性分析算法耗时最长的查找引用链的过程已经可以做到与用户线程一起并发，但根节点枚举始终还是必须在一个能保障一致性的快照中才得以进行——这里“一致性”的意思是整个枚举期间执行子系统看起来就像被冻结在某个时间点上，不会出现分析过程中，根节点集合的对象引用关系还在不断变化的情况，若这点不能满足的话，分析结果准确性也就无法保证。这是导致垃圾收集过程必须停顿所有用户线程的其中一个重要原因，即使是号称停顿时间可控，或者(几乎)不会发生停顿的CMS、G1、 ZGC等收集器，枚举根节点时也是必须要停顿的。 由于目前主流Java虚拟机使用的都是准确式垃圾收集，所以当用户线程停顿下来之后，其实并不需要一个不漏地检查完所有执行上下文和全局的引用位置，虚拟机应当是有办法直接得到哪些地方存放着对象引用的。 在HotSpot的解决方案里，是使用一组称为OopMap的数据结构来达到这个目的。一旦类加载动作完成的时候， HotSpot就会把对象内什么偏移量上是什么类型的数据计算出来，在即时编译过程中，也会在特定的位置记录下栈里和寄存器里哪些位置是引用。这样收集器在扫描时就可以直接得知这些信息了，并不需要真正一个不漏地从方法区等GC Roots开始查找。 下面代码清单3-3是HotSpot虚拟机客户端模式下生成的一段String::hashCode()方法的本地代码，可 以看到在0x026eb7a9处的call指令有OopMap记录，它指明了EBX寄存器和栈中偏移量为16的内存区域 中各有一个普通对象指针(Ordinary Object Pointer，OOP)的引用，有效范围为从call指令开始直到0x026eb730(指令流的起始位置)+142(OopMap记录的偏移量)=0x026eb7be，即hlt指令为止。 安全点 在OopMap的协助下，HotSpot可以快速准确地完成GC Roots枚举，但一个很现实的问题随之而 来:可能导致引用关系变化，或者说导致OopMap内容变化的指令非常多，如果为每一条指令都生成对应的OopMap，那将会需要大量的额外存储空间，这样垃圾收集伴随而来的空间成本就会变得无法忍受的高昂。 实际上HotSpot也的确没有为每条指令都生成OopMap，前面已经提到，只是在“特定的位置”记录 了这些信息，这些位置被称为安全点(Safepoint)。有了安全点的设定，也就决定了用户程序执行时并非在代码指令流的任意位置都能够停顿下来开始垃圾收集，而是强制要求必须执行到达安全点后才能够暂停。因此，安全点的选定既不能太少以至于让收集器等待时间过长，也不能太过频繁以至于过 分增大运行时的内存负荷。 安全点位置的选取基本上是以“是否具有让程序长时间执行的特征”为标准进行选定的，因为每条指令执行的时间都非常短暂，程序不太可能因为指令流长度太长这样的原因而长时间执行，“长时间执行”的最明显特征就是指令序列的复用，例如方法调用、循环跳转、异常跳转等都属于指令序列复用，所以只有具有这些功能的指令才会产生安全点。 对于安全点，另外一个需要考虑的问题是，如何在垃圾收集发生时让所有线程(这里其实不包括执行JNI调用的线程)都跑到最近的安全点，然后停顿下来。 这里有两种方案可供选择:抢先式中断 (Preemptive Suspension)和主动式中断(Voluntary Suspension) 抢先式中断不需要线程的执行代码主动去配合，在垃圾收集发生时，系统首先把所有用户线程全部中断，如果发现有用户线程中断的地方不在安全点上，就恢复这条线程执行，让它一会再重新中断，直到跑到安全点上。现在几乎没有虚 拟机实现采用抢先式中断来暂停线程响应GC事件。 而主动式中断的思想是当垃圾收集需要中断线程的时候，不直接对线程操作，仅仅简单地设置一个标志位，各个线程执行过程时会不停地主动去轮询这个标志，一旦发现中断标志为真时就自己在最近的安全点上主动中断挂起。轮询标志的地方和安全点是重合的，另外还要加上所有创建对象和其他需要在Java堆上分配内存的地方，这是为了检查是否即将要发生垃圾收集，避免没有足够内存分配新对象。 由于轮询操作在代码中会频繁出现，这要求它必须足够高效。HotSpot使用内存保护陷阱的方式， 把轮询操作精简至只有一条汇编指令的程度。下面代码清单3-4中的test指令就是HotSpot生成的轮询指令，当需要暂停用户线程时，虚拟机把0x160100的内存页设置为不可读，那线程执行到test指令时就会产生一个自陷异常信号，然后在预先注册的异常处理器中挂起线程实现等待，这样仅通过一条汇编指令便完成安全点轮询和触发线程中断了。 安全区域 使用安全点的设计似乎已经完美解决如何停顿用户线程，让虚拟机进入垃圾回收状态的问题了， 但实际情况却并不一定。安全点机制保证了程序执行时，在不太长的时间内就会遇到可进入垃圾收集过程的安全点。但是，程序“不执行”的时候呢?所谓的程序不执行就是没有分配处理器时间，典型的场景便是用户线程处于Sleep 状态或者Blocked状态，这时候线程无法响应虚拟机的中断请求，不能再走到安全的地方去中断挂起自己，虚拟机也显然不可能持续等待线程重新被激活分配处理器时间。对于这种情况，就必须引入安全区域(Safe Region)来解决。 安全区域是指能够确保在某一段代码片段之中，引用关系不会发生变化，因此，在这个区域中任意地方开始垃圾收集都是安全的。我们也可以把安全区域看作被扩展拉伸了的安全点。 当用户线程执行到安全区域里面的代码时，首先会标识自己已经进入了安全区域，那样当这段时间里虚拟机要发起垃圾收集时就不必去管这些已声明自己在安全区域内的线程了。当线程要离开安全区域时，它要检查虚拟机是否已经完成了根节点枚举(或者垃圾收集过程中其他需要暂停用户线程的阶段)，如果完成了，那线程就当作没事发生过，继续执行;否则它就必须一直等待，直到收到可以离开安全区域的信号为止。 记忆集与卡表 在垃圾收集,扫描时,有可能会发生老生代引用新生代对象的跨代问题,为此,垃圾收集器在新生代中建立了名为记忆集(Remembered Set)的数据结构,标记了老生代中哪些区域可能会引用新生代,从而避免把整个老年代加进GC Roots扫描范围`。 事实上并不只是新生代、老年代之间才有跨代引用的问题，所有涉及部分区域收集(Partial GC)行为的垃圾 收 集 器 ， 典 型 的 如 G 1 、 Z G C 和 Sh e n a n d o a h 收 集 器 ， 都 会 面 临 相 同 的 问 题 ， 因 此 我 们 有 必 要 进 一 步 理清记忆集的原理和实现方式，以便在后续章节里介绍几款最新的收集器相关知识时能更好地理解。 记忆集是一种用于记录从非收集区域指向收集区域的指针集合的抽象数据结构。如果我们不考虑效率和成本的话，最简单的实现可以用非收集区域中所有含跨代引用的对象数组来实现这个数据结构，如代码清单3-5所示: 这种记录全部含跨代引用对象的实现方案，无论是空间占用还是维护成本都相当高昂。而在垃圾收集的场景中，收集器只需要通过记忆集判断出某一块非收集区域是否存在有指向了收集区域的指针就可以了，并不需要了解这些跨代指针的全部细节。那设计者在实现记忆集的时候，便可以选择更为 粗犷的记录粒度来节省记忆集的存储和维护成本，下面列举了一些可供选择(当然也可以选择这个范 围以外的)的记录精度: 字长精度:每个记录精确到一个机器字长(就是处理器的寻址位数，如常见的32位或64位，这个 精度决定了机器访问物理内存地址的指针长度)，该字包含跨代指针。 对象精度:每个记录精确到一个对象，该对象里有字段含有跨代指针。 卡精度:每个记录精确到一块内存区域，该区域内有对象含有跨代指针。 其中，第三种“卡精度”所指的是用一种称为“卡表”(Card Table)的方式去实现记忆集，这也是目前最常用的一种记忆集实现形式，一些资料中甚至直接把它和记忆集混为一谈。前面定义中提到记忆集其实是一种“抽象”的数据结构，抽象的意思是只定义了记忆集的行为意图，并没有定义其行为的具体实现。卡表就是记忆集的一种具体实现，它定义了记忆集的记录精度、与堆内存的映射关系等。 关于卡表与记忆集的关系，读者不妨按照Java语言中HashMap与Map的关系来类比理解。 卡表最简单的形式可以只是一个字节数组，而HotSpot虚拟机确实也是这样做的。以下这行代码是HotSpot默认的卡表标记逻辑: 字节数组CARD_TABLE的每一个元素都对应着其标识的内存区域中一块特定大小的内存块，这个内存块被称作“卡页”(Card Page)。一般来说，卡页大小都是以2的N次幂的字节数，通过上面代码可以看出HotSpot中使用的卡页是2的9次幂，即512字节(地址右移9位，相当于用地址除以512)。那如果卡表标识内存区域的起始地址是0x0000的话，数组CARD_TABLE的第0、1、2号元素，分别对应了 地址范围为0x0000~0x01FF、0x0200~0x03FF、0x0400~0x05FF的卡页内存块，如图3-5所示。 一个卡页的内存中通常包含不止一个对象，只要卡页内有一个(或更多)对象的字段存在着跨代指针，那就将对应卡表的数组元素的值标识为1，称为这个元素变脏(Dirty)，没有则标识为0。在垃圾收集发生时，只要筛选出卡表中变脏的元素，就能轻易得出哪些卡页内存块中包含跨代指针，把它们加入GC Roots中一并扫描。 写屏障 我们已经解决了如何使用记忆集来缩减GC Roots扫描范围的问题，但还没有解决卡表元素如何维护的问题，例如它们何时变脏、谁来把它们变脏等。 卡表元素何时变脏的答案是很明确的——有其他分代区域中对象引用了本区域对象时，其对应的卡表元素就应该变脏，变脏时间点原则上应该发生在引用类型字段赋值的那一刻。 但问题是如何变脏，即如何在对象赋值的那一刻去更新维护卡表呢?假如是解释执行的字节码，那相对好处理，虚拟机负责每条字节码指令的执行，有充分的介入空间;但在编译执行的场景中呢?经过即时编译后的代 码已经是纯粹的机器指令流了，这就必须找到一个在机器码层面的手段，把维护卡表的动作放到每一 个赋值操作之中。 在HotSpot虚拟机里是通过写屏障(Write Barrier)技术维护卡表状态的。先请读者注意将这里提到的“写屏障”，以及后面在低延迟收集器中会提到的“读屏障”与解决并发乱序执行问题中的“内存屏障”区分开来，避免混淆。 写屏障可以看作在虚拟机层面对“引用类型字段赋值”这个动作的AOP切面，在引用对象赋值时会产生一个环形(Around)通知，供程序执行额外的动作，也就是说赋值的前后都在写屏障的覆盖范畴内。在赋值前的部分的写屏障叫作写前屏障(Pre-Write Barrier)，在赋值后的则叫作写后屏障(Post-Write Barrier)。HotSpot虚拟机的许多收集器中都有使用到写屏障，但直至G1收集器出现之前，其他收集器都只用到了写后屏障。下面这段代码清单3-6是一段更新卡表状态的简化逻辑: 应用写屏障后，虚拟机就会为所有赋值操作生成相应的指令，一旦收集器在写屏障中增加了更新卡表操作，无论更新的是不是老年代对新生代对象的引用，每次只要对引用进行更新，就会产生额外的开销，不过这个开销与Minor GC时扫描整个老年代的代价相比还是低得多的。 除了写屏障的开销外，卡表在高并发场景下还面临着“伪共享”(False Sharing)问题。伪共享是处理并发底层细节时一种经常需要考虑的问题，现代中央处理器的缓存系统中是以缓存行(Cache Line) 为单位存储的，当多线程修改互相独立的变量时，如果这些变量恰好共享同一个缓存行，就会彼此影响(写回、无效化或者同步)而导致性能降低，这就是伪共享问题。 假设处理器的缓存行大小为64字节，由于一个卡表元素占1个字节，64个卡表元素将共享同一个缓存行。这64个卡表元素对应的卡页总的内存为32KB(64×512字节)，也就是说如果不同线程更新的对象正好处于这32KB的内存区域内，就会导致更新卡表时正好写入同一个缓存行而影响性能。为了避免伪共享问题，一种简单的解决方案是不采用无条件的写屏障，而是先检查卡表标记，只有当该卡表元素未被标记过时才将其标记为变脏 在JDK 7之后，HotSpot虚拟机增加了一个新的参数-XX:+UseCondCardMark，用来决定是否开启 卡表更新的条件判断。开启会增加一次额外判断的开销，但能够避免伪共享问题，两者各有性能损耗，是否打开要根据应用实际运行情况来进行测试权衡。 并发的可达性分析 当前主流编程语言的垃圾收集器基本上都是依靠可达性分析算法来判定对象是否存活的，可达性分析算法理论上要求全过程都基于一个能保障一致性的快照中才能够进行分析， 这意味着必须全程冻结用户线程的运行。 在根节点枚举这个步骤中，由于GC Roots相比起整个Java堆中全部的对象毕竟还算是极少数，且在各种优化技巧(如OopMap)的加持下，它带来的停顿已经是非常短暂且相对固定(不随堆容量而增长)的了。 可从GC Roots再继续往下遍历对象图，这一步骤的停顿时间就必定会与Java堆容量直接成正比例关系了:堆越大，存储的对象越多，对象图结构越复杂，要标记更多对象而产生的停顿时间自然就更长，这听起来是理所当然的事情。 要知道包含“标记”阶段是所有追踪式垃圾收集算法的共同特征，如果这个阶段会随着堆变大而等比例增加停顿时间，其影响就会波及几乎所有的垃圾收集器，同理可知，如果能够削减这部分停顿时间的话，那收益也将会是系统性的。 想解决或者降低用户线程的停顿，就要先搞清楚为什么必须在一个能保障一致性的快照上才能进行对象图的遍历?为了能解释清楚这个问题，我们引入三色标记(Tri-color Marking)作为工具来辅助推导，把遍历对象图过程中遇到的对象，按照“是否访问过”这个条件标记成以下三种颜色: 白色:表示对象尚未被垃圾收集器访问过。显然在可达性分析刚刚开始的阶段，所有的对象都是白色的，若在分析结束的阶段，仍然是白色的对象，即代表不可达。 黑色:表示对象已经被垃圾收集器访问过，且这个对象的所有引用都已经扫描过。黑色的对象代表已经扫描过，它是安全存活的，如果有其他对象引用指向了黑色对象，无须重新扫描一遍。黑色对象不可能直接(不经过灰色对象)指向某个白色对象。 灰色:表示对象已经被垃圾收集器访问过，但这个对象上至少存在一个引用还没有被扫描过。 关于可达性分析的扫描过程，读者不妨发挥一下想象力，把它看作对象图上一股以灰色为波峰的 波纹从黑向白推进的过程，如果用户线程此时是冻结的，只有收集器线程在工作，那不会有任何问 题。但如果用户线程与收集器是并发工作呢?收集器在对象图上标记颜色，同时用户线程在修改引用 关系——即修改对象图的结构，这样可能出现两种后果。 一种是把原本消亡的对象错误标记为存活， 这不是好事，但其实是可以容忍的，只不过产生了一点逃过本次收集的浮动垃圾而已，下次收集清理 掉就好。 另一种是把原本存活的对象错误标记为已消亡，这就是非常致命的后果了，程序肯定会因此 发生错误，下面表3-1演示了这样的致命错误具体是如何产生的。 size_16,color_FFFFFF,t_70) Wilson于1994年在理论上证明了，当且仅当以下两个条件同时满足时，会产生“对象消失”的问 题，即原本应该是黑色的对象被误标为白色: 赋值器插入了一条或多条从黑色对象到白色对象的新引用; 赋值器删除了全部从灰色对象到该白色对象的直接或间接引用。 因此，我们要解决并发扫描时的对象消失问题，只需破坏这两个条件的任意一个即可。由此分别产生了两种解决方案:增量更新(Incremental Update)和原始快照(Snapshot At The Beginning， SATB )。 增量更新要破坏的是第一个条件，当黑色对象插入新的指向白色对象的引用关系时，就将这个新插入的引用记录下来，等并发扫描结束之后，再将这些记录过的引用关系中的黑色对象为根，重新扫描一次。这可以简化理解为，黑色对象一旦新插入了指向白色对象的引用之后，它就变回灰色对象了。 原始快照要破坏的是第二个条件，当灰色对象要删除指向白色对象的引用关系时，就将这个要删除的引用记录下来，在并发扫描结束之后，再将这些记录过的引用关系中的灰色对象为根，重新扫描一次。这也可以简化理解为，无论引用关系删除与否，都会按照刚刚开始扫描那一刻的对象图快照来进行搜索。 以上无论是对引用关系记录的插入还是删除，虚拟机的记录操作都是通过写屏障实现的。在 HotSpot虚拟机中，增量更新和原始快照这两种解决方案都有实际应用，譬如，CMS是基于增量更新来做并发标记的，G1、Shenandoah则是用原始快照来实现。 到这里，笔者简要介绍了HotSpot虚拟机如何发起内存回收、如何加速内存回收，以及如何保证回 收正确性等问题，但是虚拟机如何具体地进行内存回收动作仍然未涉及。因为内存回收如何进行是由 虚拟机所采用哪一款垃圾收集器所决定的，而通常虚拟机中往往有多种垃圾收集器，下面笔者将逐一 介绍HotSpot虚拟机中出现过的垃圾收集器。 HotSpot 垃圾收集器 Serial收集器 Serial收集器是最基础、历史最悠久的收集器，曾经(在JDK 1.3.1之前)是HotSpot虚拟机新生代收集器的唯一选择。大家只看名字就能够猜到，这个收集器是一个单线程工作的收集器，但它的“单线 程”的意义并不仅仅是说明它只会使用一个处理器或一条收集线程去完成垃圾收集工作，更重要的是强调在它进行垃圾收集时，必须暂停其他所有工作线程，直到它收集结束。“Stop The World”这个词语也 许听起来很酷，但这项工作是由虚拟机在后台自动发起和自动完成的，在用户不可知、不可控的情况下把用户的正常工作的线程全部停掉，这对很多应用来说都是不能接受的。读者不妨试想一下，要是 你的电脑每运行一个小时就会暂停响应五分钟，你会有什么样的心情?图3-7示意了Serial/Serial Old收 集器的运行过程。 写到这里，笔者似乎已经把Serial收集器描述成一个最早出现，但目前已经老而无用，食之无味， 弃之可惜的“鸡肋”了，但事实上，迄今为止，它依然是HotSpot虚拟机运行在客户端模式下的默认新生代收集器，有着优于其他收集器的地方，那就是简单而高效(与其他收集器的单线程相比)，对于内存资源受限的环境，它是所有收集器里额外内存消耗(Memory Footprint)[1]最小的;对于单核处理 器或处理器核心数较少的环境来说，Serial收集器由于没有线程交互的开销，专心做垃圾收集自然可以 获得最高的单线程收集效率。在用户桌面的应用场景以及近年来流行的部分微服务应用中，分配给虚拟机管理的内存一般来说并不会特别大，收集几十兆甚至一两百兆的新生代(仅仅是指新生代使用的 内存，桌面应用甚少超过这个容量)，垃圾收集的停顿时间完全可以控制在十几、几十毫秒，最多一 百多毫秒以内，只要不是频繁发生收集，这点停顿时间对许多用户来说是完全可以接受的。所以，Serial收集器对于运行在客户端模式下的虚拟机来说是一个很好的选择。 ParNew收集器 ParNew收集器实质上是Serial收集器的多线程并行版本，除了同时使用多条线程进行垃圾收集之外 ， 其 余 的 行 为 包 括 Se r i a l 收 集 器 可 用 的 所 有 控 制 参 数 ( 例 如 : - X X : Su r v i v o r R a t i o 、 - X X : PretenureSizeThreshold、-XX:HandlePromotionFailure等)、收集算法、Stop The World、对象分配规 则、回收策略等都与Serial收集器完全一致，在实现上这两种收集器也共用了相当多的代码。ParNew收 集器的工作过程如图3-8所示。 ParNew收集器除了支持多线程并行收集之外，其他与Serial收集器相比并没有太多创新之处，但它却是不少运行在服务端模式下的HotSpot虚拟机，尤其是JDK 7之前的遗留系统中首选的新生代收集器，其中有一个与功能、性能无关但其实很重要的原因是:除了Serial收集器外，目前只有它能与CMS 收集器配合工作。 在JDK 5发布时，HotSpot推出了一款在强交互应用中几乎可称为具有划时代意义的垃圾收集器 ——CMS收集器。这款收集器是HotSpot虚拟机中第一款真正意义上支持并发的垃圾收集器，它首次实现了让垃圾收集线程与用户线程(基本上)同时工作。 遗憾的是，CMS作为老年代的收集器，却无法与JDK 1.4.0中已经存在的新生代收集器Parallel Scavenge配合工作，所以在JDK 5中使用CMS来收集老年代的时候，新生代只能选择ParNew或者 Serial收集器中的一个。 ParNew收集器是激活CMS后(使用-XX:+UseConcMarkSweep GC选项)的默认新生代收集器，也可以使用-XX:+/-UseParNewGC选项来强制指定或者禁用它。 可以说直到CMS的出现才巩固了ParNew的地位，但成也萧何败也萧何，随着垃圾收集器技术的不断改进，更先进的G1收集器带着CMS继承者和替代者的光环登场。G1是一个面向全堆的收集器，不再需要其他新生代收集器的配合工作。 所以自JDK 9开始，ParNew加CMS收集器的组合就不再是官方推荐的服务端模式下的收集器解决方案了。官方希望它能完全被G1所取代，甚至还取消了ParNew加 Serial Old以及Serial加CMS这两组收集器组合的支持(其实原本也很少人这样使用)，并直接取消了- XX:+UseParNewGC参数，这意味着ParNew和CMS从此只能互相搭配使用，再也没有其他收集器能够和它们配合了。读者也可以理解为从此以后，ParNew合并入CM S，成为它专门处理新生代的组成部分。ParNew可以说是HotSpot虚拟机中第一款退出历史舞台的垃圾收集器。 ParNew收集器在单核心处理器的环境中绝对不会有比Serial收集器更好的效果，甚至由于存在线程交互的开销，该收集器在通过超线程(Hyper-Threading)技术实现的伪双核处理器环境中都不能百分之百保证超越Serial收集器。当然，随着可以被使用的处理器核心数量的增加，ParNew对于垃圾收集时 系统资源的高效利用还是很有好处的。它默认开启的收集线程数与处理器核心数量相同，在处理器核 心非常多(譬如32个，现在CPU都是多核加超线程设计，服务器达到或超过32个逻辑核心的情况非常普遍)的环境中，可以使用-XX:ParallelGCT hreads参数来限制垃圾收集的线程数。 Parallel Scavenge收集器 Parallel Scavenge收集器也是一款新生代收集器，它同样是基于标记-复制算法实现的收集器，也是 能够并行收集的多线程收集器…Parallel Scavenge的诸多特性从表面上看和ParNew非常相似，那它有 什么特别之处呢? Parallel Scavenge收集器的特点是它的关注点与其他收集器不同，CMS等收集器的关注点是尽可能地缩短垃圾收集时用户线程的停顿时间，而Parallel Scavenge收集器的目标则是达到一个可控制的吞吐 量(Throughput)。所谓吞吐量就是处理器用于运行用户代码的时间与处理器总消耗时间的比值， 即: 如果虚拟机完成某个任务，用户代码加上垃圾收集总共耗费了100分钟，其中垃圾收集花掉1分 钟，那吞吐量就是99%。 停顿时间越短就越适合需要与用户交互或需要保证服务响应质量的程序，良好的响应速度能提升用户体验; 而高吞吐量则可以最高效率地利用处理器资源，尽快完成程序的运算任务，主要适合在后台运算而不需要太多交互的分析任务。 Parallel Scavenge收集器提供了两个参数用于精确控制吞吐量，分别是控制最大垃圾收集停顿时间的-XX:MaxGCPauseMillis参数以及直接设置吞吐量大小的-XX:GCTimeRatio参数。 -XX:MaxGCPauseMillis参数允许的值是一个大于0的毫秒数，收集器将尽力保证内存回收花费的 时间不超过用户设定值。不过大家不要异想天开地认为如果把这个参数的值设置得更小一点就能使得 系统的垃圾收集速度变得更快，垃圾收集停顿时间缩短是以牺牲吞吐量和新生代空间为代价换取的: 系统把新生代调得小一些，收集300M B新生代肯定比收集500M B快，但这也直接导致垃圾收集发生得 更频繁，原来10秒收集一次、每次停顿100毫秒，现在变成5秒收集一次、每次停顿70毫秒。停顿时间的确在下降，但吞吐量也降下来了。 -XX:GCTimeRatio参数的值则应当是一个大于0小于100的整数，也就是垃圾收集时间占总时间的 比率，相当于吞吐量的倒数。譬如把此参数设置为19，那允许的最大垃圾收集时间就占总时间的5% (即1/(1+19))，默认值为99，即允许最大1%(即1/(1+99))的垃圾收集时间。 由于与吞吐量关系密切，Parallel Scavenge收集器也经常被称作“吞吐量优先收集器”。除上述两个 参数之外，Parallel Scavenge收集器还有一个参数-XX:+UseAdaptiveSizePolicy值得我们关注。 这是一 个开关参数，当这个参数被激活之后，就不需要人工指定新生代的大小(-Xmn)、Eden与Survivor区 的 比 例 ( - X X : Su r v i v o r R a t i o ) 、 晋 升 老 年 代 对 象 大 小 ( - X X : P r e t e n u r e Si z e T h r e s h o l d ) 等 细 节 参 数 了，虚拟机会根据当前系统的运行情况收集性能监控信息，动态调整这些参数以提供最合适的停顿时间或者最大的吞吐量。这种调节方式称为垃圾收集的自适应的调节策略(GC Ergonomics)。 Serial Old收集器 Serial Old是Serial收集器的老年代版本，它同样是一个单线程收集器，使用标记-整理算法。这个收集器的主要意义也是供客户端模式下的HotSpot虚拟机使用。 如果在服务端模式下，它也可能有两种用途: 一种是在JDK 5以及之前的版本中与Parallel Scavenge收集器搭配使用 另外一种就是作为CMS 收集器发生失败时的后备预案，在并发收集发生Concurrent Mode Failure时使用。这两点都将在后面的内容中继续讲解。Serial Old收集器的工作过程如图3-9所示。 注意：需要说明一下，Parallel Scavenge收集器架构中本身有PS MarkSweep收集器来进行老年代收集，并非直接调用Serial Old收集器，但是这个PS MarkSweep收集器与Serial Old的实现几乎是一样的，所以在官方的许多资料中都是直接以Serial Old代替PS MarkSweep进行讲解，这里笔者也采用这种方式。 Parallel Old收集器 Parallel Old是Parallel Scavenge收集器的老年代版本，支持多线程并发收集，基于标记-整理算法实现。这个收集器是直到JDK 6时才开始提供的，在此之前，新生代的Parallel Scavenge收集器一直处于相当尴尬的状态，原因是如果新生代选择了Parallel Scavenge收集器，老年代除了Serial Old(PS MarkSweep)收集器以外别无选择，其他表现良好的老年代收集器，如CMS无法与它配合工作。 由于老年代Serial Old收集器在服务端应用性能上的“拖累”，使用Parallel Scavenge收集器也未必能在整体上获得吞吐量最大化的效果。同样，由于单线程的老年代收集中无法充分利用服务器多处理器的并行处理能力，在老年代内存空间很大而且硬件规格比较高级的运行环境中，这种组合的总吞吐量甚至不一 定比ParNew加CMS的组合来得优秀。 直到Parallel Old收集器出现后，“吞吐量优先”收集器终于有了比较名副其实的搭配组合，在注重吞吐量或者处理器资源较为稀缺的场合，都可以优先考虑Parallel Scavenge加Parallel Old收集器这个组合。Parallel Old收集器的工作过程如图3-10所示。 CMS收集器 CMS(Concurrent Mark Sweep)收集器是一种以获取最短回收停顿时间为目标的收集器。目前很大一部分的Java应用集中在互联网网站或者基于浏览器的B/S系统的服务端上，这类应用通常都会较为 关注服务的响应速度，希望系统停顿时间尽可能短，以给用户带来良好的交互体验。CMS收集器就非常符合这类应用的需求。 从名字(包含“Mark Sweep”)上就可以看出CMS收集器是基于标记-清除算法实现的，它的运作过程相对于前面几种收集器来说要更复杂一些，整个过程分为四个步骤，包括: 初始标记(CMS initial mark) 并发标记(CMS concurrent mark) 重新标记(CMS remark) 并发清除(CMS concurrent sweep) 其中初始标记、重新标记这两个步骤仍然需要“Stop The World”。 初始标记仅仅只是标记一下GC Roots能直接关联到的对象，速度很快; 并发标记阶段就是从GC Roots的直接关联对象开始遍历整个对象图的过程，这个过程耗时较长但是不需要停顿用户线程，可以与垃圾收集线程一起并发运行; 而重新标记阶段则是为了修正并发标记期间，因用户程序继续运作而导致标记产生变动的那一部分对象的 标记记录(详见并发可达性分析种关于增量更新的讲解)，这个阶段的停顿时间通常会比初始标记阶段稍长一 些，但也远比并发标记阶段的时间短; 最后是并发清除阶段，清理删除掉标记阶段判断的已经死亡的对象，由于不需要移动存活对象，所以这个阶段也是可以与用户线程同时并发的。 由于在整个过程中耗时最长的并发标记和并发清除阶段中，垃圾收集器线程都可以与用户线程一 起工作，所以从总体上来说，CMS收集器的内存回收过程是与用户线程一起并发执行的。通过图3-11 可以比较清楚地看到CMS收集器的运作步骤中并发和需要停顿的阶段。 CM S是一款优秀的收集器，它最主要的优点在名字上已经体现出来:并发收集、低停顿，一些官方公开文档里面也称之为“并发低停顿收集器”(Concurrent Low Pause Collector)。CMS收集器是 HotSpot虚拟机追求低停顿的第一次成功尝试，但是它还远达不到完美的程度，至少有以下三个明显的缺点: 首先，CMS收集器对处理器资源非常敏感。事实上，面向并发设计的程序都对处理器资源比较敏感。在并发阶段，它虽然不会导致用户线程停顿，但却会因为占用了一部分线程(或者说处理器的计算能力)而导致应用程序变慢，降低总吞吐量。CMS默认启动的回收线程数是(处理器核心数量 +3)/4，也就是说，如果处理器核心数在四个或以上，并发回收时垃圾收集线程只占用不超过25%的 处理器运算资源，并且会随着处理器核心数量的增加而下降。但是当处理器核心数量不足四个时， CM S对用户程序的影响就可能变得很大。如果应用本来的处理器负载就很高，还要分出一半的运算能 力去执行收集器线程，就可能导致用户程序的执行速度忽然大幅降低。为了缓解这种情况，虚拟机提 供了一种称为“增量式并发收集器”(Incremental Concurrent Mark Sweep/i-CMS)的CMS收集器变种， 所做的事情和以前单核处理器年代PC机操作系统靠抢占式多任务来模拟多核并行多任务的思想一样， 是在并发标记、清理的时候让收集器线程、用户线程交替运行，尽量减少垃圾收集线程的独占资源的 时间，这样整个垃圾收集的过程会更长，但对用户程序的影响就会显得较少一些，直观感受是速度变 慢的时间更多了，但速度下降幅度就没有那么明显。实践证明增量式的CMS收集器效果很一般，从 JDK 7开始，i-CMS模式已经被声明为“deprecated”，即已过时不再提倡用户使用，到JDK 9发布后i- CM S模式被完全废弃。 然后，由于CMS收集器无法处理“浮动垃圾”(FloatingGarbage)，有可能出现“Con-current Mode Failure”失败进而导致另一次完全“Stop The World”的Full GC的产生。在CMS的并发标记和并发清理阶段，用户线程是还在继续运行的，程序在运行自然就还会伴随有新的垃圾对象不断产生，但这一部分垃圾对象是出现在标记过程结束以后，CM S无法在当次收集中处理掉它们，只好留待下一次垃圾收集 时再清理掉。这一部分垃圾就称为“浮动垃圾”。同样也是由于在垃圾收集阶段用户线程还需要持续运 行，那就还需要预留足够内存空间提供给用户线程使用，因此CMS收集器不能像其他收集器那样等待 到老年代几乎完全被填满了再进行收集，必须预留一部分空间供并发收集时的程序运作使用。在JDK 5的默认设置下，CMS收集器当老年代使用了68%的空间后就会被激活，这是一个偏保守的设置，如果 在实际应用中老年代增长并不是太快，可以适当调高参数-XX:CMSInitiatingOccu-pancyFraction的值 来提高CMS的触发百分比，降低内存回收频率，获取更好的性能。到了JDK 6时，CMS收集器的启动 阈值就已经默认提升至92%。但这又会更容易面临另一种风险:要是CM S运行期间预留的内存无法满 足程序分配新对象的需要，就会出现一次“并发失败”(Concurrent Mode Failure)，这时候虚拟机将不得不启动后备预案:冻结用户线程的执行，临时启用Serial Old收集器来重新进行老年代的垃圾收集， 但这样停顿时间就很长了。所以参数-XX:CMSInitiatingOccupancyFraction设置得太高将会很容易导致 大量的并发失败产生，性能反而降低，用户应在生产环境中根据实际应用情况来权衡设置。 还有最后一个缺点，在本节的开头曾提到，CM S是一款基于“标记-清除”算法实现的收集器，如果读者对前面这部分介绍还有印象的话，就可能想到这意味着收集结束时会有大量空间碎片产生。空间碎片过多时，将会给大对象分配带来很大麻烦，往往会出现老年代还有很多剩余空间，但就是无法找到足够大的连续空间来分配当前对象，而不得不提前触发一次Full GC的情况。为了解决这个问题， CMS收集器提供了一个-XX:+UseCMS-CompactAtFullCollection开关参数(默认是开启的，此参数从 JDK 9开始废弃)，用于在CMS收集器不得不进行Full GC时开启内存碎片的合并整理过程，由于这个 内 存 整 理 必 须 移 动 存 活 对 象 ， ( 在 Sh e n a n d o a h 和 Z G C 出 现 前 ) 是 无 法 并 发 的 。 这 样 空 间 碎 片 问 题 是 解 决了，但停顿时间又会变长，因此虚拟机设计者们还提供了另外一个参数-XX:CM SFullGCsBefore- Compaction(此参数从JDK 9开始废弃)，这个参数的作用是要求CMS收集器在执行过若干次(数量 由参数值决定)不整理空间的Full GC之后，下一次进入Full GC前会先进行碎片整理(默认值为0，表 示每次进入Full GC时都进行碎片整理)。 Garbage First收集器 Garbage First(简称G1)收集器是垃圾收集器技术发展历史上的里程碑式的成果，它开创了收集器面向局部收集的设计思路和基于Region的内存布局形式。 G1是一款主要面向服务端应用的垃圾收集器。HotSpot开发团队最初赋予它的期望是(在比较长 期的)未来可以替换掉JDK 5中发布的CMS收集器。现在这个期望目标已经实现过半了，JDK 9发布之 日，G1宣告取代Parallel Scavenge加Parallel Old组合，成为服务端模式下的默认垃圾收集器，而CM S则沦落至被声明为不推荐使用(Deprecate)的收集器。如果对JDK 9及以上版本的HotSpot虚拟机使用 参数-XX:+UseConcMarkSweep GC来开启CMS收集器的话，用户会收到一个警告信息，提示CMS未 来将会被废弃 但作为一款曾被广泛运用过的收集器，经过多个版本的开发迭代后，CMS(以及之前几款收集 器)的代码与HotSpot的内存管理、执行、编译、监控等子系统都有千丝万缕的联系，这是历史原因导 致的，并不符合职责分离的设计原则。为此，规划JDK 10功能目标时，HotSpot虚拟机提出了“统一垃圾收集器接口”，将内存回收的“行为”与“实现”进行分离，CMS以及其他收集器都重构成基于这套 接口的一种实现。以此为基础，日后要移除或者加入某一款收集器，都会变得容易许多，风险也可以 控制，这算是在为CMS退出历史舞台铺下最后的道路了。 作为CM S收集器的替代者和继承人，设计者们希望做出一款能够建立起“停顿时间模型”(Pause Prediction Model)的收集器，停顿时间模型的意思是能够支持指定在一个长度为M毫秒的时间片段内，消耗在垃圾收集上的时间大概率不超过N毫秒这样的目标，这几乎已经是实时Java(RTSJ)的中软实时垃圾收集器特征了。 首先要有一个思想上的改变，在G1收集器出现之前的所有 其他收集器，包括CMS在内，垃圾收集的目标范围要么是整个新生代(Minor GC)，要么就是整个老年代(Major GC)，再要么就是整个Java堆(Full GC)。而G1跳出了这个樊笼，它可以面向堆内存任何部分来组成回收集(Collection Set，一般简称CSet)进行回收，衡量标准不再是它属于哪个分代，而 是哪块内存中存放的垃圾数量最多，回收收益最大，这就是G1收集器的Mixed GC模式。 G1开创的基于Region的堆内存布局是它能够实现这个目标的关键。虽然G1也仍是遵循分代收集理论设计的，但其堆内存的布局与其他收集器有非常明显的差异: G1不再坚持固定大小以及固定数量的分代区域划分，而是把连续的Java堆划分为多个大小相等的独立区域(Region)，每一个Region都可以根据需要，扮演新生代的Eden空间、Survivor空间，或者老年代空间。收集器能够对扮演不同角色的 Region采用不同的策略去处理，这样无论是新创建的对象还是已经存活了一段时间、熬过多次收集的旧对象都能获取很好的收集效果。 Region中还有一类特殊的Humongous区域，专门用来存储大对象。G1认为只要大小超过了一个 R e gi o n 容 量 一 半 的 对 象 即 可 判 定 为 大 对 象 。 每 个 R e gi o n 的 大 小 可 以 通 过 参 数- X X : G 1 H e a p R e gi o n Si z e 设 定，取值范围为1M B~32M B，且应为2的N次幂。而对于那些超过了整个Region容量的超级大对象， 将会被存放在N个连续的Humongous Region之中，G1的大多数行为都把Humongous Region作为老年代 的一部分来进行看待，如图3-12所示。 虽然G1仍然保留新生代和老年代的概念，但新生代和老年代不再是固定的了，它们都是一系列区域(不需要连续)的动态集合。G1收集器之所以能建立可预测的停顿时间模型，是因为它将Region作为单次回收的最小单元，即每次收集到的内存空间都是Region大小的整数倍，这样可以有计划地避免在整个Java堆中进行全区域的垃圾收集。 更具体的处理思路是让G1收集器去跟踪各个Region里面的垃圾堆积的“价值”大小，价值即回收所获得的空间大小以及回收所需时间的经验值，然后在后台维护一 个优先级列表，每次根据用户设定允许的收集停顿时间(使用参数-XX:MaxGCPauseMillis指定，默认值是200毫秒)，优先处理回收价值收益最大的那些Region，这也就是“Garbage First”名字的由来。 这种使用Region划分内存空间，以及具有优先级的区域回收方式，保证了G1收集器在有限的时间内获 取尽可能高的收集效率。 G1将堆内存“化整为零”的“解题思路”，看起来似乎没有太多令人惊讶之处，也完全不难理解，但 其中的实现细节可是远远没有想象中那么简单，否则就不会从2004年Sun实验室发表第一篇关于G1的 论文后一直拖到2012年4月JDK 7 Update 4发布，用将近10年时间才倒腾出能够商用的G1收集器来。 G1收集器至少有(不限于)以下这些关键的细节问题需要妥善解决: 譬如，将Java堆分成多个独立Region后，Region里面存在的跨Region引用对象如何解决?使用记忆集避免全堆作为GC Roots扫描，但在G1收集器上记忆集的应用其实要复杂很多，它的每个Region都维护有自己的记忆集，这些记忆集会记录下别的Region 指向自己的指针，并标记这些指针分别在哪些卡页的范围之内。G1的记忆集在存储结构的本质上是一 种 哈 希 表 ， K e y 是 别 的 R e g i o n 的 起 始 地 址 ， Va l u e 是 一 个 集 合 ， 里 面 存 储 的 元 素 是 卡 表 的 索 引 号 。 这 种“双向”的卡表结构(卡表是“我指向谁”，这种结构还记录了“谁指向我”)比原来的卡表实现起来更 复杂，同时由于Region数量比传统收集器的分代数量明显要多得多，因此G1收集器要比其他的传统垃 圾收集器有着更高的内存占用负担。根据经验，G1至少要耗费大约相当于Java堆容量10%至20%的额 外内存来维持收集器工作。 譬如，在并发标记阶段如何保证收集线程与用户线程互不干扰地运行?这里首先要解决的是用户线程改变对象引用关系时，必须保证其不能打破原本的对象图结构，导致标记结果出现错误，该问题 的解决办法笔者已经抽出独立小节来讲解过(见3.4.6节):CMS收集器采用增量更新算法实现，而G1 收集器则是通过原始快照(SATB)算法来实现的。此外，垃圾收集对用户线程的影响还体现在回收过 程中新创建对象的内存分配上，程序要继续运行就肯定会持续有新对象被创建，G1为每一个Region设 计了两个名为TAMS(Top at Mark Start)的指针，把Region中的一部分空间划分出来用于并发回收过 程中的新对象分配，并发回收时新分配的对象地址都必须要在这两个指针位置以上。G1收集器默认在 这个地址以上的对象是被隐式标记过的，即默认它们是存活的，不纳入回收范围。与CMS中 的“Concurrent M ode Failure”失败会导致Full GC类似，如果内存回收的速度赶不上内存分配的速度， G1收集器也要被迫冻结用户线程执行，导致Full GC而产生长时间“Stop The World”。 譬如，怎样建立起可靠的停顿预测模型?用户通过-XX:M axGCPauseM illis参数指定的停顿时间 只意味着垃圾收集发生之前的期望值，但G1收集器要怎么做才能满足用户的期望呢?G1收集器的停顿 预测模型是以衰减均值(Decaying Average)为理论基础来实现的，在垃圾收集过程中，G1收集器会记 录每个Region的回收耗时、每个Region记忆集里的脏卡数量等各个可测量的步骤花费的成本，并分析得 出平均值、标准偏差、置信度等统计信息。这里强调的“衰减平均值”是指它会比普通的平均值更容易 受到新数据的影响，平均值代表整体平均状态，但衰减平均值更准确地代表“最近的”平均状态。换句 话说，Region的统计状态越新越能决定其回收的价值。然后通过这些信息预测现在开始回收的话，由 哪些Region组成回收集才可以在不超过期望停顿时间的约束下获得最高的收益。 如果我们不去计算用户线程运行过程中的动作(如使用写屏障维护记忆集的操作)，G1收集器的 运作过程大致可划分为以下四个步骤: 初始标记(Initial Marking):仅仅只是标记一下GC Roots能直接关联到的对象，并且修改TAMS 指针的值，让下一阶段用户线程并发运行时，能正确地在可用的Region中分配新对象。这个阶段需要停顿线程，但耗时很短，而且是借用进行Minor GC的时候同步完成的，所以G1收集器在这个阶段实际 并没有额外的停顿。 并发标记(Concurrent Marking):从GC Root开始对堆中对象进行可达性分析，递归扫描整个堆里的对象图，找出要回收的对象，这阶段耗时较长，但可与用户程序并发执行。当对象图扫描完成以 后，还要重新处理SATB记录下的在并发时有引用变动的对象。 最终标记(Final Marking):对用户线程做另一个短暂的暂停，用于处理并发阶段结束后仍遗留 下来的最后那少量的SATB记录。 筛选回收(Live Data Counting and Evacuation):负责更新Region的统计数据，对各个Region的回收价值和成本进行排序，根据用户所期望的停顿时间来制定回收计划，可以自由选择任意多个Region 构成回收集，然后把决定回收的那一部分Region的存活对象复制到空的Region中，再清理掉整个旧 Region的全部空间。这里的操作涉及存活对象的移动，是必须暂停用户线程，由多条收集器线程并行完成的。 从上述阶段的描述可以看出，G1收集器除了并发标记外，其余阶段也是要完全暂停用户线程的， 换言之，它并非纯粹地追求低延迟，官方给它设定的目标是在延迟可控的情况下获得尽可能高的吞吐量，所以才能担当起“全功能收集器”的重任与期望。 从Oracle官方透露出来的信息可获知，回收阶段(Evacuat ion)其实本也有想过设计成与用户程序 一起并发执行，但这件事情做起来比较复杂，考虑到G1只是回收一部分Region，停顿时间是用户可控 制的，所以并不迫切去实现，而选择把这个特性放到了G1之后出现的低延迟垃圾收集器(即ZGC) 中。另外，还考虑到G1不是仅仅面向低延迟，停顿用户线程能够最大幅度提高垃圾收集效率，为了保 证吞吐量所以才选择了完全暂停用户线程的实现方案。通过图3-13可以比较清楚地看到G1收集器的运 作步骤中并发和需要停顿的阶段。 毫无疑问，可以由用户指定期望的停顿时间是G1收集器很强大的一个功能，设置不同的期望停顿 时间，可使得G1在不同应用场景中取得关注吞吐量和关注延迟之间的最佳平衡。不过，这里设置 的“期望值”必须是符合实际的，不能异想天开，毕竟G1是要冻结用户线程来复制对象的，这个停顿时 间再怎么低也得有个限度。它·默认的停顿目标为两百毫秒·，一般来说，回收阶段占到几十到一百甚至 接近两百毫秒都很正常，但如果我们把停顿时间调得非常低，譬如设置为二十毫秒，很可能出现的结 果就是由于停顿目标时间太短，导致每次选出来的回收集只占堆内存很小的一部分，收集器收集的速 度逐渐跟不上分配器分配的速度，导致垃圾慢慢堆积。很可能一开始收集器还能从空闲的堆内存中获 得一些喘息的时间，但应用运行时间一长就不行了，最终占满堆引发Full GC反而降低性能，所以通常 把期望停顿时间设置为一两百毫秒或者两三百毫秒会是比较合理的。 G1对比CMS 相比CMS，G1的优点有很多，暂且不论可以指定最大停顿时间、分Region的内存布局、按收益动态确定回收集这些创新性设计带来的红利，单从最传统的算法理论上看，G1也更有发展潜力。与CMS 的“标记-清除”算法不同，G1从整体来看是基于“标记-整理”算法实现的收集器，但从局部(两个Region 之间)上看又是基于“标记-复制”算法实现，无论如何，这两种算法都意味着G1运作期间不会产生内存 空间碎片，垃圾收集完成之后能提供规整的可用内存。这种特性有利于程序长时间运行，在程序为大 对象分配内存时不容易因无法找到连续内存空间而提前触发下一次收集。 不过，G1相对于CMS仍然不是占全方位、压倒性优势的，从它出现几年仍不能在所有应用场景中代替CMS就可以得知这个结论。比起CMS，G1的弱项也可以列举出不少，如在用户程序运行过程 中，G1无论是为了垃圾收集产生的内存占用(Footprint)还是程序运行时的额外执行负载 (Overload)都要比CMS要高。 就内存占用来说，虽然G1和CMS都使用卡表来处理跨代指针，但G1的卡表实现更为复杂，而且 堆中每个Region，无论扮演的是新生代还是老年代角色，都必须有一份卡表，这导致G1的记忆集(和 其他内存消耗)可能会占整个堆容量的20%乃至更多的内存空间;相比起来CMS的卡表就相当简单， 只有唯一一份，而且只需要处理老年代到新生代的引用，反过来则不需要，由于新生代的对象具有朝生夕灭的不稳定性，引用变化频繁，能省下这个区域的维护开销是很划算的。 在执行负载的角度上，同样由于两个收集器各自的细节实现特点导致了用户程序运行时的负载会 有不同，譬如它们都使用到写屏障，CMS用写后屏障来更新维护卡表;而G1除了使用写后屏障来进行 同样的(由于G1的卡表结构复杂，其实是更烦琐的)卡表维护操作外，为了实现原始快照搜索 (SATB)算法，还需要使用写前屏障来跟踪并发时的指针变化情况。相比起增量更新算法，原始快照 搜索能够减少并发标记和重新标记阶段的消耗，避免CMS那样在最终标记阶段停顿时间过长的缺点， 但是在用户程序运行过程中确实会产生由跟踪引用变化带来的额外负担。由于G1对写屏障的复杂操作 要比CM S消耗更多的运算资源，所以CM S的写屏障实现是直接的同步操作，而G1就不得不将其实现 为类似于消息队列的结构，把写前屏障和写后屏障中要做的事情都放到队列里，然后再异步处理。 以上的优缺点对比仅仅是针对G1和CM S两款垃圾收集器单独某方面的实现细节的定性分析，通常 我们说哪款收集器要更好、要好上多少，往往是针对具体场景才能做的定量比较。 目前在小内存应用上CMS的表现大概率仍然要会优于G1，而在大内存应用上G1则大多能发挥其 优势，这个优劣势的Java堆容量平衡点通常在6GB至8GB之间，当然，以上这些也仅是经验之谈，不 同应用需要量体裁衣地实际测试才能得出最合适的结论，随着HotSpot的开发者对G1的不断优化，也 会让对比结果继续向G1倾斜。 "},"jvm/5.内存分配与回收策略.html":{"url":"jvm/5.内存分配与回收策略.html","title":"5.内存分配与回收策略.md","keywords":"","body":"1.对象优先在Eden分配2、大对象直接进入老年代3、长期存活的对象进入老年代4、动态对象年龄判定5、空间分配担保Java技术体系中所提倡的自动内存管理最终可以归结为自动化地解决了两个问题： 给对象分配内存 回收分配给对象的内存 对象的内存分配，从大体上讲，就是在堆内存中进行分配，对象主要是分配在新生代的Eden区上，如果启动了本地线程分配缓冲（TLAB），则将按线程优先在TLAB上分配。少数情况也会直接分配在老年代中，分配的规则不是百分之百固定的，细节取决于当前使用的是哪一种垃圾收集器组合，还有虚拟机中与内存相关的参数的设置。 一下是几条最普遍的内存分配规则。 1.对象优先在Eden分配 大多数情况下，对象在新生代的Eden区中分配。当Eden区中没有足够的内存空间进行分配时，虚拟机将发起一次MinorGC(新生代GC，分配率越高，MinorGC越频繁，但是回收速度一般也比较快）。 测试代码： private static final int_1MB = 1024 * 1024; /** * VM参数：-verbose:gc -Xms20M -Xmx20M -Xmn10M -xx:+PrintGCDetails -xx:SurvivorRatio=8 */ public static void testAllocataion(){ byte[] allocation,allocation2,allocation3,allocation4; allocation 1 = new byte[2 * _1MB]; allocation 2 = new byte[2 * _1MB]; allocation 3 = new byte[2 * _1MB]; allocation 4 = new byte[4 * _1MB]; //出现一处MinorGC } 实验参数设置： -Xms20M -Xmx20M -Xmn10M这三个参数限制了Java堆的大小为20M且不可扩展，其中10M分给新生代，10M分配给来年代 -xx:SurvivorRatio=8 确定了新生代中Eden区与Survivor区的空间比例是8:1（新生代中有两个Survivor区） 实验结果： testAllocation()方法执行，在分配allocation4对象时会触发一次MinorGC，原因是在给allocation4分配对象时候，发现Eden区域已经被占用了6MB，剩余空间无法容纳allocation4对象的4MB内存。GC期间虚拟机又发现已经分配的3个大小为2MB的对象全部无法放入Survivor空间中（因为Survivor空间只有1MB），所以只能通过分配担保机制将这三个对象提前转移到老年代中，而MinorGC后allocation4对象将分配到Eden区域。 GC结束后内存分配情况：Eden占用4MB(allocation4），Survivor空闲，老年代占用6MB（allocation1、2、3） 2、大对象直接进入老年代 所谓的大对象就是指，需要大量连续内存空间的Java对象，最典型的大对象就是那种很长的字符串以及数组（上面代码中的byte[]数组就是典型的大对象）。大对象对虚拟机的内存分配来说是一个坏消息（特别是“朝生夕灭”的“短命大对象”，写程序时应该避免），经常出现大对象容易导致内存还有不少空间时就要提前触发垃圾收集以获取足够的连续空间来安置它们。 虚拟机提供了一个-XX:PretenureSizeThreshold参数，目的是当分配大于这个值的对象时，直接在老年代中分配。这样做的目的是避免在Eden区以及两个Survivor区之间发生大量的内存复制。 3、长期存活的对象进入老年代 既然虚拟机采用了分代收集的思想来管理内存，那么内存回收时就必须能识别哪些对象应放在新生代，哪些对象应放在老年代。为了做到这点，虚拟机给每个对象定义了一个对象年龄计数器,保存在对象头种。 如果对象在Eden中出生并经过第一次MinorGC后任然可以存活，并且能被Survivor容纳的话，将被移动到Survivor空间中，并且设置对象年龄为1岁，对象在Survivor区中每熬过一次MinorGC，年龄就会增加一岁，当他年龄增加到一定程度时（默认为15岁），就会被晋升到老年代中。对象晋升到老年代的阀值可以通过参数-XX：MaxTenuringThreshold设置。 4、动态对象年龄判定 为了更好的适应不同程序的内存状况，虚拟机并不是永远的要求对象年龄必须达到了MaxTenuringThreshold才能晋升老年代。 如果在Survivor空间中相同年龄所有对象大小的总和大于Survivor空间大小的一半，年龄大于或等于该年龄的对象就会直接进入老年代。无需等到MaxTenuringThreshold中要求的年龄。 5、空间分配担保 在发生MinorGC之前，虚拟机会先检查老年代最大可用的连续空间是否大于新生代所有对象的总空间，如果这个条件成立，那么MinorGC可以确保是安全的。如果不成立，则虚拟机会查看HandlePromotionFailure设置值是否允许担保失败。如果允许，会继续检查老年代最大可用的连续空间是都大于历次晋升到老年代对象的平均大小，如果大于，会尝试进行一次MinorGC，尽管这次MinorGC是有风险的；如果小于，或者HandlePromotionFailure设置不允许冒险，那么这时也要改为进行一次Full GC。 新生代使用的是复制收集算法，为了保证内存空间的利用率，每次只能使用一块Survivor空间，因此当出现大量对象在MinorGC后依然存活，而Survivor空间不足够存放这些对象时，就需要老年代进行分配担保，把Survivor无法容纳的对象直接进入老年代。这与生活中的贷款担保相似，老年代要进行担保，那么就要确保老年代的内存空间大小足够去容纳这些对象。 有多少对象会活下来在实际完成前是无法明确得知的，所以只好取之前每一次晋升到老年代中的所有对象大小的平均值，来与老年代的剩余空间进行比较，从而决定是进行Full GC来让老年代腾出更多的空间。 取平均值并不是每一次都会成功，有可能这一次存活下来的对象所占内存空间远超这个平均值，那么依然会导致担保失败，担保失败以后会重新发起一次Full GC。虽然担保失败会绕很大一个圈子，但是为了避免Full GC太频繁，大部分情况下还是会把HandlePromotionFailure开关打开。 "},"jvm/6.类文件结构.html":{"url":"jvm/6.类文件结构.html","title":"6.类文件结构.md","keywords":"","body":"类文件结构JVM 的“无关性”Class 文件结构魔数版本信息常量池访问标志类索引、父类索引、接口索引集合字段表集合方法表集合属性表集合类文件结构 JVM 的“无关性” 谈论 JVM 的无关性，主要有以下两个： 平台无关性：任何操作系统都能运行 Java 代码 语言无关性： JVM 能运行除 Java 以外的其他代码 Java 源代码首先需要使用 Javac 编译器编译成 .class 文件，然后由 JVM 执行 .class 文件，从而程序开始运行。 JVM 只认识 .class 文件，它不关心是何种语言生成了 .class 文件，只要 .class 文件符合 JVM 的规范就能运行。 目前已经有 JRuby、Jython、Scala 等语言能够在 JVM 上运行。它们有各自的语法规则，不过它们的编译器 都能将各自的源码编译成符合 JVM 规范的 .class 文件，从而能够借助 JVM 运行它们。 Java 语言中的各种变量、关键字和运算符号的语义最终都是由多条字节码命令组合而成的， 因此字节码命令所能提供的语义描述能力肯定会比 Java 语言本身更加强大。 因此，有一些 Java 语言本身无法有效支持的语言特性，不代表字节码本身无法有效支持。 Class 文件结构 Class 文件是二进制文件，它的内容具有严格的规范，文件中没有任何空格，全都是连续的 0/1。Class 文件 中的所有内容被分为两种类型：无符号数、表。 无符号数 无符号数表示 Class 文件中的值，这些值没有任何类型，但有不同的长度。u1、u2、u4、u8 分别代表 1/2/4/8 字节的无符号数。 表 由多个无符号数或者其他表作为数据项构成的复合数据类型。 Class 文件具体由以下几个构成: 魔数 版本信息 常量池 访问标志 类索引、父类索引、接口索引集合 字段表集合 方法表集合 属性表集合 魔数 Class 文件的头 4 个字节称为魔数，用来表示这个 Class 文件的类型。 Class 文件的魔数是用 16 进制表示的“CAFE BABE”，是不是很具有浪漫色彩？ 魔数相当于文件后缀名，只不过后缀名容易被修改，不安全，因此在 Class 文件中标识文件类型比较合适。 版本信息 紧接着魔数的 4 个字节是版本信息，5-6 字节表示次版本号，7-8 字节表示主版本号，它们表示当前 Class 文件中使用的是哪个版本的 JDK。 高版本的 JDK 能向下兼容以前版本的 Class 文件，但不能运行以后版本的 Class 文件，即使文件格式并未发生任何变化，虚拟机也必需拒绝执行超过其版本号的 Class 文件。 常量池 常量池中主要存放两大类常量:字面量(Literal)和符号引用(Symbolic References)。 字面量比较接近于Java语言层面的常量概念，如文本字符串、被声明为final的常量值等。 而符号引用则属于编译原理方面的概念，主要包括下面几类常量: 被模块导出或者开放的包(Package) 类和接口的全限定名(Fully Qualified Name) 字段的名称和描述符(Descriptor) 方法的名称和描述符 方法句柄和方法类型(Method Handle、Method Type、Invoke Dynamic) 动态调用点和动态常量(Dynamically-Computed Call Site、Dynamically-Computed Constant) 常量池的特点 常量池中常量数量不固定，因此常量池开头放置一个 u2 类型的无符号数，用来存储当前常量池的容量。 常量池的每一项常量都是一个表，表开始的第一位是一个 u1 类型的标志位（tag），代表当前这个常量属于哪种常量类型。 常量池中常量类型 类型 tag 描述 CONSTANT_utf8_info 1 UTF-8 编码的字符串 CONSTANT_Integer_info 3 整型字面量 CONSTANT_Float_info 4 浮点型字面量 CONSTANT_Long_info 5 长整型字面量 CONSTANT_Double_info 6 双精度浮点型字面量 CONSTANT_Class_info 7 类或接口的符号引用 CONSTANT_String_info 8 字符串类型字面量 CONSTANT_Fieldref_info 9 字段的符号引用 CONSTANT_Methodref_info 10 类中方法的符号引用 CONSTANT_InterfaceMethodref_info 11 接口中方法的符号引用 CONSTANT_NameAndType_info 12 字段或方法的符号引用 CONSTANT_MethodHandle_info 15 表示方法句柄 CONSTANT_MethodType_info 16 标识方法类型 CONSTANT_InvokeDynamic_info 18 表示一个动态方法调用点 对于 CONSTANT_Class_info（此类型的常量代表一个类或者接口的符号引用），它的二维表结构如下： 类型 名称 数量 u1 tag 1 u2 name_index 1 tag 是标志位，用于区分常量类型；name_index 是一个索引值，它指向常量池中一个 CONSTANT_Utf8_info 类型常量，此常量代表这个类（或接口）的全限定名，这里 name_index 值若为 0x0002，也即是指向了常量池中的第二项常量。 CONSTANT_Utf8_info 型常量的结构如下： 类型 名称 数量 u1 tag 1 u2 length 1 u1 bytes length tag 是当前常量的类型；length 表示这个字符串的长度；bytes 是这个字符串的内容（采用缩略的 UTF8 编码） 访问标志 在常量池结束之后，紧接着的两个字节代表访问标志，这个标志用于识别一些类或者接口层次的访问信息，包括：这个 Class 是类还是接口；是否定义为 public 类型；是否被 abstract/final 修饰。 类索引、父类索引、接口索引集合 类索引和父类索引都是一个 u2 类型的数据，而接口索引集合是一组 u2 类型的数据的集合，Class 文件中由这三项数据来确定类的继承关系。类索引用于确定这个类的全限定名，父类索引用于确定这个类的父类的全限定名。 由于 Java 不允许多重继承，所以父类索引只有一个，除了 java.lang.Object 之外，所有的 Java 类都有父类，因此除了 java.lang.Object 外，所有 Java 类的父类索引都不为 0。一个类可能实现了多个接口，因此用接口索引集合来描述。这个集合第一项为 u2 类型的数据，表示索引表的容量，接下来就是接口的名字索引。 类索引和父类索引用两个 u2 类型的索引值表示，它们各自指向一个类型为 CONSTANT_Class_info 的类描述符常量，通过该常量总的索引值可以找到定义在 CONSTANT_Utf8_info 类型的常量中的全限定名字符串。 字段表集合 字段表集合存储本类涉及到的成员变量，包括实例变量和类变量，但不包括方法中的局部变量。 每一个字段表只表示一个成员变量，本类中的所有成员变量构成了字段表集合。字段表结构如下： 类型 名称 数量 说明 u2 access_flags 1 字段的访问标志，与类稍有不同 u2 name_index 1 字段名字的索引 u2 descriptor_index 1 描述符，用于描述字段的数据类型。 基本数据类型用大写字母表示； 对象类型用“L 对象类型的全限定名”表示。 u2 attributes_count 1 属性表集合的长度 u2 attributes attributes_count 属性表集合，用于存放属性的额外信息，如属性的值。 跟随access_flags标志的是两项索引值:name_index和descrip tor_index。它们都是对常量池项的引用，分别代表着字段的简单名称以及字段和方法的描述符。现在需要解释一下“简单名称”“描述符”以 及前面出现过多次的“全限定名”这三种特殊字符串的概念。 全 限 定 名 和 简 单 名 称 很 好 理 解 ， 以 代 码 清 单 6 - 1 中 的 代 码 为 例 ， “ o r g/ f e n i xs o f t / c l a z z / T e s t C l a s s ” 是 这个类的全限定名，仅仅是把类全名中的“ .”替换成了“ /”而已，为了使连续的多个全限定名之间不产生混 淆，在使用时最后一般会加入一个“;”号表示全限定名结束。简单名称则就是指没有类型和参数修饰 的方法或者字段名称，这个类中的inc()方法和m字段的简单名称分别就是“ inc”和“ m”。 相比于全限定名和简单名称，方法和字段的描述符就要复杂一些。描述符的作用是用来描述字段的数据类型、方法的参数列表(包括数量、类型以及顺序)和返回值。根据描述符规则，基本数据类 型(byte、char、double、float、int、long、short、boolean)以及代表无返回值的void类型都用一个大写字符来表示，而对象类型则用字符L加对象的全限定名来表示，详见表6-10。 对于数组类型，每一维度将使用一个前置的“ [”字符来描述，如一个定义为“ java.lang.St ring[][]”类型 的二维数组将被记录成“ [[Ljava/lang/String;”，一个整型数组“ int []”将被记录成“ [I”。 用描述符来描述方法时，按照先参数列表、后返回值的顺序描述`，参数列表按照参数的严格顺序 放在一组小括号“()”之内。`如方法void inc()的描述符为“()V”，方法java.lang.String toString()的描述符 为“()Ljava/lang/String;”，方法int indexOf(char[]source，int sourceOffset，int sourceCount，char[]target， int targetOffset，int targetCount，int fromIndex)的描述符为“([CII[CIII)I”。 字段表所包含的固定数据项目到descrip tor_index为止就全部结束了，不过在descrip -tor_index之后 跟随着一个属性表集合，用于存储一些额外的信息，字段表可以在属性表中附加描述零至多项的额外 信息。对于本例中的字段m，它的属性表计数器为0，也就是没有需要额外描述的信息，但是，如果将 字段m的声明改为“final static int m=123;”，那就可能会存在一项名称为ConstantValue的属性，其值指 向常量123。 字段表集合中不会列出从父类或者父接口中继承而来的字段，但有可能出现原本Java代码之中不存在的字段，譬如在内部类中为了保持对外部类的访问性，编译器就会自动添加指向外部类实例的字段。另外，在Java语言中字段是无法重载的，两个字段的数据类型、修饰符不管是否相同，都必须使 用不一样的名称，但是对于Class文件格式来讲，只要两个字段的描述符不是完全相同，那字段重名就 是合法的。 方法表集合 方法表结构与属性表类似。 volatile 关键字 和 transient 关键字不能修饰方法，所以方法表的访问标志中没有 ACC_VOLATILE 和 ACC_TRANSIENT 标志。 方法表的属性表集合中有一张 Code 属性表，用于存储当前方法经编译器编译后的字节码指令。 属性表集合 每个属性对应一张属性表，属性表的结构如下： 类型 名称 数量 u2 attribute_name_index 1 u4 attribute_length 1 u1 info attribute_length "},"jvm/7.类加载机制.html":{"url":"jvm/7.类加载机制.html","title":"7.类加载机制.md","keywords":"","body":"虚拟机类加载机制概述类的生命周期“初始化”的时机被动引用演示 Demo接口的加载过程类加载的过程类加载器类与类加载器双亲委派模型虚拟机类加载机制 概述 Java虚拟机把描述类的数据从Class文件加载到内存，并对数据进行校验、转换解析和初始化，最 终形成可以被虚拟机直接使用的Java类型，这个过程被称作虚拟机的类加载机制。 与那些在编译时需 要进行连接的语言不同，在Java语言里面，类型的加载、连接和初始化过程都是在程序运行期间完成 的，这种策略让Java语言进行提前编译会面临额外的困难，也会让类加载时稍微增加一些性能开销， 但是却为Java应用提供了极高的扩展性和灵活性，Java天生可以动态扩展的语言特性就是依赖运行期动 态加载和动态连接这个特点实现的。例如，编写一个面向接口的应用程序，可以等到运行时再指定其 实际的实现类，用户可以通过Java预置的或自定义类加载器，让某个本地的应用程序在运行时从网络 或其他地方上加载一个二进制流作为其程序代码的一部分。这种动态组装应用的方式目前已广泛应用 于Java程序之中，从最基础的Applet、JSP到相对复杂的OSGi技术，都依赖着Java语言运行期类加载才 得以诞生。 类的生命周期 类从被加载到虚拟机内存开始，到卸载出内存为止，它的整个生命周期包括以下 7 个阶段： 加载 验证 准备 解析 初始化 使用 卸载 验证、准备、解析 3 个阶段统称为连接。 加载、验证、准备、初始化和卸载这 5 个阶段的顺序是确定的，类的加载过程必须按照这种顺序按部就班地开始（注意是“开始”，而不是“进行”或“完成”），而解析阶段则不一定：它在某些情况下可以在初始化后再开始，这是为了支持 Java 语言的运行时绑定。 “初始化”的时机 Java 虚拟机规范没有强制约束类加载过程的第一阶段（即：加载）什么时候开始，但对于“初始化”阶段，有着严格的规定。有且仅有 5 种情况必须立即对类进行“初始化”： 在遇到 new、putstatic、getstatic、invokestatic 字节码指令时，如果类尚未初始化，则需要先触发其初始化。 对类进行反射调用时，如果类还没有初始化，则需要先触发其初始化。 初始化一个类时，如果其父类还没有初始化，则需要先初始化父类。 虚拟机启动时，用于需要指定一个包含 main() 方法的主类，虚拟机会先初始化这个主类。 当使用 JDK 1.7 的动态语言支持时，如果一个 java.lang.invoke.MethodHandle 实例最后的解析结果为 REF_getStatic、REF_putStatic、REF_invokeStatic 的方法句柄，并且这个方法句柄所对应的类还没初始化，则需要先触发其初始化。 这 5 种场景中的行为称为对一个类进行主动引用，除此之外，其它所有引用类的方式都不会触发初始化，称为被动引用。 被动引用演示 Demo Demo1 /** * 被动引用 Demo1: * 通过子类引用父类的静态字段，不会导致子类初始化。 * * */ class SuperClass { static { System.out.println(\"SuperClass init!\"); } public static int value = 123; } class SubClass extends SuperClass { static { System.out.println(\"SubClass init!\"); } } public class NotInitialization { public static void main(String[] args) { System.out.println(SubClass.value); // SuperClass init! } } 对于静态字段，只有直接定义这个字段的类才会被初始化，因此通过其子类来引用父类中定义的静态字段，只会触发父类的初始化而不会触发子类的初始化。 Demo2 /** * 被动引用 Demo2: * 通过数组定义来引用类，不会触发此类的初始化。 * * */ public class NotInitialization { public static void main(String[] args) { SuperClass[] superClasses = new SuperClass[10]; } } 这段代码不会触发父类的初始化，但会触发“[L 全类名”这个类的初始化，它由虚拟机自动生成，直接继承自 java.lang.Object，创建动作由字节码指令 newarray 触发。 Demo3 /** * 被动引用 Demo3: * 常量在编译阶段会存入调用类的常量池中，本质上并没有直接引用到定义常量的类，因此不会触发定义常量的类的初始化。 * * */ class ConstClass { static { System.out.println(\"ConstClass init!\"); } public static final String HELLO_BINGO = \"Hello Bingo\"; } public class NotInitialization { public static void main(String[] args) { System.out.println(ConstClass.HELLO_BINGO); } } 编译通过之后，常量存储到 NotInitialization 类的常量池中，NotInitialization 的 Class 文件中并没有 ConstClass 类的符号引用入口，这两个类在编译成 Class 之后就没有任何联系了。 接口的加载过程 接口加载过程与类加载过程稍有不同。 当一个类在初始化时，要求其父类全部都已经初始化过了，但是一个接口在初始化时，并不要求其父接口全部都完成了初始化，当真正用到父接口的时候才会初始化。 类加载的过程 加载 加载的过程 “加载”是“类加载”过程的一个阶段，不能混淆这两个名词。在加载阶段，虚拟机需要完成 3 件事： 通过类的全限定名获取该类的二进制字节流。 将二进制字节流所代表的静态结构转化为方法区的运行时数据结构。 在内存中创建一个代表该类的 java.lang.Class 对象，作为方法区这个类的各种数据的访问入口。 获取二进制字节流 对于 Class 文件，虚拟机没有指明要从哪里获取、怎样获取。除了直接从编译好的 .class 文件中读取，还有以下几种方式： 从 zip 包中读取，如 jar、war 等 从网络中获取，如 Applet 通过动态代理技术生成代理类的二进制字节流 由 JSP 文件生成对应的 Class 类 从数据库中读取，如 有些中间件服务器可以选择把程序安装到数据库中来完成程序代码在集群间的分发。 “非数组类”与“数组类”加载比较 非数组类加载阶段可以使用系统提供的引导类加载器，也可以由用户自定义的类加载器完成，开发人员可以通过定义自己的类加载器控制字节流的获取方式（如重写一个类加载器的 loadClass() 方法） 数组类本身不通过类加载器创建，它是由 Java 虚拟机直接创建的，再由类加载器创建数组中的元素类。 注意事项 虚拟机规范未规定 Class 对象的存储位置，对于 HotSpot 虚拟机而言，Class 对象比较特殊，它虽然是对象，但存放在方法区中。 加载阶段与连接阶段的部分内容交叉进行，加载阶段尚未完成，连接阶段可能已经开始了。但这两个阶段的开始时间仍然保持着固定的先后顺序。 验证 文件格式验证 验证字节流是否符合 Class 文件格式的规范，并且能被当前版本的虚拟机处理，验证点如下： 是否以魔数 0XCAFEBABE 开头 主次版本号是否在当前虚拟机处理范围内 常量池是否有不被支持的常量类型 指向常量的索引值是否指向了不存在的常量 CONSTANT_Utf8_info 型的常量是否有不符合 UTF8 编码的数据 ...... 元数据验证 对字节码描述信息进行语义分析，确保其符合 Java 语法规范。 字节码验证 本阶段是验证过程中最复杂的一个阶段，是对方法体进行语义分析，保证方法在运行时不会出现危害虚拟机的事件。 符号引用验证 本阶段发生在解析阶段，确保解析正常执行。 准备 准备阶段是正式为类变量（或称“静态成员变量”）分配内存并设置初始值的阶段。这些变量（不包括实例变量）所使用的内存都在方法区中进行分配。 初始值“通常情况下”是数据类型的零值（0, null...），假设一个类变量的定义为： public static int value = 123; 那么变量 value 在准备阶段过后的初始值为 0 而不是 123，因为这时候尚未开始执行任何 Java 方法。 存在“特殊情况”：如果类字段的字段属性表中存在 ConstantValue 属性，那么在准备阶段 value 就会被初始化为 ConstantValue 属性所指定的值，假设上面类变量 value 的定义变为： public static final int value = 123; 那么在准备阶段虚拟机会根据 ConstantValue 的设置将 value 赋值为 123。 解析 解析阶段是Java虚拟机将常量池内的符号引用替换为直接引用的过程，符号引用在讲解Class 文件格式的时候已经出现过多次，在Class文件中它以CONSTANT_Class_info、 CONSTANT_Fieldref_info、CONSTANT_M ethodref_info等类型的常量出现，那解析阶段中所说的直接引用与符号引用又有什么关联呢? 符号引用(Symbolic References):符号引用以一组符号来描述所引用的目标，符号可以是任何 形式的字面量，只要使用时能无歧义地定位到目标即可。符号引用与虚拟机实现的内存布局无关，引 用的目标并不一定是已经加载到虚拟机内存当中的内容。各种虚拟机实现的内存布局可以各不相同， 但是它们能接受的符号引用必须都是一致的，因为符号引用的字面量形式明确定义在《Java虚拟机规 范》的Class文件格式中。 直接引用(Direct References):直接引用是可以直接指向目标的指针、相对偏移量或者是一个能 间接定位到目标的句柄。直接引用是和虚拟机实现的内存布局直接相关的，同一个符号引用在不同虚 拟机实例上翻译出来的直接引用一般不会相同。如果有了直接引用，那引用的目标必定已经在虚拟机 的内存中存在。 对同一个符号引用进行多次解析请求是很常见的事情，除invokedynamic指令以外，虚拟机实现可以对第一次解析的结果进行缓存`，譬如在运行时直接引用常量池中的记录，并把常量标识为已解析状 态，从而避免解析动作重复进行。无论是否真正执行了多次解析动作，Java虚拟机都需要保证的是`在同一个实体中，如果一个符号引用之前已经被成功解析过，那么后续的引用解析请求就应当一直能够成功`;同样地，`如果第一次解析失败了，其他指令对这个符号的解析请求也应该收到相同的异常，哪 怕这个请求的符号在后来已成功加载进Java虚拟机内存之中。 不过对于invokedy namic指令，上面的规则就不成立了。当碰到某个前面已经由invokedynamic指令触发过解析的符号引用时，并不意味着这个解析结果对于其他invokedynamic指令也同样生效。因为 invokedynamic指令的目的本来就是用于动态语言支持，它对应的引用称为“动态调用点限定符 (Dynamically-Computed Call Site Specifier)”，这里“动态”的含义是指必须等到程序实际运行到这条指令时，解析动作才能进行。相对地，其余可触发解析的指令都是“静态”的，可以在刚刚完成加载阶 段，还没有开始执行代码时就提前进行解析。 解析动作主要针对类或接口、字段、类方法、接口方法、方法类型、方法句柄和调用点限定符这7 类符号引用进行，分别对应于常量池的CONSTANT_Class_info、CON-STANT_Fieldref_info、 CONSTANT_Methodref_info、CONSTANT_InterfaceM ethodref_info、 CONSTANT_MethodType_info、CONSTANT_MethodHandle_info、CONSTANT_Dyna-mic_info和 CONSTANT_InvokeDynamic_info 8种常量类型。 下面笔者将讲解前4种引用的解析过程，对于后4 种，它们都和动态语言支持密切相关，由于Java语言本身是一门静态类型语言，在没有讲解清楚 invokedy namic指令的语意之前，我们很难将它们直观地和现在的Java语言语法对应上，因此笔者将延 后到第8章介绍动态语言调用时一起分析讲解。 初始化 类的初始化阶段是类加载过程的最后一个步骤，之前介绍的几个类加载的动作里，除了在加载阶 段用户应用程序可以通过自定义类加载器的方式局部参与外，其余动作都完全由Java虚拟机来主导控 制。直到初始化阶段，Java虚拟机才真正开始执行类中编写的Java程序代码，将主导权移交给应用程序。 进行准备阶段时，变量已经赋过一次系统要求的初始零值，而在初始化阶段，则会根据程序员通过程序编码制定的主观计划去初始化类变量和其他资源。我们也可以从另外一种更直接的形式来表达:初始化阶段就是执行类构造器()方法的过程。()并不是程序员在Java代码中直接编写的方法，它是Javac编译器的自动生成物，但我们非常有必要了解这个方法具体是如何产生的，以及 、()方法执行过程中各种可能会影响程序运行行为的细节，这部分比起其他类加载过程更贴近于普通的程序开发人员的实际工作。 ()方法是由编译器自动收集类中的所有类变量的赋值动作和静态语句块(static{}块)中的语句合并产生的，编译器收集的顺序是由语句在源文件中出现的顺序决定的，静态语句块中只能访问到定义在静态语句块之前的变量，定义在它之后的变量，在前面的静态语句块可以赋值，但是不能访问，如代码清单7-5所示。 ()方法与类的构造函数(即在虚拟机视角中的实例构造器()方法)不同，它不需要显 式地调用父类构造器，Java虚拟机会保证在子类的()方法执行前，父类的()方法已经执行 完 毕 。 因 此 在 J a v a 虚 拟 机 中 第 一 个 被 执 行 的 ( ) 方 法 的 类 型 肯 定 是 j a v a . l a n g. O b j e c t 。 由于父类的()方法先执行，也就意味着父类中定义的静态语句块要优先于子类的变量赋值 操作，如代码清单7-6中，字段B的值将会是2而不是1。 ()方法对于类或接口来说并不是必需的，如果一个类中没有静态语句块，也没有对变量的 赋值操作，那么编译器可以不为这个类生成()方法。 接口中不能使用静态语句块，但仍然有变量初始化的赋值操作，因此接口与类一样都会生成 ()方法。但接口与类不同的是，执行接口的()方法不需要先执行父接口的()方法， 因为只有当父接口中定义的变量被使用时，父接口才会被初始化。此外，接口的实现类在初始化时也一样不会执行接口的()方法。 Java虚拟机必须保证一个类的()方法在多线程环境中被正确地加锁同步，如果多个线程同 时去初始化一个类，那么只会有其中一个线程去执行这个类的()方法，其他线程都需要阻塞等 待，直到活动线程执行完毕()方法。如果在一个类的()方法中有耗时很长的操作，那就可能造成多个进程阻塞，在实际应用中这种阻塞往往是很隐蔽的。代码清单7-7演示了这种场景。 类加载器 类与类加载器 判断类是否“相等” 任意一个类，都由加载它的类加载器和这个类本身一同确立其在 Java 虚拟机中的唯一性，每一个类加载器，都有一个独立的类名称空间。 因此，比较两个类是否“相等”，只有在这两个类是由同一个类加载器加载的前提下才有意义，否则，即使这两个类来源于同一个 Class 文件，被同一个虚拟机加载，只要加载它们的类加载器不同，那么这两个类就必定不相等。 这里的“相等”，包括代表类的 Class 对象的 equals() 方法、isInstance() 方法的返回结果，也包括使用 instanceof 关键字做对象所属关系判定等情况。 加载器种类 系统提供了 3 种类加载器： 启动类加载器（Bootstrap ClassLoader）： 负责将存放在 \\lib 目录中的，并且能被虚拟机识别的（仅按照文件名识别，如 rt.jar，名字不符合的类库即使放在 lib 目录中也不会被加载）类库加载到虚拟机内存中。 扩展类加载器（Extension ClassLoader）： 负责加载 \\lib\\ext 目录中的所有类库，开发者可以直接使用扩展类加载器。 应用程序类加载器（Application ClassLoader）： 由于这个类加载器是 ClassLoader 中的 getSystemClassLoader() 方法的返回值，所以一般也称它为“系统类加载器”。它负责加载用户类路径（classpath）上所指定的类库，开发者可以直接使用这个类加载器，如果应用程序中没有自定义过自己的类加载器，一般情况下这个就是程序中默认的类加载器。 当然，如果有必要，还可以加入自己定义的类加载器。 双亲委派模型 什么是双亲委派模型 双亲委派模型是描述类加载器之间的层次关系。它要求除了顶层的启动类加载器外，其余的类加载器都应当有自己的父类加载器。（父子关系一般不会以继承的关系实现，而是以组合关系来复用父加载器的代码） 工作过程 如果一个类加载器收到了类加载的请求，它首先不会自己去尝试加载这个类，而是把这个请求委派给父类加载器去完成，每一个层次的类加载器都是如此，因此所有的加载请求最终都应该传送到顶层的启动类加载器中，只有当父加载器反馈自己无法完成这个加载请求（找不到所需的类）时，子加载器才会尝试自己去加载。 在 java.lang.ClassLoader 中的 loadClass() 方法中实现该过程。 为什么使用双亲委派模型 像 java.lang.Object 这些存放在 rt.jar 中的类，无论使用哪个类加载器加载，最终都会委派给最顶端的启动类加载器加载，从而使得不同加载器加载的 Object 类都是同一个。 相反，如果没有使用双亲委派模型，由各个类加载器自行去加载的话，如果用户自己编写了一个称为 java.lang.Object 的类，并放在 classpath 下，那么系统将会出现多个不同的 Object 类，Java 类型 "},"jvm/8.jvm逃逸分析.html":{"url":"jvm/8.jvm逃逸分析.html","title":"8.逃逸分析技术","keywords":"","body":"逃逸分析同步省略标量替换栈上分配逃逸分析并不成熟在Java的编译体系中，一个Java的源代码文件变成计算机可执行的机器指令的过程中，需要经过两段编译，第一段是把.java文件转换成.class文件。第二段编译是把.class转换成机器指令的过程。 第一段编译就是javac命令。 在第二编译阶段，JVM 通过解释字节码将其翻译成对应的机器指令，逐条读入，逐条解释翻译。很显然，经过解释执行，其执行速度必然会比可执行的二进制字节码程序慢很多。这就是传统的JVM的解释器（Interpreter）的功能。为了解决这种效率问题，引入了 JIT（即时编译） 技术。 引入了 JIT 技术后，Java程序还是通过解释器进行解释执行，当JVM发现某个方法或代码块运行特别频繁的时候，就会认为这是“热点代码”（Hot Spot Code)。然后JIT会把部分“热点代码”翻译成本地机器相关的机器码，并进行优化，然后再把翻译后的机器码缓存起来，以备下次使用。 JIT优化中最重要的一个就是逃逸分析。 逃逸分析 逃逸分析的基本行为就是分析对象动态作用域：当一个对象在方法中被定义后，它可能被外部方法所引用，例如作为调用参数传递到其他地方中，称为方法逃逸。 例如以下代码： public static StringBuffer craeteStringBuffer(String s1, String s2) { StringBuffer sb = new StringBuffer(); sb.append(s1); sb.append(s2); return sb; } public static String createStringBuffer(String s1, String s2) { StringBuffer sb = new StringBuffer(); sb.append(s1); sb.append(s2); return sb.toString(); } 第一段代码中的sb就逃逸了，而第二段代码中的sb就没有逃逸。 使用逃逸分析，编译器可以对代码做如下优化： 一、同步省略。如果一个对象被发现只能从一个线程被访问到，那么对于这个对象的操作可以不考虑同步。 二、将堆分配转化为栈分配。如果一个对象在子程序中被分配，要使指向该对象的指针永远不会逃逸，对象可能是栈分配的候选，而不是堆分配。 三、分离对象或标量替换。有的对象可能不需要作为一个连续的内存结构存在也可以被访问到，那么对象的部分（或全部）可以不存储在内存，而是存储在CPU寄存器中。 在Java代码运行时，通过JVM参数可指定是否开启逃逸分析， -XX:+DoEscapeAnalysis ： 表示开启逃逸分析 -XX:-DoEscapeAnalysis ： 表示关闭逃逸分析 从jdk 1.7开始已经默认开始逃逸分析，如需关闭，需要指定-XX:-DoEscapeAnalysis 同步省略 在动态编译同步块的时候，JIT编译器可以借助逃逸分析来判断同步块所使用的锁对象是否只能够被一个线程访问而没有被发布到其他线程。 如果同步块所使用的锁对象通过这种分析被证实只能够被一个线程访问，那么JIT编译器在编译这个同步块的时候就会取消对这部分代码的同步。这个取消同步的过程就叫同步省略，也叫锁消除 如以下代码： public void f() { Object hollis = new Object(); synchronized(hollis) { System.out.println(hollis); } } 代码中对hollis这个对象进行加锁，但是hollis对象的生命周期只在f()方法中，并不会被其他线程所访问到，所以在JIT编译阶段就会被优化掉。优化成： public void f() { Object hollis = new Object(); System.out.println(hollis); } 所以，在使用synchronized的时候，如果JIT经过逃逸分析之后发现并无线程安全问题的话，就会做锁消除。 标量替换 标量（Scalar）是指一个无法再分解成更小的数据的数据。Java中的原始数据类型就是标量。相对的，那些还可以分解的数据叫做聚合量（Aggregate），Java中的对象就是聚合量，因为他可以分解成其他聚合量和标量。 在JIT阶段，如果经过逃逸分析，发现一个对象不会被外界访问的话，那么经过JIT优化，就会把这个对象拆解成若干个其中包含的若干个成员变量来代替。这个过程就是标量替换。 public static void main(String[] args) { alloc(); } private static void alloc() { Point point = new Point（1,2）; System.out.println(\"point.x=\"+point.x+\"; point.y=\"+point.y); } class Point{ private int x; private int y; } 以上代码中，point对象并没有逃逸出alloc方法，并且point对象是可以拆解成标量的。那么，JIT就会不会直接创建Point对象，而是直接使用两个标量int x ，int y来替代Point对象。 以上代码，经过标量替换后，就会变成： private static void alloc() { int x = 1; int y = 2; System.out.println(\"point.x=\"+x+\"; point.y=\"+y); } 可以看到，Point这个聚合量经过逃逸分析后，发现他并没有逃逸，就被替换成两个聚合量了。那么标量替换有什么好处呢？就是可以大大减少堆内存的占用。因为一旦不需要创建对象了，那么就不再需要分配堆内存了。 标量替换为栈上分配提供了很好的基础。 栈上分配 在Java虚拟机中，对象是在Java堆中分配内存的，这是一个普遍的常识。但是，有一种特殊情况，那就是如果经过逃逸分析后发现，一个对象并没有逃逸出方法的话，那么就可能被优化成栈上分配。这样就无需在堆上分配内存，也无须进行垃圾回收了。 这里，还是要简单说一下，其实在现有的虚拟机中，并没有真正的实现栈上分配 逃逸分析并不成熟 关于逃逸分析的论文在1999年就已经发表了，但直到JDK 1.6才有实现，而且这项技术到如今也并不是十分成熟的。 其根本原因就是无法保证逃逸分析的性能消耗一定能高于他的消耗。虽然经过逃逸分析可以做标量替换、栈上分配、和锁消除。但是逃逸分析自身也是需要进行一系列复杂的分析的，这其实也是一个相对耗时的过程。 一个极端的例子，就是经过逃逸分析之后，发现没有一个对象是不逃逸的。那这个逃逸分析的过程就白白浪费掉了。 "},"多线程/basic/0.多线程路线图.html":{"url":"多线程/basic/0.多线程路线图.html","title":"1.多线程学习路线图","keywords":"","body":" "},"多线程/basic/0.java内存模型与线程.html":{"url":"多线程/basic/0.java内存模型与线程.html","title":"2.java内存模型与线程","keywords":"","body":"前言1. 硬件的效率与缓存一致性2. Java内存模型2.1 主内存和工作内存2.2 内存间交互操作2.3 原子性、可见性和有序性2.3.1 原子性2.4.2 可见性2.4.3 有序性3. Java与线程3.1 线程的实现3.1.1 使用内核线程实现3.1.2 使用用户级线程实现3.1.3 使用用户线程加轻量级进程混合实现3.1.4 Java线程的实现3.2 Java线程调度3.2.1 状态转换1. 新建（New）2. 运行（Runable）3. 无限期等待（Waiting）4. 限期等待（Timed Waiting）5. 阻塞（Blocked）6. 结束（Terminated）前言 在看这篇文章之前,如果你还未学过操作系统,请先学习操作系统,再学习多线程,勿心急 同样,若想阅读后续多线程文章,请先理解这篇文章 大家可以通过这篇文章,对java内存模型和操作系统内存模型以及二者的线程概念进行一个比较,以便后续对 多线程进行更深刻的理解 1. 硬件的效率与缓存一致性 由于存储设备和处理器运算速度之间的存在巨大的差异，现在计算机系统在内存与处理器之间加入高速缓存来作为处理器与内存之间的缓冲。将处理器需要的数据复制到缓存中，让处理器可以快速的获取数据进行计算，计算结束后再从缓存同步带内存中去，这样处理器无需等待缓慢的内存读写。虽然它很好的解决了处理器与存储的速度矛盾，但是它也为计算机系统带来更高的复杂度以及一个新问题：缓存一致性。 在多处理器系统中，每个处理器都有自己的高速缓存，而它们又共享同一主内存，当多个处理器的运算任务都涉及同一块主内存区域时，而它们各自的缓存数据又不一致，那么同步回主内存时以谁的缓存数据为主呢？ 为了解决一致性的问题，那怎么才能解决这个问题呢？ 为了解决缓存一致性的问题，我们的操作系统提出了总线锁定机制以及缓存一致性原则。 总线锁定：当CPU要对一个操作数进行操作的时候，其在总线上发出一个LOCK#信号，其他处理器就不能操作缓存了该共享变量内存地址的缓存，也就是阻塞了其他CPU，使该处理器可以独享此共享内存。 缓存一致性：当某块CPU对缓存中的数据进行操作了之后，就通知其他CPU放弃储存在它们内部的缓存，或者从主内存中重新读取 上图说明了处理器，高速缓存，主内存之间的交互关系。Java虚拟机规范中试图定义一种Java内存模型（JMM）可以用来屏蔽掉各种硬件和操作系统的内存访问差异，以实现让Java程序在各种平台下都能达到一致的内存存储效果。 2. Java内存模型 java虚拟机试图定义一组java内存模型规范，来屏蔽掉各种硬件和操作系统的内存访问差异，以实现让java程序在各种平台下都能到达一致的内存访问结果。java内存模型的主要目标是定义程序的各个变量的访问规则，即在虚拟机中将变量存储到内存和从内存中取出来这样的细节。java内存模型规定了所有变量存储于主内存，每条线程还有自己的工作内存，存储该线程使用的变量的拷贝副本，线程对所有变量的使用都必须在工作内存中进行，不能直接访问主内存。不同线程不能相互访问工作内存，线程间的变量值传递均通过主内存完成。线程，主内存，工作内存之间的关系如下图： 2.1 主内存和工作内存 我们已经了解了Java的内存模型是什么以及它有什么用，现在就来谈一谈主内存与工作内存： 主内存：Java内存模型规定了所有变量都存储在主内存中，注意，这里说的变量与平常Java编程中说的变量有所区别，它包括了 ，它不包括局部变量与方法参数，因为后者是线程私有的。也就是说，我们可以这样理解，除过线程私有的局部变量和方法参数之外，所有的变量都存在于主内存中。 工作内存：内存可以和计算机中的物理内存进行类比，而工作内存可与高速缓存类比。工作内存是 JMM 的一个抽象概念，并不真实存在。它涵盖了缓存，写缓冲区，寄存器以及其它的硬件和编译器优化。 在我们了解了主内存和工作内存的概念之后，那我们来了解一下什么是Cache缓存和Buffer缓冲区? buffer(缓冲区)：将数据缓冲下来，解决速度慢和快的交接问题；速度快的需要通过缓冲区将数据一点一点传给速度慢的区域。例如：从内存中将数据往硬盘中写入，并不是直接写入，而是缓冲到一定大小之后刷入硬盘中。 cache(缓存)：实现数据的重复使用，速度慢的设备需要通过缓存将经常要用到的数据缓存起来，缓存下来的数据可以提供高速的传输速度给速度快的设备。例如：将硬盘中的数据读取出来放在内存的缓存区中，这样以后再次访问同一个资源，速度会快很多。 每个线程都有一个自己的工作内存，该内存空间保存了被该线程使用到的变量的主内存副本，线程对变量的所有操作（读取，赋值等）都必须在工作内存中进行，而不直接读写主内存中的变量。看了这段话也许你会问，那假如线程访问一个10MB的对象，难道也会把这10MB的内存复制一份拷贝出来？这当然是不可能的，它有可能会将对象的引用，对象中某个线程访问到的字段拷贝出来，但绝不会将整个对象拷贝一次。 这个时候你可能会有一个问题：那就是JMM和Java虚拟机运行时的数据区到底有什么区别。 这里所讲的主内存，工作内存与Java内存区域中的Java堆，栈，方法区等并不是同一个层次的划分，这两者基本上是没有关系的。如果两者一定要勉强对应起来，那么变量，主内存，工作内存依次对应Java堆中对象实例数据部分，工作内存对应虚拟机栈中的部分区域。从更低层次上来说，主内存直接对应于物理硬件的内存，工作内存优先存储于寄存器以及高速缓存。 2.2 内存间交互操作 现在我们再来详细讨论一下一个变量是怎么从主内存拷贝到工作内存的，而工作内存的变量又是怎么同步回主内存的呢？ 上图说明了工作内存和主内存之间交互的步骤，还有图上缺少的两种原子性操作分别是lock锁定，unlock解锁。由于这两个操作和内存之间的交互并没有关系，所以分开来说。Java定义了8中操作，虚拟机实现时必须保证下面提到的每一种操作都是原子的、不可再分的。 lock(锁定)：作用于主内存的变量，它把一个变量标识为一条线程独占的状态 unlock(解锁)：作用于主内存的变量，它把一个处于锁定状态的变量释放出来，释放后的变量才可以被其他线程锁定。 read（读取）：作用于主内存变量，把变量的值从主内存传输到线程的工作内存 load（载入）：作用于主内存变量，把read操作从主内存中得到的变量值放入工作内存的变量副本中 use（使用）：作用于工作内存变量 assign（赋值）：作用于工作内存变量 store（存储）：作用于工作内存变量，将工作内存中一个变量的值传送回主内存 write（写入）：作用于主内存变量，将工作内存中得到的变量值放入主内存的变量中。 下面我们来看看对于这8种原子操作的使用： read与load：从主存复制变量到当前线程工作内存 use与assign：执行代码，改变共享变量值 store与write：用工作内存数据刷新主存对应变量的值 2.3 原子性、可见性和有序性 内存模型是围绕并发过程中如何处理原子性、可见性、有序性3个特征来建立的。 2.3.1 原子性 在Java中，对基本数据类型的访问读写都是原子性的（long、double除外）。 Int y = 1是一个原子操作，x++不是原子操作（3个操作：先读取x，x加1，加1后的新值写入x）。Java内存模型提供了lock和unlock这两个操作来满足原子操作需求。在字节码层次是使用monitorenter和monitorexit指令隐式使用这两个操作，在Java代码层次就是同步块synchronize。所以synchronize块之间的操作具有原子性。 2.4.2 可见性 可见性是指一个线程修改了共享变量的值，其他线程可以立即得到这个修改。Java中synchronize和final关键字也可以实现可见性。 2.4.3 有序性 可以通过volatile关键字来保证一定的“有序性”。另外可以通过synchronized和Lock来保证有序性，很显然，synchronized和Lock保证每个时刻是有一个线程执行同步代码，相当于是让线程顺序执行同步代码，自然就保证了有序性。 Java语言提供了volatile和synchronized两个关键字来保证线程之间操作的有序性，volatile关键字本身就包含了禁止指令重排序的语义，而synchronized则是由“一个变量在同一时刻只允许一条线程对其进行lock操作”这条规则获得的，这个规则决定了持有同一个锁的两个同步块只能串行地进入。 3. Java与线程 3.1 线程的实现 实现线程主要三种方式： 使用内核线程实现。 使用用户线程实现。 使用用户线程加轻量级进程实现。 3.1.1 使用内核线程实现 内核线程（Kernel Thread， KLT）就是直接由操作系统内核（Kernel，下称内核）支持的线程，这种线程由内核来完成线程切换，内核通过操纵调度器（Scheduler）对线程进行调度，并负责将线程的任务映射到各个处理器上。每个内核线程都可以看作是内核的一个分身，这样操作系统就有能力同时处理多件事情，支持多线程的内核就叫多线程内核（Multi-Thread Kernel）。 程序一般不会直接去使用内核线程，而是去使用内核线程的一种高级接口——轻量级进程（Light Weight Process， LWP），轻量级进程就是我们通常意义上所讲的线程，由于每个轻量级进程都由一个内核线程支持，因此只有先支持内核线程，才能有轻量级进程。这种轻量级进程与内核线程之间1：1的关系称为一对一的线程模型。 轻量级进程的局限性：由于是基于内核线程实现的，所以各种进程操作，如创建/析构及同步，都需要进行系统调用。而系统调用的代价相对较高，需要在用户态（User Mode）和内核态（Kernel Mode）中来回切换；每个轻量级进程都需要有一个内核线程的支持，因此轻量级进程需要消耗一定的内核资源（如内核线程的栈空间），因此一个系统支持轻量级进程是有限的。 3.1.2 使用用户级线程实现 狭义上的用户线程指的是完全建立在用户空间的线程库上，系统内核不能感知到线程存在的实现。用户线程的建立/同步/销毁和调度完全在用户态完成，不需要内核的帮助。如果程序实现得当， 优点：这种线程不需要切换到内核态，因此操作快速且低消耗，也可以支持规模更大的线程数量，部分高性能数据库中的多线程就是由用户线程实现的。这种进程与用户线程之间1：N的关系称为一对多的线程模型。 缺陷：不需要系统内核支援，缺陷也在于没有系统内核的支援，所有的线程操作都需要用户程序自己处理。线程的创建、切换、调度都是需要考虑的问题。 3.1.3 使用用户线程加轻量级进程混合实现 既存在用户线程，也存在轻量级进程。 用户线程还是完全建立在用户空间中，因此用户线程的创建、切换、析构等操作依然廉价，并且可以支持大规模的用户线程并发 操作系统提供的轻量级进程则作为用户线程和内核线程之间的桥梁，这样可以使用内核提供的线程调度功能以及处理器映射，并且用户线程的系统调用都要通过轻量级线程来完成，大大降低了整个进程被完全阻塞的风险。 3.1.4 Java线程的实现 对于JDK来说，它使用的都是一对一的线程模型实现的。一条Java线程就映射到一条轻量级线程之中。 3.2 Java线程调度 线程调度是指系统为线程分配处理器使用权的过程。主要调度方式两种： 使用协同调度的多线程系统，线程执行时间由线程本身控制，线程把自己的工作执行完后，要主动通知系统切换到另外一个线程上去。优点：实现简单。缺点：执行时间不可控制。 使用抢占调用的多线程系统，每个线程由系统分配执行时间，线程的切换不由线程本身决定。Java使用的就是这种线程调度方式。 Java提供10个级别的线程优先级设置，不过，线程优先级不靠谱，因为Java线程是被映射到系统的原生线程上实现的，所以线程调度最终还是由操作系统决定。 3.2.1 状态转换 Java语言定义了5种进程状态，在任意一个时间点，一个线程只能有且只有其中一种状态： 1. 新建（New） 新建（New）：创建尚未启动的线程处于这种状态。 2. 运行（Runable） 运行（Runable）：包括操作系统线程状态中的Running和Ready，处于此状态的线程可能正在运行，也可能等待着CPU为它分配执行时间。 3. 无限期等待（Waiting） 无限期等待（Waiting）：处于这种状态的线程不会被分配CPU执行时间，它们要等待其他线程显示地唤醒。以下方法会让线程陷入无限期的等待状态： 没有设置Timeout参数的Object.wait()方法。 没有设置Timeout参数的Thread.join()方法。 LockSupport.park()方法。 4. 限期等待（Timed Waiting） 限期等待（Timed Waiting）：处于这种状态的线程也不会被分配CPU执行时间，不过无须等待被其他线程显示地唤醒，在一定时间后由系统自动唤醒。以下方法会让线程陷入限期的等待状态： Thread.sleep()方法。 设置了Timeout参数的Object.wait()方法。 设置了Timeout参数的Thread.join()方法。 LockSupport.parkNanos()方法。 LockSupport.parkUntil()方法。 5. 阻塞（Blocked） 阻塞（Blocked）：线程被阻塞了，“阻塞状态”与“等待状态”的区别是：“阻塞状态”在等待获取一个排它锁，这个事件将在另外一个线程放弃这个锁的时候发生；“等待状态”则是在等待一段时间，或者唤醒动作的发生。在程序进入等待进入同步块区域的时候，线程将进入这种状态。 6. 结束（Terminated） 结束（Terminated）：已终止线程的线程状态，线程已经结束执行。 "},"多线程/3.并行程序基础.html":{"url":"多线程/3.并行程序基础.html","title":"2.并行程序基础","keywords":"","body":"二、Java并行程序基础1新建线程：2中断进程3wait()和notify()4挂起（suspend）和继续执行（resume）5等待线程结束（join）和谦让（yield）6 volatile关键字7 synchronized关键字8 并发下的ArrayList二、Java并行程序基础 1新建线程： Thread thread = new Thread(); thread.start(); 调用start()方法后自动调用run()方法，但线程新建时不能直接调用run()方法。 public class Client { public static void main(String[] args) { //方法一 Thread Thread thread = new Thread(){ @Override public void run(){ System.out.println(\"run()...\"); } }; thread.start(); //方法二 Runnable Thread tread1=new Thread(()->{ System.out.println(\"run1()...\"); }) } } 2.2注意：不要随便使用stop()结束一个进程，stop()方法会直接终止进程，并释放这个进程所持有的锁，容易导致不同的线程读错数据。 结束进程的方法： public class Client extends Thread{ volatile boolean stopme = false; public void Stopme(){ stopme = true; } @Override public void run(){ while (true){ if (stopme){ System.out.println(\"exit by stopme\"); break; } } } } 2中断进程 public void Thread.interrupt(); //中断进程，设置中断标志，但不会强制中断 public boolean Thread.isInterrupted(); //检查中断标志判断是否中断 public static boolean Thread.interrupt(); //判断是否被中断，并清除当前中断状态 3wait()和notify() public class Client { final static Object object = new Object(); // 等待 public static class Thread1 extends Thread{ @Override public void run(){ synchronized (object){ System.out.println(System.currentTimeMillis()+\":T1 start!\"); try { System.out.println(System.currentTimeMillis()+\":T1 wait for Object!\"); object.wait(); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(System.currentTimeMillis()+\":T1 end!\"); } } } // 唤醒等待线程 public static class Thread2 extends Thread{ @Override public void run(){ synchronized (object){ System.out.println(System.currentTimeMillis()+\":T2 start! notify one thread\"); object.notify(); System.out.println(System.currentTimeMillis()+\":T2 end!\"); try { Thread.sleep(2000); } catch (InterruptedException e) { } } } } public static void main(String[] args) { Thread1 thread1 = new Thread1(); Thread2 thread2 = new Thread2(); thread1.start(); thread2.start(); } } 4挂起（suspend）和继续执行（resume） 不推荐使用这两个方法。 5等待线程结束（join）和谦让（yield） public class Client { public volatile static int i = 0; public static class AddThread extends Thread { @Override public void run() { for (i = 0; i 注释join()方法后，i返回为0。 yield()会使当前线程让出cpu，重新参与cpu资源的争夺。 6 volatile关键字 用volatile申明一个变量后，这个变量被修改后会通知其他的线程。 7 synchronized关键字 实现线程间的同步，对同步的代码加锁，使得每一次只能有一个线程进入同步块，从而保证线程间的安全性。 简单介绍 synchronized是Java中的关键字，是一种同步锁。它修饰的对象有以下几种： 　　1. 修饰一个代码块，被修饰的代码块称为同步语句块，其作用的范围是大括号{}括起来的代码，作用的对象是调用这个代码块的对象； 　　2. 修饰一个方法，被修饰的方法称为同步方法，其作用的范围是整个方法，作用的对象是调用这个方法的对象； 　　3. 修改一个静态的方法，其作用的范围是整个静态方法，作用的对象是这个类的所有对象； 　　4. 修改一个类，其作用的范围是synchronized后面括号括起来的部分，作用主的对象是这个类的所有对象。 修饰一个代码块 一个线程访问一个对象中的synchronized(this)同步代码块时，其他试图访问该对象的线程将被阻塞。 　例子： package com.wbg; import java.util.ArrayList; import java.util.List; public class kt { public static void main(String[] args) { System.out.println(\"使用关键字synchronized\"); SyncThread syncThread = new SyncThread(); Thread thread1 = new Thread(syncThread, \"SyncThread1\"); Thread thread2 = new Thread(syncThread, \"SyncThread2\"); thread1.start(); thread2.start(); } } class SyncThread implements Runnable { private static int count; public SyncThread() { count = 0; } public void run() { synchronized (this){ for (int i = 0; i 使用关键字synchronized运行结果 不使用关键字synchronized运行结果 当两个并发线程(thread1和thread2)访问同一个对象(syncThread)中的synchronized代码块时，在同一时刻只能有一个线程得到执行，另一个线程受阻塞，必须等待当前线程执行完这个代码块以后才能执行该代码块。Thread1和thread2是互斥的，因为在执行synchronized代码块时会锁定当前的对象，只有执行完该代码块才能释放该对象锁，下一个线程才能执行并锁定该对象。 我们再把SyncThread的调用稍微改一下： public static void main(String[] args) { System.out.println(\"使用关键字synchronized每次调用进行new SyncThread()\"); Thread thread1 = new Thread( new SyncThread(), \"SyncThread1\"); Thread thread2 = new Thread( new SyncThread(), \"SyncThread2\"); thread1.start(); thread2.start(); } 运行结果 为什么上面的例子中thread1和thread2同时在执行。这是因为synchronized只锁定对象，每个对象只有一个锁（lock）与之相关联，而上面的代码等同于下面这段代码： public static void main(String[] args) { System.out.println(\"使用关键字synchronized每次调用进行new SyncThread()\"); SyncThread syncThread1 = new SyncThread(); SyncThread syncThread2 = new SyncThread(); Thread thread1 = new Thread(syncThread1, \"SyncThread1\"); Thread thread2 = new Thread(syncThread2, \"SyncThread2\"); thread1.start(); thread2.start(); } 这时创建了两个SyncThread的对象syncThread1和syncThread2，线程thread1执行的是syncThread1对象中的synchronized代码(run)，而线程thread2执行的是syncThread2对象中的synchronized代码(run)；我们知道synchronized锁定的是对象，这时会有两把锁分别锁定syncThread1对象和syncThread2对象，而这两把锁是互不干扰的，不形成互斥，所以两个线程可以同时执行。 修饰一个方法 Synchronized修饰一个方法很简单，就是在方法的前面加synchronized，public synchronized void method(){}; synchronized修饰方法和修饰一个代码块类似，只是作用范围不一样，修饰代码块是大括号括起来的范围，而修饰方法范围是整个函数。如将的run方法改成如下的方式，实现的效果一样。 public synchronized void run() { { for (int i = 0; i 运行结果 Synchronized作用于整个方法的写法。 写法一: public synchronized void method() { // todo } 写法二: public void method() { synchronized(this) { } } 写法一修饰的是一个方法，写法二修饰的是一个代码块，但写法一与写法二是等价的，都是锁定了整个方法时的内容。 在用synchronized修饰方法时要注意以下几点： \\1. synchronized关键字不能继承。 虽然可以使用synchronized来定义方法，但synchronized并不属于方法定义的一部分，因此，synchronized关键字不能被继承。如果在父类中的某个方法使用了synchronized关键字，而在子类中覆盖了这个方法，在子类中的这个方法默认情况下并不是同步的，而必须显式地在子类的这个方法中加上synchronized关键字才可以。当然，还可以在子类方法中调用父类中相应的方法，这样虽然子类中的方法不是同步的，但子类调用了父类的同步方法，因此，子类的方法也就相当于同步了。这两种方式的例子代码如下： 在子类方法中加上synchronized关键字 lass Parent { public synchronized void method() { } } class Child extends Parent { public synchronized void method() { } } 在子类方法中调用父类的同步方法 class Parent { public synchronized void method() { } } class Child extends Parent { public void method() { super.method(); } } 修饰一个静态的方法 Synchronized也可修饰一个静态方法，用法如下： public synchronized static void method() { } 我们知道静态方法是属于类的而不属于对象的。同样的，synchronized修饰的静态方法锁定的是这个类的所有对象。 synchronized修饰静态方法 package com.wbg; import java.util.ArrayList; import java.util.List; public class kt { public static void main(String[] args) { System.out.println(\"使用关键字静态synchronized\"); SyncThread syncThread = new SyncThread(); Thread thread1 = new Thread(syncThread, \"SyncThread1\"); Thread thread2 = new Thread(syncThread, \"SyncThread2\"); thread1.start(); thread2.start(); } } class SyncThread implements Runnable { private static int count; public SyncThread() { count = 0; } public synchronized static void method() { for (int i = 0; i 运行结果 syncThread1和syncThread2是SyncThread的两个对象，但在thread1和thread2并发执行时却保持了线程同步。这是因为run中调用了静态方法method，而静态方法是属于类的，所以syncThread1和syncThread2相当于用了同一把锁。这与使用关键字synchronized运行结果相同 修饰一个类 Synchronized还可作用于一个类，用法如下： 运行结果 效果和上面synchronized修饰静态方法是一样的，synchronized作用于一个类T时，是给这个类T加锁，T的所有对象用的是同一把锁。 总结: 1、 无论synchronized关键字加在方法上还是对象上，如果它作用的对象是非静态的，则它取得的锁是对象；如果synchronized作用的对象是一个静态方法或一个类，则它取得的锁是对类，该类所有的对象同一把锁。 2、每个对象只有一个锁（lock）与之相关联，谁拿到这个锁谁就可以运行它所控制的那段代码。 3、实现同步是要很大的系统开销作为代价的，甚至可能造成死锁，所以尽量避免无谓的同步控制 8 并发下的ArrayList public class Client { static ArrayList arrayList = new ArrayList(10); public static class AddThread implements Runnable{ @Override public void run(){ for (int i = 0; i 给程序运行会有三种结果 1）程序越界: 2）多线程冲突，导致结果变动 3）极少数情况会出现正确结果 改进方法：用Vector代替ArrayList static Vector arrayList = new Vector(10); 2.10并发下的hashmap 同ArrayList问题，所以建议并发下用ConcurrentHashMap代替HashMap "},"多线程/basic/0.jhappenBefore与jvm指令重排.html":{"url":"多线程/basic/0.jhappenBefore与jvm指令重排.html","title":"3.happenBefore与jvm指令重排序","keywords":"","body":"一 jmm之happen-before二 jvm之指令重排序1.as-if-serial语义2.重排序对多线程的影响一 jmm之happen-before 在JMM中，如果一个操作执行的结果需要对另一个操作可见，那么这两个操作之间必须存在happens-before关系。 happens-before原则非常重要，它是判断数据是否存在竞争、线程是否安全的主要依据，依靠这个原则，我们解决在并发环境下两操作之间是否可能存在冲突的所有问题。下面我们就一个简单的例子稍微了解下happens-before ； i = 1; //线程A执行 j = i ; //线程B执行 j 是否等于1呢？假定线程A的操作（i = 1）happens-before线程B的操作（j = i）,那么可以确定线程B执行后j = 1 一定成立，如果他们不存在happens-before原则，那么j = 1 不一定成立。这就是happens-before原则的威力。 happens-before原则定义如下： 1. 如果一个操作happens-before另一个操作，那么第一个操作的执行结果将对第二个操作可见，而且第一个操作的执行顺序排在第二个操作之前。 2. 两个操作之间存在happens-before关系，并不意味着一定要按照happens-before原则制定的顺序来执行。如果重排序之后的执行结果与按照happens-before关系来执行的结果一致，那么这种重排序并不非法。 下面是happens-before原则规则： 程序次序规则：一个线程内，按照代码顺序，书写在前面的操作先行发生于书写在后面的操作； 锁定规则：一个unLock操作先行发生于后面对同一个锁额lock操作； volatile变量规则：对一个变量的写操作先行发生于后面对这个变量的读操作； 传递规则：如果操作A先行发生于操作B，而操作B又先行发生于操作C，则可以得出操作A先行发生于操作C； 线程启动规则：Thread对象的start()方法先行发生于此线程的每个一个动作； 线程中断规则：对线程interrupt()方法的调用先行发生于被中断线程的代码检测到中断事件的发生； 线程终结规则：线程中所有的操作都先行发生于线程的终止检测，我们可以通过Thread.join()方法结束、Thread.isAlive()的返回值手段检测到线程已经终止执行； 对象终结规则：一个对象的初始化完成先行发生于他的finalize()方法的开始； 我们来详细看看上面每条规则（摘自《深入理解Java虚拟机第12章》）： 程序次序规则：一段代码在单线程中执行的结果是有序的。注意是执行结果，因为虚拟机、处理器会对指令进行重排序（重排序后面会详细介绍）。虽然重排序了，但是并不会影响程序的执行结果，所以程序最终执行的结果与顺序执行的结果是一致的。故而这个规则只对单线程有效，在多线程环境下无法保证正确性。 volatile变量规则：这是一条比较重要的规则，它标志着volatile保证了线程可见性。通俗点讲就是如果一个线程先去写一个volatile变量，然后一个线程去读这个变量，那么这个写操作一定是happens-before读操作的。 传递规则：提现了happens-before原则具有传递性，即A happens-before B , B happens-before C，那么A happens-before C 线程启动规则：假定线程A在执行过程中，通过执行ThreadB.start()来启动线程B，那么线程A对共享变量的修改在接下来线程B开始执行后确保对线程B可见。 线程终结规则：假定线程A在执行的过程中，通过指定ThreadB.join()等待线程B终止，那么线程B在终止之前对共享变量的修改在线程A等待返回后可见。 上面八条是原生Java满足Happens-before关系的规则，但是我们可以对他们进行推导出其他满足happens-before的规则： 将一个元素放入一个线程安全的队列的操作Happens-Before从队列中取出这个元素的操作 将一个元素放入一个线程安全容器的操作Happens-Before从容器中取出这个元素的操作 在CountDownLatch上的倒数操作Happens-Before CountDownLatch#await()操作 释放Semaphore许可的操作Happens-Before获得许可操作 Future表示的任务的所有操作Happens-Before Future#get()操作 向Executor提交一个Runnable或Callable的操作Happens-Before任务开始执行操作 这里再说一遍happens-before的概念：如果两个操作不存在上述（前面8条 + 后面6条）任一一个happens-before规则，那么这两个操作就没有顺序的保障，JVM可以对这两个操作进行重排序。如果操作A happens-before操作B，那么操作A在内存上所做的操作对操作B都是可见的。 下面就用一个简单的例子来描述下happens-before原则： private int i = 0; public void write(int j ){ i = j; } public int read(){ return i; } 我们约定线程A执行write()，线程B执行read()，且线程A优先于线程B执行，那么线程B获得结果是什么？；我们就这段简单的代码一次分析happens-before的规则（规则5、6、7、8 + 推导的6条可以忽略，因为他们和这段代码毫无关系）： 由于两个方法是由不同的线程调用，所以肯定不满足程序次序规则； 两个方法都没有使用锁，所以不满足锁定规则； 变量i不是用volatile修饰的，所以volatile变量规则不满足； 传递规则肯定不满足； 所以我们无法通过happens-before原则推导出线程A happens-before线程B，虽然可以确认在时间上线程A优先于线程B指定，但是就是无法确认线程B获得的结果是什么，所以这段代码不是线程安全的。那么怎么修复这段代码呢？满足规则2、3任一即可。 happen-before原则是JMM中非常重要的原则，它是判断数据是否存在竞争、线程是否安全的主要依据，保证了多线程环境下的可见性。 下图是happens-before与JMM的关系 二 jvm之指令重排序 在执行程序时，为了提供性能，处理器和编译器常常会对指令进行重排序，但是不能随意重排序，不是你想怎么排序就怎么排序，它需要满足以下两个条件： 在单线程环境下不能改变程序运行的结果； 存在数据依赖关系的不允许重排序 其实这两点可以归结于一点：无法通过happens-before原则推导出来的，JMM允许任意的排序。 1.as-if-serial语义 as-if-serial语义的意思是，所有的操作均可以为了优化而被重排序，但是你必须要保证重排序后执行的结果不能被改变，编译器、runtime、处理器都必须遵守as-if-serial语义。注意as-if-serial只保证单线程环境，多线程环境下无效。 下面我们用一个简单的示例来说明： int a = 1 ; //A int b = 2 ; //B int c = a + b; //C A、B、C三个操作存在如下关系：A、B不存在数据依赖关系，A和C、B和C存在数据依赖关系，因此在进行重排序的时候，A、B可以随意排序，但是必须位于C的前面，执行顺序可以是A --> B --> C或者B --> A --> C。但是无论是何种执行顺序最终的结果C总是等于3。 as-if-serail语义把单线程程序保护起来了，它可以保证在重排序的前提下程序的最终结果始终都是一致的。 其实对于上段代码，他们存在这样的happen-before关系： A happens-before B B happens-before C A happens-before C 1、2是程序顺序次序规则，3是传递性。但是，不是说通过重排序，B可能会排在A之前执行么，为何还会存在存在A happens-beforeB呢？这里再次申明A happens-before B不是A一定会在B之前执行，而是A的对B可见，但是相对于这个程序A的执行结果不需要对B可见，且他们重排序后不会影响结果，所以JMM不会认为这种重排序非法。 我们需要明白这点：在不改变程序执行结果的前提下，尽可能提高程序的运行效率。 下面我们在看一段有意思的代码： public class RecordExample1 { public static void main(String[] args){ int a = 1; int b = 2; try { a = 3; //A b = 1 / 0; //B } catch (Exception e) { } finally { System.out.println(\"a = \" + a); } } } 按照重排序的规则，操作A与操作B有可能会进行重排序，如果重排序了，B会抛出异常（ / by zero），此时A语句一定会执行不到，那么a还会等于3么？如果按照as-if-serial原则它就改变了程序的结果。其实JVM对异常做了一种特殊的处理，为了保证as-if-serial语义，Java异常处理机制对重排序做了一种特殊的处理：JIT在重排序时会在catch语句中插入错误代偿代码（a = 3）,这样做虽然会导致cathc里面的逻辑变得复杂，但是JIT优化原则是：尽可能地优化程序正常运行下的逻辑，哪怕以catch块逻辑变得复杂为代价。 2.重排序对多线程的影响 在单线程环境下由于as-if-serial语义，重排序无法影响最终的结果，但是对于多线程环境呢？ 如下代码（volatile的经典用法）： public class RecordExample2 { int a = 0; boolean flag = false; /** * A线程执行 */ public void writer(){ a = 1; // 1 flag = true; // 2 } /** * B线程执行 */ public void read(){ if(flag){ // 3 int i = a + a; // 4 } } } A线程执行writer()，线程B执行read()，线程B在执行时能否读到 a = 1 呢？答案是不一定（注：X86CPU不支持写写重排序，如果是在x86上面操作，这个一定会是a=1,LZ搞了好久都没有测试出来，最后查资料才发现）。 由于操作1 和操作2 之间没有数据依赖性，所以可以进行重排序处理，操作3 和操作4 之间也没有数据依赖性，他们亦可以进行重排序，但是操作3 和操作4 之间存在控制依赖性。假如操作1 和操作2 之间重排序： 按照这种执行顺序线程B肯定读不到线程A设置的a值，在这里多线程的语义就已经被重排序破坏了，重排序不会影响单线程环境的执行结果，但是会破坏多线程的执行语义。 "},"多线程/basic/1.synchronized与volatile.html":{"url":"多线程/basic/1.synchronized与volatile.html","title":"4.synchronized与volatile.md","keywords":"","body":"一 synchronized1.实现原理2.java对象头3.Monitor4.锁优化二 volatileVolatile的特性1.可见性2.有序性3.不能保证原子性4.内存屏障一 synchronized 记得刚刚开始学习Java的时候，一遇到多线程情况就是synchronized，相对于当时的我们来说synchronized是这么的神奇而又强大，那个时候我们赋予它一个名字“同步”，也成为了我们解决多线程情况的百试不爽的良药。但是，随着我们学习的进行我们知道synchronized是一个重量级锁，相对于Lock，它会显得那么笨重，以至于我们认为它不是那么的高效而慢慢摒弃它。 诚然，随着Javs SE 1.6对synchronized进行的各种优化后，synchronized并不会显得那么重了。下面一起来探索synchronized的实现机制、Java是如何对它进行了优化、锁优化机制、锁的存储结构和升级过程； 1.实现原理 synchronized可以保证方法或者代码块在运行时，同一时刻只有一个方法可以进入到临界区，同时它还可以保证共享变量的内存可见性 Java中每一个对象都可以作为锁，这是synchronized实现同步的基础： 普通同步方法，锁是当前实例对象 静态同步方法，锁是当前类的class对象 同步方法块，锁是括号里面的对象 当一个线程访问同步代码块时，它首先是需要得到锁才能执行同步代码，当退出或者抛出异常时必须要释放锁，那么它是如何来实现这个机制的呢？我们先看一段简单的代码： public class SynchronizedTest { public synchronized void test1(){ } public void test2(){ synchronized (this){ } } } 利用javap工具查看生成的class文件信息来分析Synchronize的实现 从上面可以看出，同步代码块是使用monitorenter和monitorexit指令实现的，同步方法（在这看不出来需要看JVM底层实现）依靠的是方法修饰符上的ACC_SYNCHRONIZED实现。 同步代码块：monitorenter指令插入到同步代码块的开始位置，monitorexit指令插入到同步代码块的结束位置，JVM需要保证每一个monitorenter都有一个monitorexit与之相对应。任何对象都有一个monitor与之相关联，当且一个monitor被持有之后，他将处于锁定状态。线程执行到monitorenter指令时，将会尝试获取对象所对应的monitor所有权，即尝试获取对象的锁； 同步方法：synchronized方法则会被翻译成普通的方法调用和返回指令如:invokevirtual、areturn指令，在VM字节码层面并没有任何特别的指令来实现被synchronized修饰的方法，而是在Class文件的方法表中将该方法的access_flags字段中的synchronized标志位置1，表示该方法是同步方法并使用调用该方法的对象或该方法所属的Class在JVM的内部对象表示Klass做为锁对象 下面我们来继续分析，但是在深入之前我们需要了解两个重要的概念：Java对象头，Monitor。 2.java对象头 synchronized用的锁是存在Java对象头里的，那么什么是Java对象头呢？Hotspot虚拟机的对象头主要包括两部分数据：Mark Word（标记字段）、Klass Pointer（类型指针）。其中Klass Point是是对象指向它的类元数据的指针，虚拟机通过这个指针来确定这个对象是哪个类的实例，Mark Word用于存储对象自身的运行时数据，它是实现轻量级锁和偏向锁的关键，所以下面将重点阐述 Mark Word用于存储对象自身的运行时数据，如哈希码（HashCode）、GC分代年龄、锁状态标志、线程持有的锁、偏向线程 ID、偏向时间戳等等。Java对象头一般占有两个机器码（在32位虚拟机中，1个机器码等于4字节，也就是32bit），但是如果对象是数组类型，则需要三个机器码，因为JVM虚拟机可以通过Java对象的元数据信息确定Java对象的大小，但是无法从数组的元数据来确认数组的大小，所以用一块来记录数组长度。下图是Java对象头的存储结构（32位虚拟机）： 对象头信息是与对象自身定义的数据无关的额外存储成本，但是考虑到虚拟机的空间效率，Mark Word被设计成一个非固定的数据结构以便在极小的空间内存存储尽量多的数据，它会根据对象的状态复用自己的存储空间，也就是说，Mark Word会随着程序的运行发生变化，变化状态如下（32位虚拟机）： 简单介绍了Java对象头，我们下面再看Monitor。 3.Monitor 什么是Monitor？我们可以把它理解为一个同步工具，也可以描述为一种同步机制，它通常被描述为一个对象。 与一切皆对象一样，所有的Java对象是天生的Monitor，每一个Java对象都有成为Monitor的潜质，因为在Java的设计中 ，每一个Java对象自打娘胎里出来就带了一把看不见的锁，它叫做内部锁或者Monitor锁。 Monitor 是线程私有的数据结构，每一个线程都有一个可用monitor record列表，同时还有一个全局的可用列表。每一个被锁住的对象都会和一个monitor关联（对象头的MarkWord中的LockWord指向monitor的起始地址），同时monitor中有一个Owner字段存放拥有该锁的线程的唯一标识，表示该锁被这个线程占用。其结构如下： Owner：初始时为NULL表示当前没有任何线程拥有该monitor record，当线程成功拥有该锁后保存线程唯一标识，当锁被释放时又设置为NULL； EntryQ:关联一个系统互斥锁（semaphore），阻塞所有试图锁住monitor record失败的线程。 RcThis:表示blocked或waiting在该monitor record上的所有线程的个数。 Nest:用来实现重入锁的计数。 HashCode:保存从对象头拷贝过来的HashCode值（可能还包含GC age）。 Candidate:用来避免不必要的阻塞或等待线程唤醒，因为每一次只有一个线程能够成功拥有锁，如果每次前一个释放锁的线程唤醒所有正在阻塞或等待的线程，会引起不必要的上下文切换（从阻塞到就绪然后因为竞争锁失败又被阻塞）从而导致性能严重下降。Candidate只有两种可能的值0表示没有需要唤醒的线程1表示要唤醒一个继任线程来竞争锁。 我们知道synchronized是重量级锁，效率不怎么滴，同时这个观念也一直存在我们脑海里，不过在jdk 1.6中对synchronize的实现进行了各种优化，使得它显得不是那么重了，那么JVM采用了那些优化手段呢？ 4.锁优化 HotSpot虚拟机开发团队在这个版本上花费了大量的精力去实现各种锁优化技术，如适应性自旋、锁消除、锁粗化、轻量级锁、偏向锁。 4.1 自旋锁 解决的问题：互斥同步对性能最大的影响是阻塞的实现，挂起线程和恢复线程的操作都需要转入到内核态中完成，这些操作给系统的并发性能带来了很大的压力。 如果一个系统含有两个以上的cpu,我们就能让两个线程并行执行,我们就可以让后面请求锁的线程\"稍微等一会\",看看持有锁的线程是否很快就会是否锁. 解决方案：为了让线程等待，我们需要让线程执行一个忙循环（自旋），这项技术就是自旋锁 自旋等待不能代替阻塞，且先不说对处理器量的要求，自旋等待本身虽然避免了线程切换的开销，但它要占用处理器的时间，因此，如果锁被占用的时间短，自旋等待效果好，反之，自旋的线程只会白白消耗处理器资源，而不做任何有用工作，带来性能上的浪费。因此，自旋等待的时间必须要有一定的限度，如果自旋超过了限定的次数仍然没有获得锁，就应当使用传统的方式挂起线程。自旋次数的默认值是10次，用户可以使用参数 -XX:PerBlockSpin 来修改。 4.2 自适应的自旋锁 解决自旋锁的问题：由于自旋锁每次选择的时间是固定的，对于系统来说是没有必要的。 JDK1.6引入了自适应的自旋锁。自旋的时间是由前一次在同一个锁上的自旋时间及锁的拥有者的状态来决定。如果在同一个锁对象上，自旋等待刚刚成功获得过锁，并且持有锁的线程正在运行中，那么虚拟机就认为这次自旋也很可能成功，进而它将允许自旋等待持续相对更长的时间。如果对于某个锁，自旋很少成功获得，在以后获得这个锁时将可能省略自旋过程，以避免浪费处理器资源。有了自适应自旋，随着程序运行和性能监控信息的不断完善，虚拟机对程序锁的状况预测就会越来越准确，虚拟机就变得越来越“聪明”了。 4.3 锁消除 是指虚拟机即时编译器在运行时，对一些代码上要求同步，但是被检测到不可能存在共享数据竞争的锁进行消除。锁消除主要判定依据来源于逃逸分析的数据支持，如果判断在一段代码中，堆上的所有数据都不会逃逸出去，从而被其他线程访问到，那就可以把它们当做栈上数据对待，认为它们是线程私有的，同步加锁自然无须进行。 许多同步措施并不是程序员自己加入的，同步的代码在Java程序中的普遍程度也许超过我们的想象。 public String concatString(String s1, String s2, String s3){ return s1 + s2 + s3; } 由于String是一个不可变类，对字符串的连接操作总是通过生成新的String对象来进行，因此Javac编译器会对String连接做自动优化。在JDK1.5前，会转换为StringBuffer对象的append()操作，在JDK1.5及以后版本，会转换为StringBuilder对象的连续append()操作。上段代码可能会变成下面的样子。 public String concatString(String s1, String s2, String s3){ StringBuilder sb = new StringBuilder(); sb.append(s1); sb.append(s2); sb.append(s3); return sb.toString(); } 每个StringBuilder.append()中都有一个同步块，锁是sb对象。虚拟机观察变量sb，很快就会发现它的动态作用域被限制在concatString()的内部。也就是sb的所有引用永远不会“逃逸”到concatString()外，其他线程无法访问到它，所以虽然这里有锁，但是可以被安全地消除掉，在即时编译后，这段代码会忽略所有的同步而直接执行。 4.4 锁粗化 部分情况下，我们在编写代码时，总是推荐将同步块的作用范围限制得尽量小——只在共享数据的实际作用域才进行同步，如果存在锁竞争，那等待锁的线程也可能尽快拿到锁。 但是如果一系列的连续操作都是对同一对象反复加锁和解锁，甚至加锁操作是出现在循环体中的，那即使没有线程竞争，频繁地进行互斥同步操作也会导致不必要的性能损耗。 上段代码连续的append()就属于这种情况。如果虚拟机探测到有这样一串零碎的操作都对同一个对象加锁，将会把加锁同步的范围扩展（粗化）到整个操作序列的外部，以上段代码为例，就是扩展到第一个append()之前直至最后一个append()之后，这样只需要加锁一次就可以了。 也即,将多次上锁解锁的过程,合并为一次 4.5 轻量级锁 解决问题：由于传统的重量级锁采用操作系统互斥量来实现同步，每次加锁解锁操作都需要从用户态抓换到内核态，这样会给整个系统带来一定的性能消耗。 轻量级锁原理： 要理解轻量级锁以及偏向锁，则必须从HotSpot虚拟机的对象（对象头）的内存布局开始。HotSpot虚拟机的对象头分为两部分信息，第一部分用于存储对象自身运行时数据，如哈希码（HashCode）、GC分代年龄（GC Age）等，这部分数据的长度在32位和64位的虚拟机中分别为32bit和64bit，官方称为“Mark Word”。另外一部分用于存储指向方法区对象类型数据的指针。在32位的虚拟机中25bit用于存储对象哈希码，4bit存储对象分代年龄，2bit存储锁标志位，其他略。 实现原理: 在代码进入同步块的时候，如果此同步对象没有被锁定（锁标志位为“01”状态），虚拟机首先将在当前线程的栈帧中建立一个名为锁记录（Lock Record）的空间，用于存储锁对象目前的Mark Word的拷贝（官方把这份拷贝加了一个Displaced前缀，即Displaced Mark Word） 虚拟机将使用CAS操作尝试将对象的Mark Word更新为指向Lock Record的指针。如果这个更新动作成功了，那么这个线程就拥有了该对象的锁，并且对象Mark Word的锁标志位（Mark Word的最后2bit）将转变为“00”，即表示此对象处于轻量级锁定状态 如果这个更新操作失败了，虚拟机首先会检查对象的Mark Word是否指向当前线程的栈帧，如果只说明当前线程已经拥有了这个对象的锁，那就可以直接进入同步块继续执行，否则说明这个锁对象已经被其他线程抢占了。 如果有两条以上的线程争用同一个锁，那轻量级锁就不再有效，要膨胀为重量级锁，锁标志的状态值变为“10”，Mark Word中存储的就是指向重量级锁（互斥量）的指针，后面等待锁的线程也要进入阻塞状态。 轻量级锁提升程序性能的依据是“对于绝大多数的锁，在整个同步周期内都是不存在竞争的”，这是一个经验数据。如果没有竞争，轻量级锁使用CAS操作避免了使用互斥量的开销。如果有两条以上的线程争用同一个轻量级锁，那轻量级锁就不再有效，要膨胀为重量级锁，锁标志状态变为\"10\"。 4.6 偏向锁 解决问题：轻量级锁在无竞争情况下使用CAS操作，每次加锁解锁操作都需要使用CAS指令原语。这种操作对于只有一个线程访问对象来说，会造成一定的性能开销。 偏向锁是JDK1.6引入的锁优化，目的是消除数据在无竞争情况下的同步原语，进一步提高程序的运行性能。如果说轻量级锁是在无竞争的情况下使用CAS操作消除同步使用的互斥量，那偏向锁就是在无竞争的情况下把整个同步都消除掉，连CAS操作都不做了。 实现原理： 当锁对象第一次被线程获取的时候，虚拟机将会把对象头中的标志位设为“01”，即偏向模式。 同时使用CAS操作把获取到这个锁的线程的ID记录在对象的Mark Word之中，如果CAS操作成功，持有偏向锁的线程以后每次进入这个锁相关的同步块时，虚拟机都可以不再进行任何同步操作 当有另外一个线程去尝试获取这个锁时，偏向模式就宣告结束。 根据锁对象目前是否处于被锁定的状态，撤销偏向（Revoke Bias）后恢复到未锁定（标志位为“01”）或轻量级锁定（标志位为“00”）的状态，后续的同步操作就如上面介绍的轻量级锁那样执行 偏向锁可以提高带有同步但无竞争的程序性能。它同样是一个带有效益权衡（Trade Off）性质的优化，它对程序运行不一定有利，如果程序中大多数的锁都总是被多个不同的线程访问，那偏向锁局势多余的。在具体问题具体分析的前提下，有时使用参数 -XX:-UseBiasedLocking 来禁止偏向锁优化反而可以提升性能。 4.7 重量级锁 重量级锁通过对象内部的监视器（monitor）实现，其中monitor的本质是依赖于底层操作系统的Mutex Lock实现，操作系统实现线程之间的切换需要从用户态到内核态的切换，切换成本非常高。 4.8执行顺序 先尝试锁偏向,失败后不会立即挂起,而是使用轻量级锁 若再次失败,在真正挂起前,还会自旋一段时间, 自旋失败,继续膨胀为重量级锁 最后才在操作系统层面挂起 二 volatile Volatile的特性 1.可见性 volatile的可见性是指当一个变量被volatile修饰后，这个变量就对所有线程均可见。白话点就是说当一个线程修改了一个volatile修饰的变量后，其他线程可以立刻得知这个变量的修改，拿到最这个变量最新的值。 volatile关键字作用：强制从公共堆中取得变量的值，而不是从线程的私有堆栈中取得变量的值。 public class VolatileVisibleDemo { // private boolean isReady = true; private volatile boolean isReady = true; void m() { System.out.println(Thread.currentThread().getName() + \" m start...\"); while (isReady) { } System.out.println(Thread.currentThread().getName() + \" m end...\"); } public static void main(String[] args) { VolatileVisibleDemo demo = new VolatileVisibleDemo(); new Thread(() -> demo.m(), \"t1\").start(); try {TimeUnit.SECONDS.sleep(1);} catch (InterruptedException e) {e.printStackTrace();} demo.isReady = false; // 刚才一秒过后开始执行 } } 分析：一开始isReady为true，m方法中的while会一直循环，而主线程开启开线程之后会延迟1s将isReady赋值为false，若不加volatile修饰，则程序一直在运行，若加了volatile修饰，则程序最后会输出t1 m end… 2.有序性 有序性是指程序代码的执行是按照代码的实现顺序来按序执行的；volatile的有序性特性则是指禁止JVM指令重排优化。 public class Singleton { private static Singleton instance = null; // valotile //private static volatile Singleton instance = null; private Singleton() { } // 私有 public static Singleton getInstance() { // 双重校验 //第一次判断 if(instance == null) { synchronized (Singleton.class) { // 加锁 if(instance == null) { //初始化，并非原子操作 instance = new Singleton(); // 这一行代码展开其实分三步走 } } } return instance; } } 上面的代码是一个很常见的单例模式实现方式，但是上述代码在多线程环境下是有问题的。为什么呢，问题出在instance对象的初始化上，因为instance = new Singleton();这个初始化操作并不是原子的，在JVM上会对应下面的几条指令： memory =allocate(); //1. 分配对象的内存空间 ctorInstance(memory); //2. 初始化对象 instance =memory; //3. 设置instance指向刚分配的内存地址 上面三个指令中，步骤2依赖步骤1，但是步骤3不依赖步骤2，所以JVM可能针对他们进行指令重拍序优化，重排后的指令如下： memory =allocate(); //1. 分配对象的内存空间 instance =memory; //3. 设置instance指向刚分配的内存地址 ctorInstance(memory); //2. 初始化对象 这样优化之后，内存的初始化被放到了instance分配内存地址的后面，这样的话当线程1执行步骤3这段赋值指令后，刚好有另外一个线程2进入getInstance方法判断instance不为null，这个时候线程2拿到的instance对应的内存其实还未初始化，这个时候拿去使用就会导致出错。 所以我们在用这种方式实现单例模式时，会使用volatile关键字修饰instance变量，这是因为volatile关键字除了可以保证变量可见性之外，还具有防止指令重排序的作用。当用volatile修饰instance之后，JVM执行时就不会对上面提到的初始化指令进行重排序优化，这样也就不会出现多线程安全问题了。 3.不能保证原子性 volatile关键字能保证变量的可见性和代码的有序性，但是不能保证变量的原子性，下面我再举一个volatile与原子性的例子： public class VolatileAtomicDemo { public static volatile int count = 0; public static void increase() { count++; } public static void main(String[] args) { Thread[] threads = new Thread[20]; for(int i = 0; i { for(int j = 0; j 1) { Thread.yield(); } System.out.println(count); } } 上面这段代码创建了20个线程，每个线程对变量count进行1000次自增操作，如果这段代码并发正常的话，结果应该是20000，但实际运行过程中经常会出现小于20000的结果，因为count++这个自增操作不是原子操作。 4.内存屏障 volatile汇编 0x000000011214bb49: mov %rdi,%rax 0x000000011214bb4c: dec %eax 0x000000011214bb4e: mov %eax,0x10(%rsi) 0x000000011214bb51: lock addl $0x0,(%rsp) ;*putfield v1 ; - com.earnfish.VolatileBarrierExample::readAndWrite@21 (line 35) 0x000000011214bb56: imul %edi,%ebx 0x000000011214bb59: mov %ebx,0x14(%rsi) 0x000000011214bb5c: lock addl $0x0,(%rsp) ;*putfield v2 ; - com.earnfish.VolatileBarrierExample::readAndWrite@28 (line 36) v1 = i - 1; // 第一个volatile写 v2 = j * i; // 第二个volatile写 可见其本质是通过一个lock指令来实现的。那么lock是什么意思呢？ volatile实现的两条原则（保证可见性和禁止指令重排序） Lock前缀指令会引起处理器缓存回写到内存。 lock前缀指令相当于一个内存屏障（也称内存栅栏），内存屏障主要提供3个功能： 确保指令重排序时不会把其后面的指令排到内存屏障之前的位置，也不会把前面的指令排到内存屏障的后面；即在执行到内存屏障这句指令时，在它前面的操作已经全部完成； 强制将对缓存的修改操作立即写入主存，利用缓存一致性机制，并且缓存一致性机制会阻止同时修改由两个以上CPU缓存的内存区域数据； 如果是写操作，它会导致其他CPU中对应的缓存行无效。 一个处理器的缓存回写到内存会导致其他处理器的缓存失效。 处理器使用嗅探技术保证它的内部缓存、系统内存和其他处理器的缓存的数据在总线上保持一致。例如CPU A嗅探到CPU B打算写内存地址，且这个地址处于共享状态，那么正在嗅探的处理器将使它的缓存行无效，在下次访问相同内存地址时，强制执行缓存行填充。 使用volatile的场景 必须具备以下两个条件（其实就是先保证原子性）： 对变量的写不依赖当前值（比如++操作） 该变量没有包含在具有其他变量的不等式中 比如： 状态标记（while(flag){}) double check（单例模式） "},"多线程/basic/2.ThreadLocal.html":{"url":"多线程/basic/2.ThreadLocal.html","title":"5.ThreadLocal.md","keywords":"","body":"如果想实现每一个线程都有自己的专属本地变量该如何解决呢？ JDK中提供的ThreadLocal类正是为了解决这样的问题。 ThreadLocal类主要解决的就是让每个线程绑定自己的值，可以将ThreadLocal类形象的比喻成存放数据的盒子，盒子中可以存储每个线程的私有数据。**如果你创建了一个ThreadLocal变量，那么访问这个变量的每个线程都会有这个变量的本地副本，这也是ThreadLocal变量名的由来。他们可以使用 get（） 和 set（） 方法来获取默认值或将其值更改为当前线程所存的副本的值，从而避免了线程安全问题。** 原理 public class Thread implements Runnable { ...... //与此线程有关的ThreadLocal值。由ThreadLocal类维护 ThreadLocal.ThreadLocalMap threadLocals = null; //与此线程有关的InheritableThreadLocal值。由InheritableThreadLocal类维护 ThreadLocal.ThreadLocalMap inheritableThreadLocals = null; ...... } 从上面Thread类 源代码可以看出Thread 类中有一个 threadLocals 和 一个 inheritableThreadLocals 变量，它们都是 ThreadLocalMap 类型的变量,我们可以把 ThreadLocalMap 理解为ThreadLocal 类实现的定制化的 HashMap。默认情况下这两个变量都是null，只有当前线程调用 ThreadLocal 类的 set或get方法时才创建它们，实际上调用这两个方法的时候，我们调用的是ThreadLocalMap类对应的 get()、set()方法。 ThreadLocal类的set()方法 public void set(T value) { Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) map.set(this, value); else createMap(t, value); } ThreadLocalMap getMap(Thread t) { return t.threadLocals; } 最终的变量是放在了当前线程的 ThreadLocalMap 中，并不是存在 ThreadLocal 上，ThreadLocal 可以理解为只是ThreadLocalMap的封装，传递了变量值。 每个Thread中都具备一个ThreadLocalMap，而ThreadLocalMap可以存储以ThreadLocal为key的键值对。 比如我们在同一个线程中声明了两个 ThreadLocal 对象的话，会使用 Thread内部都是使用仅有那个ThreadLocalMap 存放数据的，ThreadLocalMap的 key 就是 ThreadLocal对象，value 就是 ThreadLocal 对象调用set方法设置的值。ThreadLocal 是 map结构是为了让每个线程可以关联多个 ThreadLocal变量。这也就解释了ThreadLocal声明的变量为什么在每一个线程都有自己的专属本地变量。 内存泄露 ThreadLocalMap 中使用的 key 为 ThreadLocal 的弱引用,而 value 是强引用。所以，如果 ThreadLocal 没有被外部强引用的情况下，在垃圾回收的时候会 key 会被清理掉，而 value 不会被清理掉。这样一来，ThreadLocalMap 中就会出现key为null的Entry。假如我们不做任何措施的话，value 永远无法被GC 回收，这个时候就可能会产生内存泄露。ThreadLocalMap实现中已经考虑了这种情况，在调用 set()、get()、remove() 方法的时候，会清理掉 key 为 null 的记录。使用完 ThreadLocal方法后 最好手动调用remove()方法 "},"多线程/cas/1.乐观锁与悲观锁.html":{"url":"多线程/cas/1.乐观锁与悲观锁.html","title":"6.乐观锁与悲观锁.md","keywords":"","body":"1.何谓悲观锁与乐观锁1.何谓悲观锁与乐观锁 乐观锁对应于生活中乐观的人总是想着事情往好的方向发展，悲观锁对应于生活中悲观的人总是想着事情往坏的方向发展。这两种人各有优缺点，不能不以场景而定说一种人好于另外一种人。 悲观锁 总是假设最坏的情况，每次去拿数据的时候都认为别人会修改，所以每次在拿数据的时候都会上锁，这样别人想拿这个数据就会阻塞直到它拿到锁（共享资源每次只给一个线程使用，其它线程阻塞，用完后再把资源转让给其它线程）。传统的关系型数据库里边就用到了很多这种锁机制，比如行锁，表锁等，读锁，写锁等，都是在做操作之前先上锁。Java中synchronized和ReentrantLock等独占锁就是悲观锁思想的实现。 乐观锁 总是假设最好的情况，每次去拿数据的时候都认为别人不会修改，所以不会上锁，但是在更新的时候会判断一下在此期间别人有没有去更新这个数据，可以使用版本号机制和CAS算法实现。乐观锁适用于多读的应用类型，这样可以提高吞吐量，像数据库提供的类似于write_condition机制，其实都是提供的乐观锁。在Java中java.util.concurrent.atomic包下面的原子变量类就是使用了乐观锁的一种实现方式CAS实现的。 两种锁的使用场景 从上面对两种锁的介绍，我们知道两种锁各有优缺点，不可认为一种好于另一种，像乐观锁适用于写比较少的情况下（多读场景），即冲突真的很少发生的时候，这样可以省去了锁的开销，加大了系统的整个吞吐量。但如果是多写的情况，一般会经常产生冲突，这就会导致上层应用会不断的进行retry，这样反倒是降低了性能，所以一般多写的场景下用悲观锁就比较合适。 乐观锁常见的两种实现方式 乐观锁一般会使用版本号机制或CAS算法实现。 1. 版本号机制 一般是在数据表中加上一个数据版本号version字段，表示数据被修改的次数，当数据被修改时，version值会加一。当线程A要更新数据值时，在读取数据的同时也会读取version值，在提交更新时，若刚才读取到的version值为当前数据库中的version值相等时才更新，否则重试更新操作，直到更新成功。 举一个简单的例子： 假设数据库中帐户信息表中有一个 version 字段，当前值为 1 ；而当前帐户余额字段（ balance ）为 $100 。 操作员 A 此时将其读出（ version=1 ），并从其帐户余额中扣除 $50（ $100-$50 ）。 在操作员 A 操作的过程中，操作员B 也读入此用户信息（ version=1 ），并从其帐户余额中扣除 $20 （ $100-$20 ）。 操作员 A 完成了修改工作，将数据版本号加一（ version=2 ），连同帐户扣除后余额（ balance=$50 ），提交至数据库更新，此时由于提交数据版本大于数据库记录当前版本，数据被更新，数据库记录 version 更新为 2 。 操作员 B 完成了操作，也将版本号加一（ version=2 ）试图向数据库提交数据（ balance=$80 ），但此时比对数据库记录版本时发现，操作员 B 提交的数据版本号为 2 ，数据库记录当前版本也为 2 ，不满足 “ 提交版本必须大于记录当前版本才能执行更新 “ 的乐观锁策略，因此，操作员 B 的提交被驳回。 这样，就避免了操作员 B 用基于 version=1 的旧数据修改的结果覆盖操作员A 的操作结果的可能。 2. CAS算法 即compare and swap（比较与交换），是一种有名的无锁算法。无锁编程，即不使用锁的情况下实现多线程之间的变量同步，也就是在没有线程被阻塞的情况下实现变量的同步，所以也叫非阻塞同步（Non-blocking Synchronization）。CAS算法涉及到三个操作数 需要读写的内存值 V 进行比较的值 A 拟写入的新值 B 当且仅当 V 的值等于 A时，CAS通过原子方式用新值B来更新V的值，否则不会执行任何操作（比较和替换是一个原子操作）。一般情况下是一个自旋操作，即不断的重试。 "},"多线程/cas/2.CAS与atomic类.html":{"url":"多线程/cas/2.CAS与atomic类.html","title":"7.CAS与atomic类.md","keywords":"","body":"CASCAS分析CAS缺陷atomic无锁类CAS CAS，Compare And Swap，即比较并交换。Doug lea大神在同步组件中大量使用CAS技术鬼斧神工地实现了Java多线程的并发操作。整个AQS同步组件、Atomic原子类操作等等都是以CAS实现的，甚至ConcurrentHashMap在1.8的版本中也调整为了CAS+Synchronized。可以说CAS是整个JUC的基石。 CAS分析 在CAS中有三个参数：内存值V、旧的预期值A、要更新的值B，当且仅当内存值V的值等于旧的预期值A时才会将内存值V的值修改为B，否则什么都不干。其伪代码如下： if(this.value == A){ this.value = B return true; }else{ return false; } JUC下的atomic类都是通过CAS来实现的，下面就以AtomicInteger为例来阐述CAS的实现。如下： private static final Unsafe unsafe = Unsafe.getUnsafe(); private static final long valueOffset; static { try { valueOffset = unsafe.objectFieldOffset (AtomicInteger.class.getDeclaredField(\"value\")); } catch (Exception ex) { throw new Error(ex); } } private volatile int value; Unsafe是CAS的核心类，Java无法直接访问底层操作系统，而是通过本地（native）方法来访问。不过尽管如此，JVM还是开了一个后门：Unsafe，它提供了硬件级别的原子操作。 valueOffset为变量值在内存中的偏移地址，unsafe就是通过偏移地址来得到数据的原值的。 value当前值，使用volatile修饰，保证多线程环境下看见的是同一个。 我们就以AtomicInteger的addAndGet()方法来做说明，先看源代码： public final int addAndGet(int delta) { return unsafe.getAndAddInt(this, valueOffset, delta) + delta; } public final int getAndAddInt(Object var1, long var2, int var4) { int var5; do { var5 = this.getIntVolatile(var1, var2); } while(!this.compareAndSwapInt(var1, var2, var5, var5 + var4)); return var5; } 内部调用unsafe的getAndAddInt方法，在getAndAddInt方法中主要是看compareAndSwapInt方法： public final native boolean compareAndSwapInt(Object var1, long var2, int var4, int var5); 该方法为本地方法，有四个参数，分别代表：对象、对象的地址、预期值、修改值。该方法的实现这里就不做详细介绍了，有兴趣的伙伴可以看看openjdk的源码。 CAS可以保证一次的读-改-写操作是原子操作，在单处理器上该操作容易实现，但是在多处理器上实现就有点儿复杂了。 CPU提供了两种方法来实现多处理器的原子操作：总线加锁或者缓存加锁。 总线加锁：总线加锁就是就是使用处理器提供的一个LOCK#信号，当一个处理器在总线上输出此信号时，其他处理器的请求将被阻塞住,那么该处理器可以独占使用共享内存。但是这种处理方式显得有点儿霸道，不厚道，他把CPU和内存之间的通信锁住了，在锁定期间，其他处理器都不能其他内存地址的数据，其开销有点儿大。所以就有了缓存加锁。 缓存加锁：其实针对于上面那种情况我们只需要保证在同一时刻对某个内存地址的操作是原子性的即可。缓存加锁就是缓存在内存区域的数据如果在加锁期间，当它执行锁操作写回内存时，处理器不在输出LOCK#信号，而是修改内部的内存地址，利用缓存一致性协议来保证原子性。缓存一致性机制可以保证同一个内存区域的数据仅能被一个处理器修改，也就是说当CPU1修改缓存行中的i时使用缓存锁定，那么CPU2就不能同时缓存了i的缓存行。 CAS缺陷 CAS虽然高效地解决了原子操作，但是还是存在一些缺陷的，主要表现在三个方法：循环时间太长、只能保证一个共享变量原子操作、ABA问题。 循环时间太长 如果CAS一直不成功呢？这种情况绝对有可能发生，如果自旋CAS长时间地不成功，则会给CPU带来非常大的开销。在JUC中有些地方就限制了CAS自旋的次数，例如BlockingQueue的SynchronousQueue。 只能保证一个共享变量原子操作 看了CAS的实现就知道这只能针对一个共享变量，如果是多个共享变量就只能使用锁了，当然如果你有办法把多个变量整成一个变量，利用CAS也不错。例如读写锁中state的高地位 ABA问题 CAS需要检查操作值有没有发生改变，如果没有发生改变则更新。但是存在这样一种情况：如果一个值原来是A，变成了B，然后又变成了A，那么在CAS检查的时候会发现没有改变，但是实质上它已经发生了改变，这就是所谓的ABA问题。对于ABA问题其解决方案是加上版本号，即在每个变量都加上一个版本号，每次改变时加1，即A —> B —> A，变成1A —> 2B —> 3A。 CAS的ABA隐患问题，解决方案则是版本号，Java提供了AtomicStampedReference来解决。AtomicStampedReference通过包装[E,Integer]的元组来对对象标记版本戳stamp，从而避免ABA问题。对于上面的案例应该线程1会失败。 ​ 的compareAndSet()方法定义如下： public boolean compareAndSet(V expectedReference, V newReference, int expectedStamp, int newStamp) { Pair current = pair; return expectedReference == current.reference && expectedStamp == current.stamp && ((newReference == current.reference && newStamp == current.stamp) || casPair(current, Pair.of(newReference, newStamp))); } compareAndSet有四个参数，分别表示：预期引用、更新后的引用、预期标志、更新后的标志。源码部门很好理解预期的引用 == 当前引用，预期的标识 == 当前标识，如果更新后的引用和标志和当前的引用和标志相等则直接返回true，否则通过Pair生成一个新的pair对象与当前pair CAS替换。Pair为AtomicStampedReference的内部类，主要用于记录引用和版本戳信息（标识），定义如下： private static class Pair { final T reference; final int stamp; private Pair(T reference, int stamp) { this.reference = reference; this.stamp = stamp; } static Pair of(T reference, int stamp) { return new Pair(reference, stamp); } } private volatile Pair pair; Pair记录着对象的引用和版本戳，版本戳为int型，保持自增。同时Pair是一个不可变对象，其所有属性全部定义为final，对外提供一个of方法，该方法返回一个新建的Pari对象。pair对象定义为volatile，保证多线程环境下的可见性。在AtomicStampedReference中，大多方法都是通过调用Pair的of方法来产生一个新的Pair对象，然后赋值给变量pair。如set方法： public void set(V newReference, int newStamp) { Pair current = pair; if (newReference != current.reference || newStamp != current.stamp) this.pair = Pair.of(newReference, newStamp); } atomic无锁类 "},"多线程/aqs/1.AQS.html":{"url":"多线程/aqs/1.AQS.html","title":"8.AQS.md","keywords":"","body":"一 AQS简介1 AQS 简单介绍2 AQS 原理二 CLH同步队列入列出列三 AQS之同步状态的获取独占式共享式四 AQS之阻塞和唤醒线程阻塞唤醒LockSupport一 AQS简介 1 AQS 简单介绍 AQS 的全称为（AbstractQueuedSynchronizer），这个类在 java.util.concurrent.locks 包下面。 AQS 是一个用来构建锁和同步器的框架，使用 AQS 能简单且高效地构造出应用广泛的大量的同步器，比如我们提到的 ReentrantLock，Semaphore，其他的诸如 ReentrantReadWriteLock，SynchronousQueue，FutureTask(jdk1.7) 等等皆是基于 AQS 的。当然，我们自己也能利用 AQS 非常轻松容易地构造出符合我们自己需求的同步器。 2 AQS 原理 2.1 AQS 原理概览 AQS 核心思想是，如果被请求的共享资源空闲，则将当前请求资源的线程设置为有效的工作线程，并且将共享资源设置为锁定状态。如果被请求的共享资源被占用，那么就需要一套线程阻塞等待以及被唤醒时锁分配的机制，这个机制 AQS 是用 CLH 队列锁实现的，即将暂时获取不到锁的线程加入到队列中。 CLH(Craig,Landin,and Hagersten)队列是一个虚拟的双向队列（虚拟的双向队列即不存在队列实例，仅存在结点之间的关联关系）。AQS 是将每条请求共享资源的线程封装成一个 CLH 锁队列的一个结点（Node）来实现锁的分配。 看个 AQS(AbstractQueuedSynchronizer)原理图： AQS 使用一个 int 成员变量来表示同步状态，通过内置的 FIFO 队列来完成获取资源线程的排队工作。AQS 使用 CAS 对该同步状态进行原子操作实现对其值的修改。 private volatile int state;//共享变量，使用volatile修饰保证线程可见性 状态信息通过 protected 类型的getState，setState，compareAndSetState进行操作 //返回同步状态的当前值 protected final int getState() { return state; } // 设置同步状态的值 protected final void setState(int newState) { state = newState; } //原子地（CAS操作）将同步状态值设置为给定值update如果当前同步状态的值等于expect（期望值） protected final boolean compareAndSetState(int expect, int update) { return unsafe.compareAndSwapInt(this, stateOffset, expect, update); } 2.2 AQS 对资源的共享方式 AQS 定义两种资源共享方式 1)Exclusive（独占） 只有一个线程能执行，如 ReentrantLock。又可分为公平锁和非公平锁,ReentrantLock 同时支持两种锁,下面以 ReentrantLock 对这两种锁的定义做介绍： 公平锁：按照线程在队列中的排队顺序，先到者先拿到锁 非公平锁：当线程要获取锁时，先通过两次 CAS 操作去抢锁，如果没抢到，当前线程再加入到队列中等待唤醒。 下面来看 ReentrantLock 中相关的源代码： ReentrantLock 默认采用非公平锁，因为考虑获得更好的性能，通过 boolean 来决定是否用公平锁（传入 true 用公平锁）。 /** Synchronizer providing all implementation mechanics */ private final Sync sync; public ReentrantLock() { // 默认非公平锁 sync = new NonfairSync(); } public ReentrantLock(boolean fair) { sync = fair ? new FairSync() : new NonfairSync(); } 注意,在reentrantLock中,真正继承AbstractQueuedSynchronizer的是内部类sync,而FairSync和noneFairSync又继承了sync,真正体现了模板方法设计模式之美. ReentrantLock 中公平锁的 lock 方法 static final class FairSync extends Sync { final void lock() { acquire(1); } // AbstractQueuedSynchronizer.acquire(int arg) public final void acquire(int arg) { if (!tryAcquire(arg) && acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) selfInterrupt(); } protected final boolean tryAcquire(int acquires) { final Thread current = Thread.currentThread(); int c = getState(); if (c == 0) { // 1. 和非公平锁相比，这里多了一个判断：是否有线程在等待 if (!hasQueuedPredecessors() && compareAndSetState(0, acquires)) { setExclusiveOwnerThread(current); return true; } } else if (current == getExclusiveOwnerThread()) { int nextc = c + acquires; if (nextc 非公平锁的 lock 方法： static final class NonfairSync extends Sync { final void lock() { // 2. 和公平锁相比，这里会直接先进行一次CAS，成功就返回了 if (compareAndSetState(0, 1)) setExclusiveOwnerThread(Thread.currentThread()); else acquire(1); } // AbstractQueuedSynchronizer.acquire(int arg) public final void acquire(int arg) { if (!tryAcquire(arg) && acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) selfInterrupt(); } protected final boolean tryAcquire(int acquires) { return nonfairTryAcquire(acquires); } } /** * Performs non-fair tryLock. tryAcquire is implemented in * subclasses, but both need nonfair try for trylock method. */ final boolean nonfairTryAcquire(int acquires) { final Thread current = Thread.currentThread(); int c = getState(); if (c == 0) { // 这里没有对阻塞队列进行判断 if (compareAndSetState(0, acquires)) { setExclusiveOwnerThread(current); return true; } } else if (current == getExclusiveOwnerThread()) { int nextc = c + acquires; if (nextc 总结：公平锁和非公平锁只有两处不同： 非公平锁在调用 lock 后，首先就会调用 CAS 进行一次抢锁，如果这个时候恰巧锁没有被占用，那么直接就获取到锁返回了。 非公平锁在 CAS 失败后，和公平锁一样都会进入到 tryAcquire 方法，在 tryAcquire 方法中，如果发现锁这个时候被释放了（state == 0），非公平锁会直接 CAS 抢锁，但是公平锁会判断等待队列是否有线程处于等待状态，如果有则不去抢锁，乖乖排到后面。 公平锁和非公平锁就这两点区别，如果这两次 CAS 都不成功，那么后面非公平锁和公平锁是一样的，都要进入到阻塞队列等待唤醒。 相对来说，非公平锁会有更好的性能，因为它的吞吐量比较大。当然，非公平锁让获取锁的时间变得更加不确定，可能会导致在阻塞队列中的线程长期处于饥饿状态。 2)Share（共享） 多个线程可同时执行，如 Semaphore/CountDownLatch。Semaphore、CountDownLatCh、 CyclicBarrier、ReadWriteLock 我们都会在后面讲到。 ReentrantReadWriteLock 可以看成是组合式，因为 ReentrantReadWriteLock 也就是读写锁允许多个线程同时对某一资源进行读。 不同的自定义同步器争用共享资源的方式也不同。自定义同步器在实现时只需要实现共享资源 state 的获取与释放方式即可，至于具体线程等待队列的维护（如获取资源失败入队/唤醒出队等），AQS 已经在上层已经帮我们实现好了。 2.3 AQS 底层使用了模板方法模式 同步器的设计是基于模板方法模式的，如果需要自定义同步器一般的方式是这样（模板方法模式很经典的一个应用）： 使用者继承 AbstractQueuedSynchronizer 并重写指定的方法。（这些重写方法很简单，无非是对于共享资源 state 的获取和释放） 将 AQS 组合在自定义同步组件的实现中，并调用其模板方法，而这些模板方法会调用使用者重写的方法。 这和我们以往通过实现接口的方式有很大区别，这是模板方法模式很经典的一个运用，下面简单的给大家介绍一下模板方法模式，模板方法模式是一个很容易理解的设计模式之一。 模板方法模式是基于”继承“的，主要是为了在不改变模板结构的前提下在子类中重新定义模板中的内容以实现复用代码。举个很简单的例子假如我们要去一个地方的步骤是：购票buyTicket()->安检securityCheck()->乘坐某某工具回家ride()->到达目的地arrive()。我们可能乘坐不同的交通工具回家比如飞机或者火车，所以除了ride()方法，其他方法的实现几乎相同。我们可以定义一个包含了这些方法的抽象类，然后用户根据自己的需要继承该抽象类然后修改 ride()方法。 AQS 使用了模板方法模式，自定义同步器时需要重写下面几个 AQS 提供的模板方法： isHeldExclusively()//该线程是否正在独占资源。只有用到condition才需要去实现它。 tryAcquire(int)//独占方式。尝试获取资源，成功则返回true，失败则返回false。 tryRelease(int)//独占方式。尝试释放资源，成功则返回true，失败则返回false。 tryAcquireShared(int)//共享方式。尝试获取资源。负数表示失败；0表示成功，但没有剩余可用资源；正数表示成功，且有剩余资源。 tryReleaseShared(int)//共享方式。尝试释放资源，成功则返回true，失败则返回false。 默认情况下，每个方法都抛出 UnsupportedOperationException。 这些方法的实现必须是内部线程安全的，并且通常应该简短而不是阻塞。AQS 类中的其他方法都是 final ，所以无法被其他类使用，只有这几个方法可以被其他类使用。 以 ReentrantLock 为例，state 初始化为 0，表示未锁定状态。A 线程 lock()时，会调用 tryAcquire()独占该锁并将 state+1。此后，其他线程再 tryAcquire()时就会失败，直到 A 线程 unlock()到 state=0（即释放锁）为止，其它线程才有机会获取该锁。当然，释放锁之前，A 线程自己是可以重复获取此锁的（state 会累加），这就是可重入的概念。但要注意，获取多少次就要释放多么次，这样才能保证 state 是能回到零态的。 再以 CountDownLatch 以例，任务分为 N 个子线程去执行，state 也初始化为 N（注意 N 要与线程个数一致）。这 N 个子线程是并行执行的，每个子线程执行完后 countDown()一次，state 会 CAS(Compare and Swap)减 1。等到所有子线程都执行完后(即 state=0)，会 unpark()主调用线程，然后主调用线程就会从 await()函数返回，继续后余动作。 一般来说，自定义同步器要么是独占方法，要么是共享方式，他们也只需实现tryAcquire-tryRelease、tryAcquireShared-tryReleaseShared中的一种即可。但 AQS 也支持自定义同步器同时实现独占和共享两种方式，如ReentrantReadWriteLock。 推荐两篇 AQS 原理和相关源码分析的文章： http://www.cnblogs.com/waterystone/p/4920797.html https://www.cnblogs.com/chengxiao/archive/2017/07/24/7141160.html 二 CLH同步队列 AQS内部维护着一个FIFO队列，该队列就是CLH同步队列。 CLH同步队列是一个FIFO双向队列，AQS依赖它来完成同步状态的管理，当前线程如果获取同步状态失败时，AQS则会将当前线程已经等待状态等信息构造成一个节点（Node）并将其加入到CLH同步队列，同时会阻塞当前线 程，当同步状态释放时，会把首节点唤醒（公平锁），使其再次尝试获取同步状态。 在CLH同步队列中，一个节点表示一个线程，它保存着线程的引用（thread）、状态（waitStatus）、前驱节点（prev）、后继节点（next），其定义如下： static final class Node { /** 共享 */ static final Node SHARED = new Node(); /** 独占 */ static final Node EXCLUSIVE = null; /** * 因为超时或者中断，节点会被设置为取消状态，被取消的节点时不会参与到竞争中的，他会一直保持取消状态不会转变为其他状态； */ static final int CANCELLED = 1; /** * 后继节点的线程处于等待状态，而当前节点的线程如果释放了同步状态或者被取消，将会通知后继节点，使后继节点的线程得以运行 */ static final int SIGNAL = -1; /** * 节点在等待队列中，节点线程等待在Condition上，当其他线程对Condition调用了signal()后，改节点将会从等待队列中转移到同步队列中，加入到同步状态的获取中 */ static final int CONDITION = -2; /** * 表示下一次共享式同步状态获取将会无条件地传播下去 */ static final int PROPAGATE = -3; /** 等待状态 */ volatile int waitStatus; /** 前驱节点 */ volatile Node prev; /** 后继节点 */ volatile Node next; /** 获取同步状态的线程 */ volatile Thread thread; Node nextWaiter; final boolean isShared() { return nextWaiter == SHARED; } final Node predecessor() throws NullPointerException { Node p = prev; if (p == null) throw new NullPointerException(); else return p; } Node() { } Node(Thread thread, Node mode) { this.nextWaiter = mode; this.thread = thread; } Node(Thread thread, int waitStatus) { this.waitStatus = waitStatus; this.thread = thread; } } 入列 学了数据结构的我们，CLH队列入列是再简单不过了，无非就是tail指向新节点、新节点的prev指向当前最后的节点，当前最后一个节点的next指向当前节点。代码我们可以看看addWaiter(Node node)方法： private Node addWaiter(Node mode) { //新建Node Node node = new Node(Thread.currentThread(), mode); //快速尝试添加尾节点 Node pred = tail; if (pred != null) { node.prev = pred; //CAS设置尾节点 if (compareAndSetTail(pred, node)) { pred.next = node; return node; } } //多次尝试 enq(node); return node; } addWaiter(Node node)先通过快速尝试设置尾节点，如果失败，则调用enq(Node node)方法设置尾节点 private Node enq(final Node node) { //多次尝试，直到成功为止 for (;;) { Node t = tail; //tail不存在，设置为首节点 if (t == null) { if (compareAndSetHead(new Node())) tail = head; } else { //设置为尾节点 node.prev = t; if (compareAndSetTail(t, node)) { t.next = node; return t; } } } } 在上面代码中，两个方法都是通过一个CAS方法compareAndSetTail(Node expect, Node update)来设置尾节点，该方法可以确保节点是线程安全添加的。在enq(Node node)方法中，AQS通过“死循环”的方式来保证节点可以正确添加，只有成功添加后，当前线程才会从该方法返回，否则会一直执行下去。 过程图如下： 出列 CLH同步队列遵循FIFO，首节点的线程释放同步状态后，将会唤醒它的后继节点（next），而后继节点将会在获取同步状态成功时将自己设置为首节点，这个过程非常简单，head执行该节点并断开原首节点的next和当前节点的prev即可，注意在这个过程是不需要使用CAS来保证的，因为只有一个线程能够成功获取到同步状态。过程图如下： 三 AQS之同步状态的获取 在前面提到过，AQS是构建Java同步组件的基础，我们期待它能够成为实现大部分同步需求的基础。AQS的设计模式采用的模板方法模式，子类通过继承的方式，实现它的抽象方法来管理同步状态，对于子类而言它并没有太多的活要做，AQS提供了大量的模板方法来实现同步，主要是分为三类：独占式获取和释放同步状态、共享式获取和释放同步状态、查询同步队列中的等待线程情况。自定义子类使用AQS提供的模板方法就可以实现自己的同步语义。 独占式 独占式，同一时刻仅有一个线程持有同步状态。 独占式同步状态获取 acquire(int arg)方法为AQS提供的模板方法，该方法为独占式获取同步状态，但是该方法对中断不敏感，也就是说由于线程获取同步状态失败加入到CLH同步队列中，后续对线程进行中断操作时，线程不会从同步队列中移除。代码如下： public final void acquire(int arg) { if (!tryAcquire(arg) && acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) selfInterrupt(); } 各个方法定义如下： tryAcquire：去尝试获取锁，获取成功则设置锁状态并返回true，否则返回false。该方法自定义同步组件自己实现，该方法必须要保证线程安全的获取同步状态。 addWaiter：如果tryAcquire返回FALSE（获取同步状态失败），则调用该方法将当前线程加入到CLH同步队列尾部。 acquireQueued：当前线程会根据公平性原则来进行阻塞等待（自旋）,直到获取锁为止；并且返回当前线程在等待过程中有没有中断过。 selfInterrupt：产生一个中断。 acquireQueued方法为一个自旋的过程，也就是说当前线程（Node）进入同步队列后，就会进入一个自旋的过程，每个节点都会自省地观察，当条件满足，获取到同步状态后，就可以从这个自旋过程中退出，否则会一直执行下去。如下： final boolean acquireQueued(final Node node, int arg) { boolean failed = true; try { //中断标志 boolean interrupted = false; /* * 自旋过程，其实就是一个死循环而已 */ for (;;) { //当前线程的前驱节点 final Node p = node.predecessor(); //当前线程的前驱节点是头结点，且同步状态成功 if (p == head && tryAcquire(arg)) { setHead(node); p.next = null; // help GC failed = false; return interrupted; } //获取失败，线程等待--具体后面介绍 if (shouldParkAfterFailedAcquire(p, node) && parkAndCheckInterrupt()) interrupted = true; } } finally { if (failed) cancelAcquire(node); } } 从上面代码中可以看到，当前线程会一直尝试获取同步状态，当然前提是只有其前驱节点为头结点才能够尝试获取同步状态，理由： 保持FIFO同步队列原则。 头节点释放同步状态后，将会唤醒其后继节点，后继节点被唤醒后需要检查自己是否为头节点。 acquire(int arg)方法流程图如下： 独占式获取响应中断 AQS提供了acquire(int arg)方法以供独占式获取同步状态，但是该方法对中断不响应，对线程进行中断操作后，该线程会依然位于CLH同步队列中等待着获取同步状态。为了响应中断，AQS提供了acquireInterruptibly(int arg)方法，该方法在等待获取同步状态时，如果当前线程被中断了，会立刻响应中断抛出异常InterruptedException。 public final void acquireInterruptibly(int arg) throws InterruptedException { if (Thread.interrupted()) throw new InterruptedException(); if (!tryAcquire(arg)) doAcquireInterruptibly(arg); } 首先校验该线程是否已经中断了，如果是则抛出InterruptedException，否则执行tryAcquire(int arg)方法获取同步状态，如果获取成功，则直接返回，否则执行doAcquireInterruptibly(int arg)。doAcquireInterruptibly(int arg)定义如下： private void doAcquireInterruptibly(int arg) throws InterruptedException { final Node node = addWaiter(Node.EXCLUSIVE); boolean failed = true; try { for (;;) { final Node p = node.predecessor(); if (p == head && tryAcquire(arg)) { setHead(node); p.next = null; // help GC failed = false; return; } if (shouldParkAfterFailedAcquire(p, node) && parkAndCheckInterrupt()) throw new InterruptedException(); } } finally { if (failed) cancelAcquire(node); } } doAcquireInterruptibly(int arg)方法与acquire(int arg)方法仅有两个差别。1.方法声明抛出InterruptedException异常，2.在中断方法处不再是使用interrupted标志，而是直接抛出InterruptedException异常。 独占式超时获取 AQS除了提供上面两个方法外，还提供了一个增强版的方法：tryAcquireNanos(int arg,long nanos)。该方法为acquireInterruptibly方法的进一步增强，它除了响应中断外，还有超时控制。即如果当前线程没有在指定时间内获取同步状态，则会返回false，否则返回true。如下： public final boolean tryAcquireNanos(int arg, long nanosTimeout) throws InterruptedException { if (Thread.interrupted()) throw new InterruptedException(); return tryAcquire(arg) || doAcquireNanos(arg, nanosTimeout); } tryAcquireNanos(int arg, long nanosTimeout)方法超时获取最终是在doAcquireNanos(int arg, long nanosTimeout)中实现的，如下： private boolean doAcquireNanos(int arg, long nanosTimeout) throws InterruptedException { //nanosTimeout spinForTimeoutThreshold) LockSupport.parkNanos(this, nanosTimeout); //线程是否已经中断了 if (Thread.interrupted()) throw new InterruptedException(); } } finally { if (failed) cancelAcquire(node); } } 针对超时控制，程序首先记录唤醒时间deadline ，deadline = System.nanoTime() + nanosTimeout（时间间隔）。如果获取同步状态失败，则需要计算出需要休眠的时间间隔nanosTimeout（= deadline - System.nanoTime()），如果nanosTimeout 独占式同步状态释放 当线程获取同步状态后，执行完相应逻辑后就需要释放同步状态。AQS提供了release(int arg)方法释放同步状态： public final boolean release(int arg) { if (tryRelease(arg)) { Node h = head; if (h != null && h.waitStatus != 0) unparkSuccessor(h); return true; } return false; } 该方法同样是先调用自定义同步器自定义的tryRelease(int arg)方法来释放同步状态，释放成功后，会调用unparkSuccessor(Node node)方法唤醒后继节点（如何唤醒LZ后面介绍）。 这里稍微总结下： 在AQS中维护着一个FIFO的同步队列，当线程获取同步状态失败后，则会加入到这个CLH同步队列的对尾并一直保持着自旋。在CLH同步队列中的线程在自旋时会判断其前驱节点是否为首节点，如果为首节点则不断尝试获取同步状态，获取成功则退出CLH同步队列。当线程执行完逻辑后，会释放同步状态，释放后会唤醒其后继节点。 共享式 共享式与独占式的最主要区别在于同一时刻独占式只能有一个线程获取同步状态，而共享式在同一时刻可以有多个线程获取同步状态。例如读操作可以有多个线程同时进行，而写操作同一时刻只能有一个线程进行写操作，其他操作都会被阻塞。 共享式同步状态获取 AQS提供acquireShared(int arg)方法共享式获取同步状态： public final void acquireShared(int arg) { if (tryAcquireShared(arg) 从上面程序可以看出，方法首先是调用tryAcquireShared(int arg)方法尝试获取同步状态，如果获取失败则调用doAcquireShared(int arg)自旋方式获取同步状态，共享式获取同步状态的标志是返回 >= 0 的值表示获取成功。自选式获取同步状态如下： private void doAcquireShared(int arg) { /共享式节点 final Node node = addWaiter(Node.SHARED); boolean failed = true; try { boolean interrupted = false; for (;;) { //前驱节点 final Node p = node.predecessor(); //如果其前驱节点，获取同步状态 if (p == head) { //尝试获取同步 int r = tryAcquireShared(arg); if (r >= 0) { setHeadAndPropagate(node, r); p.next = null; // help GC if (interrupted) selfInterrupt(); failed = false; return; } } if (shouldParkAfterFailedAcquire(p, node) && parkAndCheckInterrupt()) interrupted = true; } } finally { if (failed) cancelAcquire(node); } } tryAcquireShared(int arg)方法尝试获取同步状态，返回值为int，当其 >= 0 时，表示能够获取到同步状态，这个时候就可以从自旋过程中退出。 acquireShared(int arg)方法不响应中断，与独占式相似，AQS也提供了响应中断、超时的方法，分别是：acquireSharedInterruptibly(int arg)、tryAcquireSharedNanos(int arg,long nanos)，这里就不做解释了。 共享式同步状态释放 获取同步状态后，需要调用release(int arg)方法释放同步状态，方法如下： public final boolean releaseShared(int arg) { if (tryReleaseShared(arg)) { doReleaseShared(); return true; } return false; } 因为可能会存在多个线程同时进行释放同步状态资源，所以需要确保同步状态安全地成功释放，一般都是通过CAS和循环来完成的。 四 AQS之阻塞和唤醒线程 阻塞 在线程获取同步状态时如果获取失败，则加入CLH同步队列，通过通过自旋的方式不断获取同步状态，但是在自旋的过程中则需要判断当前线程是否需要阻塞，其主要方法在acquireQueued()： if (shouldParkAfterFailedAcquire(p, node) && parkAndCheckInterrupt()) interrupted = true; 通过这段代码我们可以看到，在获取同步状态失败后，线程并不是立马进行阻塞，需要检查该线程的状态，检查状态的方法为 shouldParkAfterFailedAcquire(Node pred, Node node) 方法，该方法主要靠前驱节点判断当前线程是否应该被阻塞，代码如下： private static boolean shouldParkAfterFailedAcquire(Node pred, Node node) { //前驱节点 int ws = pred.waitStatus; //状态为signal，表示当前线程处于等待状态，直接放回true if (ws == Node.SIGNAL) return true; //前驱节点状态 > 0 ，则为Cancelled,表明该节点已经超时或者被中断了，需要从同步队列中取消 if (ws > 0) { do { node.prev = pred = pred.prev; } while (pred.waitStatus > 0); pred.next = node; } //前驱节点状态为Condition、propagate else { compareAndSetWaitStatus(pred, ws, Node.SIGNAL); } return false; } 这段代码主要检查当前线程是否需要被阻塞，具体规则如下： 如果当前线程的前驱节点状态为SINNAL，则表明当前线程需要被阻塞，调用unpark()方法唤醒，直接返回true，当前线程阻塞 如果当前线程的前驱节点状态为CANCELLED（ws > 0），则表明该线程的前驱节点已经等待超时或者被中断了，则需要从CLH队列中将该前驱节点删除掉，直到回溯到前驱节点状态 如果前驱节点非SINNAL，非CANCELLED，则通过CAS的方式将其前驱节点设置为SINNAL，返回false 如果 shouldParkAfterFailedAcquire(Node pred, Node node) 方法返回true，则调用parkAndCheckInterrupt()方法阻塞当前线程： private final boolean parkAndCheckInterrupt() { LockSupport.park(this); return Thread.interrupted(); } parkAndCheckInterrupt() 方法主要是把当前线程挂起，从而阻塞住线程的调用栈，同时返回当前线程的中断状态。其内部则是调用LockSupport工具类的park()方法来阻塞该方法。 唤醒 当线程释放同步状态后，则需要唤醒该线程的后继节点： public final boolean release(int arg) { if (tryRelease(arg)) { Node h = head; if (h != null && h.waitStatus != 0) //唤醒后继节点 unparkSuccessor(h); return true; } return false; } 调用unparkSuccessor(Node node)唤醒后继节点： private void unparkSuccessor(Node node) { //当前节点状态 int ws = node.waitStatus; //当前状态 0 (超时或者被中断了) if (s == null || s.waitStatus > 0) { s = null; //从tail节点来找可用节点 for (Node t = tail; t != null && t != node; t = t.prev) if (t.waitStatus 可能会存在当前线程的后继节点为null，超时、被中断的情况，如果遇到这种情况了，则需要跳过该节点，但是为何是从tail尾节点开始，而不是从node.next开始呢？原因在于node.next仍然可能会存在null或者取消了，所以采用tail回溯办法找第一个可用的线程。最后调用LockSupport的unpark(Thread thread)方法唤醒该线程。 LockSupport 从上面我可以看到，当需要阻塞或者唤醒一个线程的时候，AQS都是使用LockSupport这个工具类来完成的。 LockSupport是用来创建锁和其他同步类的基本线程阻塞原语 每个使用LockSupport的线程都会与一个许可关联，如果该许可可用，并且可在进程中使用，则调用park()将会立即返回，否则可能阻塞。如果许可尚不可用，则可以调用 unpark 使其可用。但是注意许可不可重入，也就是说只能调用一次park()方法，否则会一直阻塞。 LockSupport定义了一系列以park开头的方法来阻塞当前线程，unpark(Thread thread)方法来唤醒一个被阻塞的线程。如下： 方法的blocker参数，主要是用来标识当前线程在等待的对象，该对象主要用于问题排查和系统监控。 park方法和unpark(Thread thread)都是成对出现的，同时unpark必须要在park执行之后执行，当然并不是说没有不调用unpark线程就会一直阻塞，park有一个方法，它带了时间戳（parkNanos(long nanos)：为了线程调度禁用当前线程，最多等待指定的等待时间，除非许可可用）。 park()方法的源码如下： public static void park() { UNSAFE.park(false, 0L); } unpark(Thread thread)方法源码如下： public static void unpark(Thread thread) { if (thread != null) UNSAFE.unpark(thread); } 从上面可以看出，其内部的实现都是通过UNSAFE（sun.misc.Unsafe UNSAFE）来实现的，其定义如下： public native void park(boolean var1, long var2); public native void unpark(Object var1); 两个都是native本地方法。Unsafe 是一个比较危险的类，主要是用于执行低级别、不安全的方法集合。尽管这个类和所有的方法都是公开的（public），但是这个类的使用仍然受限，你无法在自己的java程序中直接使用该类，因为只有授信的代码才能获得该类的实例。 "},"多线程/aqs/2.基于AQS的并发工具.html":{"url":"多线程/aqs/2.基于AQS的并发工具.html","title":"9.基于AQS的并发工具.md","keywords":"","body":"一 ReentrantLock获取锁释放锁公平锁与非公平锁ReentrantLock与synchronized的区别二 condition等待队列等待通知总结Condition的应用三 ReentrantReadWriteLock写锁读锁锁降级四 CyclicBarrier实现分析应用场景应用示例五 countDownLatch实现分析await()countDown()总结应用示例一 ReentrantLock ReentrantLock，可重入锁，是一种递归无阻塞的同步机制。它可以等同于synchronized的使用，但是ReentrantLock提供了比synchronized更强大、灵活的锁机制，可以减少死锁发生的概率。 API介绍如下： 一个可重入的互斥锁定 Lock，它具有与使用 synchronized 方法和语句所访问的隐式监视器锁定相同的一些基本行为和语义，但功能更强大。ReentrantLock 将由最近成功获得锁定，并且还没有释放该锁定的线程所拥有。当锁定没有被另一个线程所拥有时，调用 lock 的线程将成功获取该锁定并返回。如果当前线程已经拥有该锁定，此方法将立即返回。可以使用 isHeldByCurrentThread() 和 getHoldCount() 方法来检查此情况是否发生。 ReentrantLock还提供了公平锁也非公平锁的选择，构造方法接受一个可选的公平参数（默认非公平锁），当设置为true时，表示公平锁，否则为非公平锁。公平锁与非公平锁的区别在于公平锁的锁获取是有顺序的。但是公平锁的效率往往没有非公平锁的效率高，在许多线程访问的情况下，公平锁表现出较低的吞吐量。 获取锁 我们一般都是这么使用ReentrantLock获取锁的： //非公平锁 ReentrantLock lock = new ReentrantLock(); lock.lock(); lock方法： public void lock() { sync.lock(); } Sync为ReentrantLock里面的一个内部类，它继承AQS（AbstractQueuedSynchronizer），它有两个子类：公平锁FairSync和非公平锁NonfairSync。 ReentrantLock里面大部分的功能都是委托给Sync来实现的，同时Sync内部定义了lock()抽象方法由其子类去实现，默认实现了nonfairTryAcquire(int acquires)方法，可以看出它是非公平锁的默认实现方式。下面我们看非公平锁的lock()方法： final void lock() { //尝试获取锁 if (compareAndSetState(0, 1)) setExclusiveOwnerThread(Thread.currentThread()); else //获取失败，调用AQS的acquire(int arg)方法 acquire(1); } 首先会第一次尝试快速获取锁，如果获取失败，则调用acquire(int arg)方法，该方法定义在AQS中，如下： public final void acquire(int arg) { if (!tryAcquire(arg) && acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) selfInterrupt(); } 这个方法首先调用tryAcquire(int arg)方法，在AQS中讲述过，tryAcquire(int arg)需要自定义同步组件提供实现，非公平锁实现如下： protected final boolean tryAcquire(int acquires) { return nonfairTryAcquire(acquires); } final boolean nonfairTryAcquire(int acquires) { //当前线程 final Thread current = Thread.currentThread(); //获取同步状态 int c = getState(); //state == 0,表示没有该锁处于空闲状态 if (c == 0) { //获取锁成功，设置为当前线程所有 if (compareAndSetState(0, acquires)) { setExclusiveOwnerThread(current); return true; } } //线程重入 //判断锁持有的线程是否为当前线程 else if (current == getExclusiveOwnerThread()) { int nextc = c + acquires; if (nextc 该方法主要逻辑：首先判断同步状态state == 0 ?，如果是表示该锁还没有被线程持有，直接通过CAS获取同步状态，如果成功返回true。如果state != 0，则判断当前线程是否为获取锁的线程，如果是则获取锁，成功返回true。成功获取锁的线程再次获取锁，这是增加了同步状态state,这里就是可重入的概念。 释放锁 获取同步锁后，使用完毕则需要释放锁，ReentrantLock提供了unlock释放锁： public void unlock() { sync.release(1); } unlock内部使用Sync的release(int arg)释放锁，release(int arg)是在AQS中定义的： public final boolean release(int arg) { if (tryRelease(arg)) { Node h = head; if (h != null && h.waitStatus != 0) unparkSuccessor(h); return true; } return false; } 与获取同步状态的acquire(int arg)方法相似，释放同步状态的tryRelease(int arg)同样是需要自定义同步组件自己实现： protected final boolean tryRelease(int releases) { //减掉releases int c = getState() - releases; //如果释放的不是持有锁的线程，抛出异常 if (Thread.currentThread() != getExclusiveOwnerThread()) throw new IllegalMonitorStateException(); boolean free = false; //state == 0 表示已经释放完全了，其他线程可以获取同步状态了 if (c == 0) { free = true; setExclusiveOwnerThread(null); } setState(c); return free; } 只有当同步状态彻底释放后该方法才会返回true。当state == 0 时，则将锁持有线程设置为null，free= true，表示释放成功。 公平锁与非公平锁 公平锁与非公平锁的区别在于获取锁的时候是否按照FIFO的顺序来。释放锁不存在公平性和非公平性，上面以非公平锁为例，下面我们来看看公平锁的tryAcquire(int arg)： protected final boolean tryAcquire(int acquires) { final Thread current = Thread.currentThread(); int c = getState(); if (c == 0) { if (!hasQueuedPredecessors() && compareAndSetState(0, acquires)) { setExclusiveOwnerThread(current); return true; } } else if (current == getExclusiveOwnerThread()) { int nextc = c + acquires; if (nextc 比较非公平锁和公平锁获取同步状态的过程，会发现两者唯一的区别就在于公平锁在获取同步状态时多了一个限制条件：hasQueuedPredecessors()，定义如下： public final boolean hasQueuedPredecessors() { Node t = tail; //尾节点 Node h = head; //头节点 Node s; //头节点 != 尾节点 //同步队列第一个节点不为null //当前线程是同步队列第一个节点 return h != t && ((s = h.next) == null || s.thread != Thread.currentThread()); } 该方法主要做一件事情：主要是判断当前线程是否位于CLH同步队列中的第一个。如果是则返回true，否则返回false。 ReentrantLock与synchronized的区别 前面提到ReentrantLock提供了比synchronized更加灵活和强大的锁机制，那么它的灵活和强大之处在哪里呢？他们之间又有什么相异之处呢？ 首先他们肯定具有相同的功能和内存语义。 与synchronized相比，ReentrantLock提供了更多，更加全面的功能，具备更强的扩展性。例如：时间锁等候，可中断锁等候，锁投票。 ReentrantLock还提供了条件Condition，对线程的等待、唤醒操作更加详细和灵活，所以在多个条件变量和高度竞争锁的地方，ReentrantLock更加适合（以后会阐述Condition）。 ReentrantLock提供了可轮询的锁请求。它会尝试着去获取锁，如果成功则继续，否则可以等到下次运行时处理，而synchronized则一旦进入锁请求要么成功要么阻塞，所以相比synchronized而言，ReentrantLock会不容易产生死锁些。 ReentrantLock支持更加灵活的同步代码块，但是使用synchronized时，只能在同一个synchronized块结构中获取和释放。注：ReentrantLock的锁释放一定要在finally中处理，否则可能会产生严重的后果。 ReentrantLock支持中断处理，且性能较synchronized会好些。 二 condition 在没有Lock之前，我们使用synchronized来控制同步，配合Object的wait()、notify()系列方法可以实现等待/通知模式。在Java SE5后，Java提供了Lock接口，相对于Synchronized而言，Lock提供了条件Condition，对线程的等待、唤醒操作更加详细和灵活。下图是Condition与Object的监视器方法的对比（摘自《Java并发编程的艺术》）： Condition提供了一系列的方法来对阻塞和唤醒线程： await() ：造成当前线程在接到信号或被中断之前一直处于等待状态。 await(long time, TimeUnit unit) ：造成当前线程在接到信号、被中断或到达指定等待时间之前一直处于等待状态。 awaitNanos(long nanosTimeout) ：造成当前线程在接到信号、被中断或到达指定等待时间之前一直处于等待状态。返回值表示剩余时间，如果在nanosTimesout之前唤醒，那么返回值 = nanosTimeout - 消耗时间，如果返回值 awaitUninterruptibly() ：造成当前线程在接到信号之前一直处于等待状态。【注意：该方法对中断不敏感】。 awaitUntil(Date deadline) ：造成当前线程在接到信号、被中断或到达指定最后期限之前一直处于等待状态。如果没有到指定时间就被通知，则返回true，否则表示到了指定时间，返回返回false。 signal() ：唤醒一个等待线程。该线程从等待方法返回前必须获得与Condition相关的锁。 signal()All ：唤醒所有等待线程。能够从等待方法返回的线程必须获得与Condition相关的锁。 Condition是一种广义上的条件队列。他为线程提供了一种更为灵活的等待/通知模式，线程在调用await方法后执行挂起操作，直到线程等待的某个条件为真时才会被唤醒。Condition必须要配合锁一起使用，因为对共享状态变量的访问发生在多线程环境下。一个Condition的实例必须与一个Lock绑定，因此Condition一般都是作为Lock的内部实现。 获取一个Condition必须要通过Lock的newCondition()方法。该方法定义在接口Lock下面，返回的结果是绑定到此 Lock 实例的新 Condition 实例。Condition为一个接口，其下仅有一个实现类ConditionObject，由于Condition的操作需要获取相关的锁，而AQS则是同步锁的实现基础，所以ConditionObject则定义为AQS的内部类。定义如下： public class ConditionObject implements Condition, java.io.Serializable { } 等待队列 每个Condition对象都包含着一个FIFO队列，该队列是Condition对象通知/等待功能的关键。在队列中每一个节点都包含着一个线程引用，该线程就是在该 Condition对象上等待的线程。我们看Condition的定义就明白了： public class ConditionObject implements Condition, java.io.Serializable { private static final long serialVersionUID = 1173984872572414699L; //头节点 private transient Node firstWaiter; //尾节点 private transient Node lastWaiter; public ConditionObject() { } /** 省略方法 **/ } 从上面代码可以看出Condition拥有首节点（firstWaiter），尾节点（lastWaiter）。当前线程调用await()方法，将会以当前线程构造成一个节点（Node），并将节点加入到该队列的尾部。结构如下： Node里面包含了当前线程的引用。Node定义与AQS的CLH同步队列的节点使用的都是同一个类（AbstractQueuedSynchronized.Node静态内部类）。 Condition的队列结构比CLH同步队列的结构简单些，新增过程较为简单只需要将原尾节点的nextWaiter指向新增节点，然后更新lastWaiter即可。 等待 调用Condition的await()方法会使当前线程进入等待状态，同时会加入到Condition等待队列同时释放锁。当从await()方法返回时，当前线程一定是获取了Condition相关连的锁。 public final void await() throws InterruptedException { // 当前线程中断 if (Thread.interrupted()) throw new InterruptedException(); //当前线程加入等待队列 Node node = addConditionWaiter(); //释放锁 long savedState = fullyRelease(node); int interruptMode = 0; /** * 检测此节点的线程是否在同步队上，如果不在，则说明该线程还不具备竞争锁的资格，则继续等待 * 直到检测到此节点在同步队列上 */ while (!isOnSyncQueue(node)) { //线程挂起 LockSupport.park(this); //如果已经中断了，则退出 if ((interruptMode = checkInterruptWhileWaiting(node)) != 0) break; } //竞争同步状态 if (acquireQueued(node, savedState) && interruptMode != THROW_IE) interruptMode = REINTERRUPT; //清理下条件队列中的不是在等待条件的节点 if (node.nextWaiter != null) // clean up if cancelled unlinkCancelledWaiters(); if (interruptMode != 0) reportInterruptAfterWait(interruptMode); } 此段代码的逻辑是：首先将当前线程新建一个节点同时加入到条件队列中，然后释放当前线程持有的同步状态。然后则是不断检测该节点代表的线程释放出现在CLH同步队列中（收到signal信号之后就会在AQS队列中检测到），如果不存在则一直挂起，否则参与竞争同步状态。 加入条件队列（addConditionWaiter()）源码如下： private Node addConditionWaiter() { Node t = lastWaiter; //尾节点 //Node的节点状态如果不为CONDITION，则表示该节点不处于等待状态，需要清除节点 if (t != null && t.waitStatus != Node.CONDITION) { //清除条件队列中所有状态不为Condition的节点 unlinkCancelledWaiters(); t = lastWaiter; } //当前线程新建节点，状态CONDITION Node node = new Node(Thread.currentThread(), Node.CONDITION); /** * 将该节点加入到条件队列中最后一个位置 */ if (t == null) firstWaiter = node; else t.nextWaiter = node; lastWaiter = node; return node; } 该方法主要是将当前线程加入到Condition条件队列中。当然在加入到尾节点之前会清楚所有状态不为Condition的节点。 fullyRelease(Node node)，负责释放该线程持有的锁。 final long fullyRelease(Node node) { boolean failed = true; try { //节点状态--其实就是持有锁的数量 long savedState = getState(); //释放锁 if (release(savedState)) { failed = false; return savedState; } else { throw new IllegalMonitorStateException(); } } finally { if (failed) node.waitStatus = Node.CANCELLED; } } isOnSyncQueue(Node node)：如果一个节点刚开始在条件队列上，现在在同步队列上获取锁则返回true final boolean isOnSyncQueue(Node node) { //状态为Condition，获取前驱节点为null，返回false if (node.waitStatus == Node.CONDITION || node.prev == null) return false; //后继节点不为null，肯定在CLH同步队列中 if (node.next != null) return true; return findNodeFromTail(node); } unlinkCancelledWaiters()：负责将条件队列中状态不为Condition的节点删除 private void unlinkCancelledWaiters() { Node t = firstWaiter; Node trail = null; while (t != null) { Node next = t.nextWaiter; if (t.waitStatus != Node.CONDITION) { t.nextWaiter = null; if (trail == null) firstWaiter = next; else trail.nextWaiter = next; if (next == null) lastWaiter = trail; } else trail = t; t = next; } } 通知 调用Condition的signal()方法，将会唤醒在等待队列中等待最长时间的节点（条件队列里的首节点），在唤醒节点前，会将节点移到CLH同步队列中。 public final void signal() { //检测当前线程是否为拥有锁的独 if (!isHeldExclusively()) throw new IllegalMonitorStateException(); //头节点，唤醒条件队列中的第一个节点 Node first = firstWaiter; if (first != null) doSignal(first); //唤醒 } 该方法首先会判断当前线程是否已经获得了锁，这是前置条件。然后唤醒条件队列中的头节点。 doSignal(Node first)：唤醒头节点 private void doSignal(Node first) { do { //修改头结点，完成旧头结点的移出工作 if ( (firstWaiter = first.nextWaiter) == null) lastWaiter = null; first.nextWaiter = null; } while (!transferForSignal(first) && (first = firstWaiter) != null); } doSignal(Node first)主要是做两件事：1.修改头节点，2.调用transferForSignal(Node first) 方法将节点移动到CLH同步队列中。transferForSignal(Node first)源码如下： final boolean transferForSignal(Node node) { //将该节点从状态CONDITION改变为初始状态0, if (!compareAndSetWaitStatus(node, Node.CONDITION, 0)) return false; //将节点加入到syn队列中去，返回的是syn队列中node节点前面的一个节点 Node p = enq(node); int ws = p.waitStatus; //如果结点p的状态为cancel 或者修改waitStatus失败，则直接唤醒 if (ws > 0 || !compareAndSetWaitStatus(p, ws, Node.SIGNAL)) LockSupport.unpark(node.thread); return true; } 整个通知的流程如下： 判断当前线程是否已经获取了锁，如果没有获取则直接抛出异常，因为获取锁为通知的前置条件。 如果线程已经获取了锁，则将唤醒条件队列的首节点 唤醒首节点是先将条件队列中的头节点移出，然后调用AQS的enq(Node node)方法将其安全地移到CLH同步队列中 最后判断如果该节点的同步状态是否为Cancel，或者修改状态为Signal失败时，则直接调用LockSupport唤醒该节点的线程。 总结 一个线程获取锁后，通过调用Condition的await()方法，会将当前线程先加入到条件队列中，然后释放锁，最后通过isOnSyncQueue(Node node)方法不断自检看节点是否已经在CLH同步队列了，如果是则尝试获取锁，否则一直挂起。当线程调用signal()方法后，程序首先检查当前线程是否获取了锁，然后通过doSignal(Node first)方法唤醒CLH同步队列的首节点。被唤醒的线程，将从await()方法中的while循环中退出来，然后调用acquireQueued()方法竞争同步状态。 Condition的应用 只知道原理，如果不知道使用那就坑爹了，下面是用Condition实现的生产者消费者问题： public class ConditionTest { private LinkedList buffer; //容器 private int maxSize ; //容器最大 private Lock lock; private Condition fullCondition; private Condition notFullCondition; ConditionTest(int maxSize){ this.maxSize = maxSize; buffer = new LinkedList(); lock = new ReentrantLock(); fullCondition = lock.newCondition(); notFullCondition = lock.newCondition(); } public void set(String string) throws InterruptedException { lock.lock(); //获取锁 try { while (maxSize == buffer.size()){ notFullCondition.await(); //满了，添加的线程进入等待状态 } buffer.add(string); fullCondition.signal(); } finally { lock.unlock(); //记得释放锁 } } public String get() throws InterruptedException { String string; lock.lock(); try { while (buffer.size() == 0){ fullCondition.await(); } string = buffer.poll(); notFullCondition.signal(); } finally { lock.unlock(); } return string; } } 三 ReentrantReadWriteLock 重入锁ReentrantLock是排他锁，排他锁在同一时刻仅有一个线程可以进行访问，但是在大多数场景下，大部分时间都是提供读服务，而写服务占有的时间较少。然而读服务不存在数据竞争问题，如果一个线程在读时禁止其他线程读势必会导致性能降低。所以就提供了读写锁。 读写锁维护着一对锁，一个读锁和一个写锁。通过分离读锁和写锁，使得并发性比一般的排他锁有了较大的提升：在同一时间可以允许多个读线程同时访问，但是在写线程访问时，所有读线程和写线程都会被阻塞。 读写锁的主要特性： 公平性：支持公平性和非公平性。 重入性：支持重入。读写锁最多支持65535个递归写入锁和65535个递归读取锁。 锁降级：遵循获取写锁、获取读锁在释放写锁的次序，写锁能够降级成为读锁 读写锁ReentrantReadWriteLock实现接口ReadWriteLock，该接口维护了一对相关的锁，一个用于只读操作，另一个用于写入操作。只要没有 writer，读取锁可以由多个 reader 线程同时保持。写入锁是独占的。 public interface ReadWriteLock { Lock readLock(); Lock writeLock(); } ReadWriteLock定义了两个方法。readLock()返回用于读操作的锁，writeLock()返回用于写操作的锁。ReentrantReadWriteLock定义如下： /** 内部类 读锁 */ private final ReentrantReadWriteLock.ReadLock readerLock; /** 内部类 写锁 */ private final ReentrantReadWriteLock.WriteLock writerLock; final Sync sync; /** 使用默认（非公平）的排序属性创建一个新的 ReentrantReadWriteLock */ public ReentrantReadWriteLock() { this(false); } /** 使用给定的公平策略创建一个新的 ReentrantReadWriteLock */ public ReentrantReadWriteLock(boolean fair) { sync = fair ? new FairSync() : new NonfairSync(); readerLock = new ReadLock(this); writerLock = new WriteLock(this); } /** 返回用于写入操作的锁 */ public ReentrantReadWriteLock.WriteLock writeLock() { return writerLock; } /** 返回用于读取操作的锁 */ public ReentrantReadWriteLock.ReadLock readLock() { return readerLock; } abstract static class Sync extends AbstractQueuedSynchronizer { /** * 省略其余源代码 */ } public static class WriteLock implements Lock, java.io.Serializable{ /** * 省略其余源代码 */ } public static class ReadLock implements Lock, java.io.Serializable { /** * 省略其余源代码 */ } ReentrantReadWriteLock与ReentrantLock一样，其锁主体依然是Sync，它的读锁、写锁都是依靠Sync来实现的。所以ReentrantReadWriteLock实际上只有一个锁，只是在获取读取锁和写入锁的方式上不一样而已，它的读写锁其实就是两个类：ReadLock、writeLock，这两个类都是lock实现。 在ReentrantLock中使用一个int类型的state来表示同步状态，该值表示锁被一个线程重复获取的次数。但是读写锁ReentrantReadWriteLock内部维护着两个一对锁，需要用一个变量维护多种状态。所以读写锁采用“按位切割使用”的方式来维护这个变量，将其切分为两部分，高16为表示读，低16为表示写。分割之后，读写锁是如何迅速确定读锁和写锁的状态呢？通过为运算。假如当前同步状态为S，那么写状态等于 S & 0x0000FFFF（将高16位全部抹去），读状态等于S >>> 16(无符号补0右移16位)。代码如下： static final int SHARED_SHIFT = 16; static final int SHARED_UNIT = (1 >> SHARED_SHIFT; } static int exclusiveCount(int c) { return c & EXCLUSIVE_MASK; } 写锁 写锁就是一个支持可重入的排他锁。 写锁的获取 写锁的获取最终会调用tryAcquire(int arg)，该方法在内部类Sync中实现： protected final boolean tryAcquire(int acquires) { Thread current = Thread.currentThread(); //当前锁个数 int c = getState(); //写锁 int w = exclusiveCount(c); if (c != 0) { //c != 0 && w == 0 表示存在读锁 //当前线程不是已经获取写锁的线程 if (w == 0 || current != getExclusiveOwnerThread()) return false; //超出最大范围 if (w + exclusiveCount(acquires) > MAX_COUNT) throw new Error(\"Maximum lock count exceeded\"); setState(c + acquires); return true; } //是否需要阻塞 if (writerShouldBlock() || !compareAndSetState(c, c + acquires)) return false; //设置获取锁的线程为当前线程 setExclusiveOwnerThread(current); return true; } 该方法和ReentrantLock的tryAcquire(int arg)大致一样，在判断重入时增加了一项条件：读锁是否存在。因为要确保写锁的操作对读锁是可见的，如果在存在读锁的情况下允许获取写锁，那么那些已经获取读锁的其他线程可能就无法感知当前写线程的操作。因此只有等读锁完全释放后，写锁才能够被当前线程所获取，一旦写锁获取了，所有其他读、写线程均会被阻塞。 写锁的释放 获取了写锁用完了则需要释放，WriteLock提供了unlock()方法释放写锁： public void unlock() { sync.release(1); } public final boolean release(int arg) { if (tryRelease(arg)) { Node h = head; if (h != null && h.waitStatus != 0) unparkSuccessor(h); return true; } return false; } 写锁的释放最终还是会调用AQS的模板方法release(int arg)方法，该方法首先调用tryRelease(int arg)方法尝试释放锁，tryRelease(int arg)方法为读写锁内部类Sync中定义了，如下： protected final boolean tryRelease(int releases) { //释放的线程不为锁的持有者 if (!isHeldExclusively()) throw new IllegalMonitorStateException(); int nextc = getState() - releases; //若写锁的新线程数为0，则将锁的持有者设置为null boolean free = exclusiveCount(nextc) == 0; if (free) setExclusiveOwnerThread(null); setState(nextc); return free; } 写锁释放锁的整个过程和独占锁ReentrantLock相似，每次释放均是减少写状态，当写状态为0时表示 写锁已经完全释放了，从而等待的其他线程可以继续访问读写锁，获取同步状态，同时此次写线程的修改对后续的线程可见。 读锁 读锁为一个可重入的共享锁，它能够被多个线程同时持有，在没有其他写线程访问时，读锁总是或获取成功。 读锁的获取 读锁的获取可以通过ReadLock的lock()方法： public void lock() { sync.acquireShared(1); } Sync的acquireShared(int arg)定义在AQS中： public final void acquireShared(int arg) { if (tryAcquireShared(arg) tryAcqurireShared(int arg)尝试获取读同步状态，该方法主要用于获取共享式同步状态，获取成功返回 >= 0的返回结果，否则返回 protected final int tryAcquireShared(int unused) { //当前线程 Thread current = Thread.currentThread(); int c = getState(); //exclusiveCount(c)计算写锁 //如果存在写锁，且锁的持有者不是当前线程，直接返回-1 //存在锁降级问题，后续阐述 if (exclusiveCount(c) != 0 && getExclusiveOwnerThread() != current) return -1; //读锁 int r = sharedCount(c); /* * readerShouldBlock():读锁是否需要等待（公平锁原则） * r 读锁获取的过程相对于独占锁而言会稍微复杂下，整个过程如下： 因为存在锁降级情况，如果存在写锁且锁的持有者不是当前线程则直接返回失败，否则继续 依据公平性原则，判断读锁是否需要阻塞，读锁持有线程数小于最大值（65535），且设置锁状态成功，执行以下代码（对于HoldCounter下面再阐述），并返回1。如果不满足改条件，执行fullTryAcquireShared()。 final int fullTryAcquireShared(Thread current) { HoldCounter rh = null; for (;;) { int c = getState(); //锁降级 if (exclusiveCount(c) != 0) { if (getExclusiveOwnerThread() != current) return -1; } //读锁需要阻塞 else if (readerShouldBlock()) { //列头为当前线程 if (firstReader == current) { } //HoldCounter后面讲解 else { if (rh == null) { rh = cachedHoldCounter; if (rh == null || rh.tid != getThreadId(current)) { rh = readHolds.get(); if (rh.count == 0) readHolds.remove(); } } if (rh.count == 0) return -1; } } //读锁超出最大范围 if (sharedCount(c) == MAX_COUNT) throw new Error(\"Maximum lock count exceeded\"); //CAS设置读锁成功 if (compareAndSetState(c, c + SHARED_UNIT)) { //如果是第1次获取“读取锁”，则更新firstReader和firstReaderHoldCount if (sharedCount(c) == 0) { firstReader = current; firstReaderHoldCount = 1; } //如果想要获取锁的线程(current)是第1个获取锁(firstReader)的线程,则将firstReaderHoldCount+1 else if (firstReader == current) { firstReaderHoldCount++; } else { if (rh == null) rh = cachedHoldCounter; if (rh == null || rh.tid != getThreadId(current)) rh = readHolds.get(); else if (rh.count == 0) readHolds.set(rh); //更新线程的获取“读取锁”的共享计数 rh.count++; cachedHoldCounter = rh; // cache for release } return 1; } } } fullTryAcquireShared(Thread current)会根据“是否需要阻塞等待”，“读取锁的共享计数是否超过限制”等等进行处理。如果不需要阻塞等待，并且锁的共享计数没有超过限制，则通过CAS尝试获取锁，并返回1 读锁的释放 与写锁相同，读锁也提供了unlock()释放读锁： public void unlock() { sync.releaseShared(1); } unlcok()方法内部使用Sync的releaseShared(int arg)方法，该方法定义在AQS中： public final boolean releaseShared(int arg) { if (tryReleaseShared(arg)) { doReleaseShared(); return true; } return false; } 调用tryReleaseShared(int arg)尝试释放读锁，该方法定义在读写锁的Sync内部类中： protected final boolean tryReleaseShared(int unused) { Thread current = Thread.currentThread(); //如果想要释放锁的线程为第一个获取锁的线程 if (firstReader == current) { //仅获取了一次，则需要将firstReader 设置null，否则 firstReaderHoldCount - 1 if (firstReaderHoldCount == 1) firstReader = null; else firstReaderHoldCount--; } //获取rh对象，并更新“当前线程获取锁的信息” else { HoldCounter rh = cachedHoldCounter; if (rh == null || rh.tid != getThreadId(current)) rh = readHolds.get(); int count = rh.count; if (count HoldCounter 在读锁获取锁和释放锁的过程中，我们一直都可以看到一个变量rh （HoldCounter ），该变量在读锁中扮演着非常重要的作用。 我们了解读锁的内在机制其实就是一个共享锁，为了更好理解HoldCounter ，我们暂且认为它不是一个锁的概率，而相当于一个计数器。一次共享锁的操作就相当于在该计数器的操作。获取共享锁，则该计数器 + 1，释放共享锁，该计数器 - 1。只有当线程获取共享锁后才能对共享锁进行释放、重入操作。所以HoldCounter的作用就是当前线程持有共享锁的数量，这个数量必须要与线程绑定在一起，否则操作其他线程锁就会抛出异常。我们先看HoldCounter的定义： static final class HoldCounter { int count = 0; final long tid = getThreadId(Thread.currentThread()); } HoldCounter 定义非常简单，就是一个计数器count 和线程 id tid 两个变量。按照这个意思我们看到HoldCounter 是需要和某给线程进行绑定了，我们知道如果要将一个对象和线程绑定仅仅有tid是不够的，而且从上面的代码我们可以看到HoldCounter 仅仅只是记录了tid，根本起不到绑定线程的作用。那么怎么实现呢？答案是ThreadLocal，定义如下： static final class ThreadLocalHoldCounter extends ThreadLocal { public HoldCounter initialValue() { return new HoldCounter(); } } 通过上面代码HoldCounter就可以与线程进行绑定了。故而，HoldCounter应该就是绑定线程上的一个计数器，而ThradLocalHoldCounter则是线程绑定的ThreadLocal。从上面我们可以看到ThreadLocal将HoldCounter绑定到当前线程上，同时HoldCounter也持有线程Id，这样在释放锁的时候才能知道ReadWriteLock里面缓存的上一个读取线程（cachedHoldCounter）是否是当前线程。这样做的好处是可以减少ThreadLocal.get()的次数，因为这也是一个耗时操作。需要说明的是这样HoldCounter绑定线程id而不绑定线程对象的原因是避免HoldCounter和ThreadLocal互相绑定而GC难以释放它们（尽管GC能够智能的发现这种引用而回收它们，但是这需要一定的代价），所以其实这样做只是为了帮助GC快速回收对象而已。 看到这里我们明白了HoldCounter作用了，我们在看一个获取读锁的代码段： else if (firstReader == current) { firstReaderHoldCount++; } else { if (rh == null) rh = cachedHoldCounter; if (rh == null || rh.tid != getThreadId(current)) rh = readHolds.get(); else if (rh.count == 0) readHolds.set(rh); rh.count++; cachedHoldCounter = rh; // cache for release } 这段代码涉及了几个变量：firstReader 、firstReaderHoldCount、cachedHoldCounter 。我们先理清楚这几个变量： private transient Thread firstReader = null; private transient int firstReaderHoldCount; private transient HoldCounter cachedHoldCounter; firstReader 看名字就明白了为第一个获取读锁的线程，firstReaderHoldCount为第一个获取读锁的重入数，cachedHoldCounter为HoldCounter的缓存。 理清楚上面所有的变量了，HoldCounter也明白了，我们就来给上面那段代码标明注释，如下： //如果获取读锁的线程为第一次获取读锁的线程，则firstReaderHoldCount重入数 + 1 else if (firstReader == current) { firstReaderHoldCount++; } else { //非firstReader计数 if (rh == null) rh = cachedHoldCounter; //rh == null 或者 rh.tid != current.getId()，需要获取rh if (rh == null || rh.tid != getThreadId(current)) rh = readHolds.get(); //加入到readHolds中 else if (rh.count == 0) readHolds.set(rh); //计数+1 rh.count++; cachedHoldCounter = rh; // cache for release } 这里解释下为何要引入firstRead、firstReaderHoldCount。这是为了一个效率问题，firstReader是不会放入到readHolds中的，如果读锁仅有一个的情况下就会避免查找readHolds。 锁降级 上开篇是LZ就阐述了读写锁有一个特性就是锁降级，锁降级就意味着写锁是可以降级为读锁的，但是需要遵循先获取写锁、获取读锁在释放写锁的次序。注意如果当前线程先获取写锁，然后释放写锁，再获取读锁这个过程不能称之为锁降级，锁降级一定要遵循那个次序。 在获取读锁的方法tryAcquireShared(int unused)中，有一段代码就是来判读锁降级的： int c = getState(); //exclusiveCount(c)计算写锁 //如果存在写锁，且锁的持有者不是当前线程，直接返回-1 //存在锁降级问题，后续阐述 if (exclusiveCount(c) != 0 && getExclusiveOwnerThread() != current) return -1; //读锁 int r = sharedCount(c); 锁降级中读锁的获取释放为必要？肯定是必要的。试想，假如当前线程A不获取读锁而是直接释放了写锁，这个时候另外一个线程B获取了写锁，那么这个线程B对数据的修改是不会对当前线程A可见的。如果获取了读锁，则线程B在获取写锁过程中判断如果有读锁还没有释放则会被阻塞，只有当前线程A释放读锁后，线程B才会获取写锁成功。 四 CyclicBarrier CyclicBarrier，一个同步辅助类，在API中是这么介绍的： 它允许一组线程互相等待，直到到达某个公共屏障点 (common barrier point)。在涉及一组固定大小的线程的程序中，这些线程必须不时地互相等待，此时 CyclicBarrier 很有用。因为该 barrier 在释放等待线程后可以重用，所以称它为循环 的 barrier。 通俗点讲就是：让一组线程到达一个屏障时被阻塞，直到最后一个线程到达屏障时，屏障才会开门，所有被屏障拦截的线程才会继续干活。 实现分析 CyclicBarrier的结构如下： 通过上图我们可以看到CyclicBarrier的内部是使用重入锁ReentrantLock和Condition。它有两个构造函数： CyclicBarrier(int parties)：创建一个新的 CyclicBarrier，它将在给定数量的参与者（线程）处于等待状态时启动，但它不会在启动 barrier 时执行预定义的操作。 CyclicBarrier(int parties, Runnable barrierAction) ：创建一个新的 CyclicBarrier，它将在给定数量的参与者（线程）处于等待状态时启动，并在启动 barrier 时执行给定的屏障操作，该操作由最后一个进入 barrier 的线程执行。 parties表示拦截线程的数量。 barrierAction 为CyclicBarrier接收的Runnable命令，用于在线程到达屏障时，优先执行barrierAction ，用于处理更加复杂的业务场景。 public CyclicBarrier(int parties, Runnable barrierAction) { if (parties 在CyclicBarrier中最重要的方法莫过于await()方法，在所有参与者都已经在此 barrier 上调用 await 方法之前，将一直等待。如下： public int await() throws InterruptedException, BrokenBarrierException { try { return dowait(false, 0L);//不超时等待 } catch (TimeoutException toe) { throw new Error(toe); // cannot happen } } await()方法内部调用dowait(boolean timed, long nanos)方法： private int dowait(boolean timed, long nanos) throws InterruptedException, BrokenBarrierException, TimeoutException { //获取锁 final ReentrantLock lock = this.lock; lock.lock(); try { //分代 final Generation g = generation; //当前generation“已损坏”，抛出BrokenBarrierException异常 //抛出该异常一般都是某个线程在等待某个处于“断开”状态的CyclicBarrie if (g.broken) //当某个线程试图等待处于断开状态的 barrier 时，或者 barrier 进入断开状态而线程处于等待状态时，抛出该异常 throw new BrokenBarrierException(); //如果线程中断，终止CyclicBarrier if (Thread.interrupted()) { breakBarrier(); throw new InterruptedException(); } //进来一个线程 count - 1 int index = --count; //count == 0 表示所有线程均已到位，触发Runnable任务 if (index == 0) { // tripped boolean ranAction = false; try { final Runnable command = barrierCommand; //触发任务 if (command != null) command.run(); ranAction = true; //唤醒所有等待线程，并更新generation nextGeneration(); return 0; } finally { if (!ranAction) breakBarrier(); } } for (;;) { try { //如果不是超时等待，则调用Condition.await()方法等待 if (!timed) trip.await(); else if (nanos > 0L) //超时等待，调用Condition.awaitNanos()方法等待 nanos = trip.awaitNanos(nanos); } catch (InterruptedException ie) { if (g == generation && ! g.broken) { breakBarrier(); throw ie; } else { // We're about to finish waiting even if we had not // been interrupted, so this interrupt is deemed to // \"belong\" to subsequent execution. Thread.currentThread().interrupt(); } } if (g.broken) throw new BrokenBarrierException(); //generation已经更新，返回index if (g != generation) return index; //“超时等待”，并且时间已到,终止CyclicBarrier，并抛出异常 if (timed && nanos 其实await()的处理逻辑还是比较简单的：如果该线程不是到达的最后一个线程，则他会一直处于等待状态，除非发生以下情况： 最后一个线程到达，即index == 0 超出了指定时间（超时等待） 其他的某个线程中断当前线程 其他的某个线程中断另一个等待的线程 其他的某个线程在等待barrier超时 其他的某个线程在此barrier调用reset()方法。reset()方法用于将屏障重置为初始状态。 在上面的源代码中，我们可能需要注意Generation 对象，在上述代码中我们总是可以看到抛出BrokenBarrierException异常，那么什么时候抛出异常呢？如果一个线程处于等待状态时，如果其他线程调用reset()，或者调用的barrier原本就是被损坏的，则抛出BrokenBarrierException异常。同时，任何线程在等待时被中断了，则其他所有线程都将抛出BrokenBarrierException异常，并将barrier置于损坏状态。 同时，Generation描述着CyclicBarrier的更显换代。在CyclicBarrier中，同一批线程属于同一代。当有parties个线程到达barrier，generation就会被更新换代。其中broken标识该当前CyclicBarrier是否已经处于中断状态。 private static class Generation { boolean broken = false; } 默认barrier是没有损坏的。 当barrier损坏了或者有一个线程中断了，则通过breakBarrier()来终止所有的线程： private void breakBarrier() { generation.broken = true; count = parties; trip.signalAll(); } 在breakBarrier()中除了将broken设置为true，还会调用signalAll将在CyclicBarrier处于等待状态的线程全部唤醒。 当所有线程都已经到达barrier处（index == 0），则会通过nextGeneration()进行更新换地操作，在这个步骤中，做了三件事：唤醒所有线程，重置count，generation。 private void nextGeneration() { trip.signalAll(); count = parties; generation = new Generation(); } CyclicBarrier同时也提供了await(long timeout, TimeUnit unit) 方法来做超时控制，内部还是通过调用doawait()实现的。 应用场景 CyclicBarrier试用与多线程结果合并的操作，用于多线程计算数据，最后合并计算结果的应用场景。比如我们需要统计多个Excel中的数据，然后等到一个总结果。我们可以通过多线程处理每一个Excel，执行完成后得到相应的结果，最后通过barrierAction来计算这些线程的计算结果，得到所有Excel的总和。 应用示例 比如我们开会只有等所有的人到齐了才会开会，如下： public class CyclicBarrierTest { private static CyclicBarrier cyclicBarrier; static class CyclicBarrierThread extends Thread{ public void run() { System.out.println(Thread.currentThread().getName() + \"到了\"); //等待 try { cyclicBarrier.await(); } catch (Exception e) { e.printStackTrace(); } } } public static void main(String[] args){ cyclicBarrier = new CyclicBarrier(5, new Runnable() { @Override public void run() { System.out.println(\"人到齐了，开会吧....\"); } }); for(int i = 0 ; i 五 countDownLatch CyclicBarrier所描述的是“允许一组线程互相等待，直到到达某个公共屏障点，才会进行后续任务\"，而CountDownLatch所描述的是”在完成一组正在其他线程中执行的操作之前，它允许一个或多个线程一直等待“。在API中是这样描述的： 用给定的计数 初始化 CountDownLatch。由于调用了 countDown() 方法，所以在当前计数到达零之前，await 方法会一直受阻塞。之后，会释放所有等待的线程，await 的所有后续调用都将立即返回。这种现象只出现一次——计数无法被重置。如果需要重置计数，请考虑使用 CyclicBarrier。 CountDownLatch是通过一个计数器来实现的，当我们在new 一个CountDownLatch对象的时候需要带入该计数器值，该值就表示了线程的数量。每当一个线程完成自己的任务后，计数器的值就会减1。当计数器的值变为0时，就表示所有的线程均已经完成了任务，然后就可以恢复等待的线程继续执行了。 虽然，CountDownlatch与CyclicBarrier有那么点相似，但是他们还是存在一些区别的： CountDownLatch的作用是允许1或N个线程等待其他线程完成执行；而CyclicBarrier则是允许N个线程相互等待 CountDownLatch的计数器无法被重置；CyclicBarrier的计数器可以被重置后使用，因此它被称为是循环的barrier 实现分析 CountDownLatch结构如下 通过上面的结构图我们可以看到，CountDownLatch内部依赖Sync实现，而Sync继承AQS。CountDownLatch仅提供了一个构造方法： CountDownLatch(int count) ： 构造一个用给定计数初始化的 CountDownLatch public CountDownLatch(int count) { if (count sync为CountDownLatch的一个内部类，其定义如下： private static final class Sync extends AbstractQueuedSynchronizer { private static final long serialVersionUID = 4982264981922014374L; Sync(int count) { setState(count); } //获取同步状态 int getCount() { return getState(); } //获取同步状态 protected int tryAcquireShared(int acquires) { return (getState() == 0) ? 1 : -1; } //释放同步状态 protected boolean tryReleaseShared(int releases) { for (;;) { int c = getState(); if (c == 0) return false; int nextc = c-1; if (compareAndSetState(c, nextc)) return nextc == 0; } } } 通过这个内部类Sync我们可以清楚地看到CountDownLatch是采用共享锁来实现的。 await() CountDownLatch提供await()方法来使当前线程在锁存器倒计数至零之前一直等待，除非线程被中断，定义如下： public void await() throws InterruptedException { sync.acquireSharedInterruptibly(1); } await其内部使用AQS的acquireSharedInterruptibly(int arg)： public final void acquireSharedInterruptibly(int arg) throws InterruptedException { if (Thread.interrupted()) throw new InterruptedException(); if (tryAcquireShared(arg) 在内部类Sync中重写了tryAcquireShared(int arg)方法： protected int tryAcquireShared(int acquires) { return (getState() == 0) ? 1 : -1; } getState()获取同步状态，其值等于计数器的值，从这里我们可以看到如果计数器值不等于0，则会调用doAcquireSharedInterruptibly(int arg)，该方法为一个自旋方法会尝试一直去获取同步状态： private void doAcquireSharedInterruptibly(int arg) throws InterruptedException { final Node node = addWaiter(Node.SHARED); boolean failed = true; try { for (;;) { final Node p = node.predecessor(); if (p == head) { /** * 对于CountDownLatch而言，如果计数器值不等于0，那么r 会一直小于0 */ int r = tryAcquireShared(arg); if (r >= 0) { setHeadAndPropagate(node, r); p.next = null; // help GC failed = false; return; } } //等待 if (shouldParkAfterFailedAcquire(p, node) && parkAndCheckInterrupt()) throw new InterruptedException(); } } finally { if (failed) cancelAcquire(node); } } countDown() CountDownLatch提供countDown() 方法递减锁存器的计数，如果计数到达零，则释放所有等待的线程。 public void countDown() { sync.releaseShared(1); } 内部调用AQS的releaseShared(int arg)方法来释放共享锁同步状态： public final boolean releaseShared(int arg) { if (tryReleaseShared(arg)) { doReleaseShared(); return true; } return false; } tryReleaseShared(int arg)方法被CountDownLatch的内部类Sync重写： protected boolean tryReleaseShared(int releases) { for (;;) { //获取锁状态 int c = getState(); //c == 0 直接返回，释放锁成功 if (c == 0) return false; //计算新“锁计数器” int nextc = c-1; //更新锁状态（计数器） if (compareAndSetState(c, nextc)) return nextc == 0; } } 总结 CountDownLatch内部通过共享锁实现。在创建CountDownLatch实例时，需要传递一个int型的参数：count，该参数为计数器的初始值，也可以理解为该共享锁可以获取的总次数。当某个线程调用await()方法，程序首先判断count的值是否为0，如果不会0的话则会一直等待直到为0为止。当其他线程调用countDown()方法时，则执行释放共享锁状态，使count值 - 1。当在创建CountDownLatch时初始化的count参数，必须要有count线程调用countDown方法才会使计数器count等于0，锁才会释放，前面等待的线程才会继续运行。注意CountDownLatch不能回滚重置。 应用示例 示例仍然使用开会案例。老板进入会议室等待5个人全部到达会议室才会开会。所以这里有两个线程老板等待开会线程、员工到达会议室： public class CountDownLatchTest { private static CountDownLatch countDownLatch = new CountDownLatch(5); /** * Boss线程，等待员工到达开会 */ static class BossThread extends Thread{ @Override public void run() { System.out.println(\"Boss在会议室等待，总共有\" + countDownLatch.getCount() + \"个人开会...\"); try { //Boss等待 countDownLatch.await(); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(\"所有人都已经到齐了，开会吧...\"); } } //员工到达会议室 static class EmpleoyeeThread extends Thread{ @Override public void run() { System.out.println(Thread.currentThread().getName() + \"，到达会议室....\"); //员工到达会议室 count - 1 countDownLatch.countDown(); } } public static void main(String[] args){ //Boss线程启动 new BossThread().start(); for(int i = 0,j = countDownLatch.getCount() ; i 运行结果： "},"多线程/blockingQueue/1.阻塞队列.html":{"url":"多线程/blockingQueue/1.阻塞队列.html","title":"10.阻塞队列.md","keywords":"","body":"阻塞队列是什么？为什么要使用阻塞队列?阻塞队列的主要方法重点方法重点介绍常见的阻塞队列ArrayBlockingQueueLinkedBlockingQueue两者对比用阻塞队列实现生产者消费者阻塞队列是什么？ 首先了解队列，队列是数据先进先出的一种数据结构。阻塞队列，关键字是阻塞，先理解阻塞的含义，在阻塞队列中，线程阻塞有这样的两种情况： 1.当阻塞队列为空时，获取队列元素的线程将等待，直到该则塞队列非空；2.当阻塞队列变满时，使用该阻塞队列的线程会等待，直到该阻塞队列变成非满。 为什么要使用阻塞队列? 在常见的情况下，生产者消费者模式需要用到队列，生产者线程生产数据，放进队列，然后消费从队列中获取数据，这个在单线程的情况下没有问题。但是当多线程的情况下，某个特定时间下，（峰值高并发）出现消费者速度远大于生产者速度，消费者必须阻塞来等待生产者，以保证生产者能够生产出新的数据；当生产者速度远大于消费者速度时，同样也是一个道理。这些情况都要程序员自己控制阻塞，同时又要线程安全和运行效率。 阻塞队列的出现使得程序员不需要关心这些细节，比如什么时候阻塞线程，什么时候唤醒线程，这些都由阻塞队列完成了。 阻塞队列的主要方法 阻塞队列的方法，在不能立即满足但可能在将来某一时刻满足的情况下，按处理方式可以分为三类： 抛出异常：抛出一个异常； 特殊值：返回一个特殊值（null或false,视情况而定） 则塞：在成功操作之前，一直阻塞线程 超时：放弃前只在最大的时间内阻塞 工欲善其事必先利其器，学会用阻塞队列，必须要知道它有哪些方法，怎么用，有哪些注意事项，这样到真正使用的时候，就能少踩雷了。 首先介绍插入操作： 1.public abstract boolean add(E paramE); 将指定元素插入此队列中（如果立即可行且不会违反容量限制），成功时返回 true，如果当前没有可用的空间，则抛出 IllegalStateException。 如果该元素是NULL，则会抛出NullPointerException异常。 2.public abstract boolean offer(E paramE); 将指定元素插入此队列中（如果立即可行且不会违反容量限制），成功时返回 true，如果当前没有可用的空间，则返回 false。 3.public abstract void put(E paramE) throws InterruptedException; 将指定元素插入此队列中，将等待可用的空间（如果有必要） 4.offer(E o, long timeout, TimeUnit unit) 可以设定等待的时间，如果在指定的时间内，还不能往队列中加入BlockingQueue，则返回失败。 获取数据操作： 1.poll(time):取走BlockingQueue里排在首位的对象,若不能立即取出,则可以等time参数规定的时间,取不到时返回null; 2.poll(long timeout, TimeUnit unit)：从BlockingQueue取出一个队首的对象，如果在指定时间内，队列一旦有数据可取，则立即返回队列中的数据。否则知道时间 超时还没有数据可取，返回失败。 3.take():取走BlockingQueue里排在首位的对象,若BlockingQueue为空,阻断进入等待状态直到BlockingQueue有新的数据被加入; 4.drainTo():一次性从BlockingQueue获取所有可用的数据对象（还可以指定获取数据的个数），通过该方法，可以提升获取数据效率；不需要多次分批加锁或释放锁。 重点方法重点介绍 首先来看put方法 public void put(E paramE) throws InterruptedException { checkNotNull(paramE); ReentrantLock localReentrantLock = this.lock; localReentrantLock.lockInterruptibly(); try { while (this.count == this.items.length) this.notFull.await(); enqueue(paramE); localReentrantLock.unlock(); } finally { localReentrantLock.unlock(); } } 一行一行来看代码，首先进行空校验。checkNotNull(paramE); private static void checkNotNull(Object paramObject) { if (paramObject != null) return; throw new NullPointerException(); } 这是一个私有方法，需要注意的就是如果put的参数为空，则抛出空指针异常。（这个很值得我们学习，先进行空校验，在维护的时候就很容易定位错误），接着 ReentrantLock localReentrantLock = this.lock;实例化锁，这个ReentrantLock 在我之前的博客中也介绍过，可以共同探讨一下。 下一行localReentrantLock.lockInterruptibly();这里特别强调一下： lockInterruptibly()允许在等待时由其他线程的Thread.interrupt()方法来中断等待线程而直接返回，这时是不用获取锁的，而会抛出一个InterruptException。而ReentrantLock.lock()方法则不允许Thread.interrupt()中断，即使检测到了Thread.interruptted一样会继续尝试获取锁，失败则继续休眠。只是在最后获取锁成功之后在把当前线程置为interrupted状态。 注意这里已经锁住，每次进行此操作时时候只有一个线程，回到代码中，接着进行 while (this.count == this.items.length) this.notFull.await(); 这里向我们说明了一个信息，当队列满的时候，将会等待。这里使用了private final Condition notFull;这个实例化的Condition,这个Condition用来控制队列满的等待。 接着执行了enqueue(paramE)方法，进入这个方法来继续看 private void enqueue(E paramE) { Object[] arrayOfObject = this.items; arrayOfObject[this.putIndex] = paramE; if (++this.putIndex == arrayOfObject.length) this.putIndex = 0; this.count += 1; this.notEmpty.signal(); } 来看第一行，Object[] arrayOfObject = this.items;这个items是在构造器时候实例化的，final Object[] items = new Object[paramInt]；将item赋值到arrayObject中 继续 arrayOfObject[this.putIndex] = paramE;将put方法传入的参数赋值到arrayOfObject中，这里其实是items也改变了，因为java是值引用的缘故。 if (++this.putIndex == arrayOfObject.length) this.putIndex = 0; 如果这个偏移值+1之后等于数组的长度，那么偏移值变为0。this.count += 1;count值加1；这个count代表数组的总数。this.notEmpty.signal();唤醒被Condition notEmpty阻塞的方法，最后 localReentrantLock.unlock();解锁（这个操作不能够忘了） 这里不禁要问，是什么方法被阻塞了呢？带着这个疑问来看take方法。 public E take() throws InterruptedException { ReentrantLock localReentrantLock = this.lock; localReentrantLock.lockInterruptibly(); try { while (this.count == 0) this.notEmpty.await(); Object localObject1 = dequeue(); return localObject1; } finally { localReentrantLock.unlock(); } } 首先看前两行，和put方法一样先上锁，使得每次持有本段代码的时候只有一个线程。 while (this.count == 0) this.notEmpty.await(); 当数组的数量为空时，也就是无任何数据供区出来的时候，notEmpty这个Condition就会进行阻塞，知道被notEmpty唤醒，还记得上文提到的吗。就是在put方法中唤醒的，这里可以发现，只要成功进行一个put操作，就会唤醒一次。 继续看代码，接着执行Object localObject1 = dequeue();获取元素，跟进dequeue()方法继续： private E dequeue() { Object[] arrayOfObject = this.items; Object localObject = arrayOfObject[this.takeIndex]; arrayOfObject[this.takeIndex] = null; if (++this.takeIndex == arrayOfObject.length) this.takeIndex = 0; this.count -= 1; if (this.itrs != null) this.itrs.elementDequeued(); this.notFull.signal(); return localObject; } Object[] arrayOfObject = this.items;进行值传递操作，takeIndex是取元素的时候的偏移值，由此可见，put和take操作的偏移量分别是由putIndex和takeIndex控制的。 Object localObject = arrayOfObject[this.takeIndex];取出在数组中的数据，然后 arrayOfObject[this.takeIndex] = null;将原来位置的数据b变成null. if (++this.takeIndex == arrayOfObject.length) this.takeIndex = 0; 如果当前的++takeIndex等于该数组的长度，则takeIndex赋值0，结合put方法，这两个操作是用数组形成队列操作。接着唤醒持有notFull这个Condition的线程。 方法就总结到这里，其实看put和take是有很多相似之处的，继续看下一章节。 常见的阻塞队列 首先来看这张图，这个是阻塞队列的继承图（双端队列，没有列出来，没有太大区别） 主要有ArrayBlockingQueue,LinkedBlockingQueue,PriorityBlockingQueue,SynchronousQueue,DelayQueue这个五个实现类。 在这五个阻塞队列中，比较常用的是ArrayBlockingQueue，LinkedBlockingQueue，本文也会重点介绍这两个类。 ArrayBlockingQueue 在上面的源码分析中就是分析的ArrayBlockingQueue的源码。数组阻塞队列必须传入的参数是数组大小，还可以指定是否公平性。公平性就是当队列可用时，线程访问队列的顺序按照它排队时候的顺序，非公平锁则不按照这样的顺序，但是非公平队列要比公平队列执行的速度快。 继续看ArrayBlockingQueue其实是一个数组有界队列，此队列按照先进先出的原则维护数组中的元素顺序，看源码可知，是由两个整形变量（上文提到的putIndex和takeIndex）分别指着头和尾的位置。 LinkedBlockingQueue LinkedBlockingQueue是基于链表的阻塞队列，内部维持的数据缓冲队列是由链表组成的，也是按照先进先出的原则。 如果构造一个LinkedBlockingQueue对象，而没有指定其容量大小，LinkedBlockingQueue会默认一个类似无限大小（Integer.Max_VALUE）的容量，这样的话，如果生产者的速度一旦大于消费者的速度，也许还没有等到队列满阻塞产生，系统内存就有可能已经被消耗殆尽了。 LinkedBlockingQueue之所以能够高效的处理并发数据，是因为take()方法和put(E param)方法使用了不同的可重入锁，分别为private final ReentrantLock putLock和private final ReentrantLock takeLock，这也意味着在高并发的情况下生产者和消费者可以并行地操作队列中的数据，以此来提高整个队列的并发性能。 两者对比 1.ArrayBlockingQueue在put,take操作使用了同一个锁，两者操作不能同时进行，而LinkedBlockingQueue使用了不同的锁，put操作和take操作可同时进行。 2.ArrayBlockingQueue和LinkedBlockingQueue间还有一个明显的不同之处在于，前者在插入或删除元素时不会产生或销毁任何额外的对象实例，而后者则会生成一个额外的Node对象，这在长时间内需要高效并发地处理大批量数据的系统中，其对于GC的影响还是存在一定的区别。 其他还有优先级阻塞队列：PriorityBlockingQueue，延时队列：DelayQueue，**SynchronousQueue等，**因为使用频率较低，这里就不重点介绍了，有兴趣的读者可以深入研究。 用阻塞队列实现生产者消费者 模拟洗盘子的经历，洗碗工洗好一个盘子放在工作台上，然后厨师看到工作台上有空余的盘子，便使用盘子。写到代码里就是洗碗工就是一个生产者线程，厨师就是消费者线程，工作台就是阻塞队列。 public class TestBlockingQueue { /** * 生产和消费业务操作 * * @author tang * */ protected class WorkDesk { BlockingQueue desk = new LinkedBlockingQueue(10); public void washDish() throws InterruptedException { desk.put(\"洗好一个盘子\"); } public String useDish() throws InterruptedException { return desk.take(); } } /** * 生产者类 * * @author tang * */ class Producer implements Runnable { private String producerName; private WorkDesk workDesk; public Producer(String producerName, WorkDesk workDesk) { this.producerName = producerName; this.workDesk = workDesk; } @Override public void run() { try { for (;;) { System.out.println(producerName + \"洗好一个盘子\"); workDesk.washDish(); Thread.sleep(1000); } } catch (Exception e) { e.printStackTrace(); } } } /** * 消费者类 * * @author tang * */ class Consumer implements Runnable { private String consumerName; private WorkDesk workDesk; public Consumer(String consumerName, WorkDesk workDesk) { this.consumerName = consumerName; this.workDesk = workDesk; } @Override public void run() { try { for (;;) { System.out.println(consumerName + \"使用一个盘子\"); workDesk.useDish(); Thread.sleep(1000); } } catch (Exception e) { e.printStackTrace(); } } } public static void main(String args[]) throws InterruptedException { TestBlockingQueue testQueue = new TestBlockingQueue(); WorkDesk workDesk = testQueue.new WorkDesk(); ExecutorService service = Executors.newCachedThreadPool(); //四个生产者线程 Producer producer1 = testQueue.new Producer(\"生产者-1-\", workDesk); Producer producer2 = testQueue.new Producer(\"生产者-2-\", workDesk); Producer producer3 = testQueue.new Producer(\"生产者-3-\", workDesk); Producer producer4 = testQueue.new Producer(\"生产者-4-\", workDesk); //两个消费者线程 Consumer consumer1 = testQueue.new Consumer(\"消费者-1-\", workDesk); Consumer consumer2 = testQueue.new Consumer(\"消费者-2-\", workDesk); service.submit(producer1); service.submit(producer2); service.submit(producer3); service.submit(producer4); service.submit(consumer1); service.submit(consumer2); } } 查看打印结果： 总的来说生产者的速度是会大于消费者的速度的，但是因为阻塞队列的缘故，所以我们不需要控制阻塞，当阻塞队列满的时候，生产者线程就会被阻塞，直到不再满。反之亦然，当消费者线程多于生产者线程时，消费者速度大于生产者速度，当队列为空时，就会阻塞消费者线程，直到队列非空。 "},"多线程/pond/1.线程池与底层原理.html":{"url":"多线程/pond/1.线程池与底层原理.html","title":"11.线程池与底层原理.md","keywords":"","body":"一 线程池基础1、线程池的优势2、线程池的主要参数3、线程池流程4.阻塞队列5.常见线程池6.拒绝策略二 execute与submit1.使用submit的坑3. execute和submit的区别三 线程池线程复用原理1.核心原理2.jdk线程池复用一 线程池基础 1、线程池的优势 （1）、降低系统资源消耗，通过重用已存在的线程，降低线程创建和销毁造成的消耗； （2）、提高系统响应速度，当有任务到达时，通过复用已存在的线程，无需等待新线程的创建便能立即执行； （3） 方便线程并发数的管控。因为线程若是无限制的创建，可能会导致内存占用过多而产生OOM，并且会造成cpu过度切换（cpu切换线程是有时间成本的（需要保持当前执行线程的现场，并恢复要执行线程的现场））。 （4）提供更强大的功能，延时定时线程池。 2、线程池的主要参数 public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue workQueue) { this(corePoolSize, maximumPoolSize, keepAliveTime, unit, workQueue, Executors.defaultThreadFactory(), defaultHandler); } 1、corePoolSize（线程池基本大小）：当向线程池提交一个任务时，若线程池已创建的线程数小于corePoolSize，即便此时存在空闲线程，也会通过创建一个新线程来执行该任务，直到已创建的线程数大于或等于corePoolSize时，将任务送入任务队列中 2、maximumPoolSize（线程池最大大小）：线程池所允许的最大线程个数。当队列满了，且已创建的线程数小于maximumPoolSize，则线程池会创建新的线程来执行任务。另外，对于无界队列，可忽略该参数。 3、keepAliveTime（线程存活保持时间）当线程池中线程数大于核心线程数时，线程的空闲时间如果超过线程存活时间，那么这个线程就会被销毁，直到线程池中的线程数小于等于核心线程数。 4、workQueue（任务队列）：用于传输和保存等待执行任务的阻塞队列。 5、threadFactory（线程工厂）：用于创建新线程。threadFactory创建的线程也是采用new Thread()方式，threadFactory创建的线程名都具有统一的风格：pool-m-thread-n（m为线程池的编号，n为线程池内的线程编号）。 6、handler（线程饱和策略）：当线程池和队列都满了，再加入线程会执行此策略。 3、线程池流程 线程池流程 1、判断核心线程池是否已满，没满则创建一个新的工作线程来执行任务。已满则。 2、判断任务队列是否已满，没满则将新提交的任务添加在工作队列，已满则。 3、判断线程池中当前线程数是否大于核心线程数，如果小于，在创建一个新的线程来执行任务，如果大于则执行饱和策略。 举个栗子：现有一个线程池，corePoolSize=10，maxPoolSize=20，队列长度为100，那么当任务过来会先创建10个核心线程数，接下来进来的任务会进入到队列中直到队列满了，会创建额外的线程来执行任务(最多20个线程)，这个时候如果再来任务就会执行拒绝策略。 4.阻塞队列 SynchronousQueue(同步移交队列)：队列不作为任务的缓冲方式，可以简单理解为队列长度为零 LinkedBlockingQueue(无界队列)：队列长度不受限制，当请求越来越多时(任务处理速度跟不上任务处理速度造成请求堆积)可能导致内存占用过多或OOM ArrayBlockintQueue(有界队列)：队列长度受限，当队列满了就需要创建多余的线程来执行任务 5.常见线程池 public class ThreadPoolDemo { public static void main(String[] args) { ExecutorService executorService = Executors.newFixedThreadPool(4); for (int i = 0; i System.out.println(\"i:\" + index + \" executorService\")); } executorService.shutdown(); } } 自动创建线程池的几种方式都封装在Executors工具类中： newFixedThreadPool：使用的构造方式为new ThreadPoolExecutor(var0, var0, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue())，设置了corePoolSize=maxPoolSize，keepAliveTime=0(此时该参数没作用)，无界队列，任务可以无限放入，当请求过多时(任务处理速度跟不上任务提交速度造成请求堆积)可能导致占用过多内存或直接导致OOM异常 newSingleThreadExector：使用的构造方式为new ThreadPoolExecutor(1, 1, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue(), var0)，基本同newFixedThreadPool，但是将线程数设置为了1，单线程，弊端和newFixedThreadPool一致 newCachedThreadPool：使用的构造方式为new ThreadPoolExecutor(0, 2147483647, 60L, TimeUnit.SECONDS, new SynchronousQueue())，corePoolSize=0，maxPoolSize为很大的数，同步移交队列，也就是说不维护常驻线程(核心线程)，每次来请求直接创建新线程来处理任务，也不使用队列缓冲，会自动回收多余线程，由于将maxPoolSize设置成Integer.MAX_VALUE，当请求很多时就可能创建过多的线程，导致资源耗尽OOM newScheduledThreadPool：使用的构造方式为new ThreadPoolExecutor(var1, 2147483647, 0L, TimeUnit.NANOSECONDS, new ScheduledThreadPoolExecutor.DelayedWorkQueue())，支持定时周期性执行，注意一下使用的是延迟队列，弊端同newCachedThreadPool一致 所以根据上面分析我们可以看到，FixedThreadPool和SigleThreadExecutor中之所以用LinkedBlockingQueue无界队列，是因为设置了corePoolSize=maxPoolSize，线程数无法动态扩展，于是就设置了无界阻塞队列来应对不可知的任务量；而CachedThreadPool则使用的是SynchronousQueue同步移交队列，为什么使用这个队列呢？因为CachedThreadPool设置了corePoolSize=0，maxPoolSize=Integer.MAX_VALUE，来一个任务就创建一个线程来执行任务，用不到队列来存储任务；SchduledThreadPool用的是延迟队列DelayedWorkQueue。在实际项目开发中也是推荐使用手动创建线程池的方式，而不用默认方式，关于这点在《阿里巴巴开发规范》中是这样描述的： 遵循阿里巴巴编码规范的提示，示例如下： public class ThreadPoolDemo { public static void main(String[] args) { ExecutorService executorService = new ThreadPoolExecutor(2, 2, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue<>(10), Executors.defaultThreadFactory(), new ThreadPoolExecutor.AbortPolicy()); for (int i = 0; i System.out.println(\"i:\" + index + \" executorService\")); } executorService.shutdown(); } } 6.拒绝策略 AbortPolicy：中断抛出异常 DiscardPolicy：默默丢弃任务，不进行任何通知 DiscardOldestPolicy：丢弃掉在队列中存在时间最久的任务 CallerRunsPolicy：让提交任务的线程去执行任务(对比前三种比较友好一丢丢) 二 execute与submit 1.使用submit的坑 首先看一下实例： public class ThreadPoolDemo3 { public static void main(String[] args) { ExecutorService executorService = Executors.newFixedThreadPool(4); for (int i = 0; i divTask(100, index)); } executorService.shutdown(); } private static void divTask(int a, int b) { double result = a / b; System.out.println(result); } } 运行结果： 　　上述代码，可以看出运行结果为4个，因该是有5个的，但是当i=0的时候，100/0是会报错的，但是日志信息中没有任何信息，是为什么那？如果使用了submit(Runnable task) 就会出现这种情况，任何的错误信息都出现不了！ 　　这是因为使用submit(Runnable task) 的时候，错误的堆栈信息跑出来的时候会被内部捕获到，所以打印不出来具体的信息让我们查看，解决的方法有如下两种： 1、使用execute（）代替submit（）； public class ThreadPoolDemo3 { public static void main(String[] args) { ExecutorService executorService = Executors.newFixedThreadPool(4); for (int i = 0; i divTask(100, index)); } executorService.shutdown(); } private static void divTask(int a, int b) { double result = a / b; System.out.println(result); } } 运行结果： 2、使用Future public class ThreadPoolDemo3 { public static void main(String[] args) { ExecutorService executorService = Executors.newFixedThreadPool(4); for (int i = 0; i divTask(200, index)); try { future.get(); } catch (InterruptedException | ExecutionException e) { e.printStackTrace(); } } executorService.shutdown(); } private static void divTask(int a, int b) { double result = a / b; System.out.println(result); } } 运行结果： 3. execute和submit的区别 （1）execute()方法用于提交不需要返回值的任务，所以无法判断任务是否被线程池执行成功。通过以下代码可知execute()方法输入的任务是一个Runnable类的实例。 （2）submit()方法用于提交需要返回值的任务。线程池会返回一个future类型的对象，通过这个future对象可以判断任务是否执行成功，并且可以通过future的get()方法来获取返回值，get()方法会阻塞当前线程直到任务完成，而使用get（long timeout，TimeUnit unit）方法则会阻塞当前线程一段时间后立即返回，这时候有可能任务没有执行完。 三 线程池线程复用原理 1.核心原理 一个线程一般在执行完任务后就结束了，怎么再让他执行下一个任务呢？　 　　线程重用的核心是，我们知道，Thread.start()只能调用一次，一旦这个调用结束，则该线程就到了stop状态，不能再次调用start。 则要达到复用的目的，则必须从Runnable接口的run()方法上入手，可以这样设计这个Runnable.run()方法（就叫外面的run()方法）： 它本质上是个无限循环，跑的过程中不断检查我们是否有新加入的子Runnable对象（就叫内部的runnable:run()吧，它就是用来实现我们自己的任务），有就调一下我们的run()，其实就一个大run()把其它小run()#1,run()#2,...给串联起来了，基本原理就这么简单 不停地处理我们提交的Runnable任务。 public void run() { while(true) { if(tasks available) { Runnable task = taskqueue.dequeue(); task.run(); } else { // wait or whatever } } } 　下面举个代码实例来模拟实现线程池复用线程 　　生产了两个 线程作为工人 　　生产了10个同样的任务，让他们执行 　　利用复用让 2个线程完成10个任务 import java.util.ArrayList; import java.util.LinkedList; import java.util.concurrent.TimeUnit; public class Mythreadpool { LinkedList taskList = new LinkedList(); class Task { //任务类 int id; Task(int id){ this.id=id; System.out.println(\"第\"+id+\"个任务产生\"); } public void run() {//具体的工作 System.out.println(\"第\"+id+\"个任务正在执行\"); try { TimeUnit.SECONDS.sleep(1); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(\"第\"+id+\"个任务执行完毕\"); } } class Worker extends Thread { //工人实体 String name; Worker(String name) { this.name = name; } public void run() { while(true) { if(taskList.size() == 0) { try { synchronized (taskList) { System.out.println(\"Worker \" + name+\" 没有任务\"); taskList.wait(); //没得到任务，进入tasklist的等待队列 } } catch (InterruptedException e) { e.printStackTrace(); } } synchronized (taskList) { System.out.println(\"Worker \" + name+\" 得到任务\"); taskList.removeFirst().run(); } } } } void pool() { //工人。只生产了两个工人 ArrayList wokerlist=new ArrayList(); for(int i=0;i 2.jdk线程池复用 ThreadPoolExecutor.execute() 分析，getTask是从线程池中，获取的任务。即所有的任务都放在ThreadPoolExecutor中，线程池启动多个Worker去执行任务，每个worker不停的从ThreadPoolExector的workQueue中取出任务，执行task.run()方法，直至所有的任务执行完毕 "},"多线程/10面试题一.html":{"url":"多线程/10面试题一.html","title":"12.面试题一","keywords":"","body":"现在有T1、T2、T3三个线程，你怎样保证T2在T1执行完后执行，T3在T2执行完后执行？在Java中Lock接口比synchronized块的优势是什么？你需要实现一个高效的缓存，它允许多个用户读，但只允许一个用户写，以此来保持它的完整性，你会怎样去实现它？在java中wait和sleep方法的不同？用Java实现阻塞队列。BlockingQueue介绍：用Java写代码来解决生产者——消费者问题。用Java写一个会导致死锁的程序，你将怎么解决？什么是原子操作，Java中的原子操作是什么？Java中的volatile关键是什么作用？怎样使用它？在Java中它跟synchronized方法有什么不同？什么是竞争条件(race condition)？你怎样发现和解决的？Hhread Jumpjava线程的状态转换介绍为什么我们调用start()方法时会执行run()方法，为什么我们不能直接调用run()方法？Java中你怎样唤醒一个阻塞的线程？在Java中CycliBarriar和CountdownLatch有什么区别？什么是不可变对象，它对写并发应用有什么帮助？你在多线程环境中遇到的常见的问题是什么？你是怎么解决它的？Java并发编程问题是面试过程中很容易遇到的问题，提前准备是解决问题的最好办法 (转载自https://blog.csdn.net/qq_34039315/article/details/78432606) 现在有T1、T2、T3三个线程，你怎样保证T2在T1执行完后执行，T3在T2执行完后执行？ 这个线程问题通常会在第一轮或电话面试阶段被问到，目的是检测你对”join”方法是否熟悉。这个多线程问题比较简单，可以用join方法实现。 核心： thread.Join把指定的线程加入到当前线程，可以将两个交替执行的线程合并为顺序执行的线程。 比如在线程B中调用了线程A的Join()方法，直到线程A执行完毕后，才会继续执行线程B。 想要更深入了解，建议看一下join的源码，也很简单的，使用wait方法实现的。 t.join(); //调用join方法，等待线程t执行完毕 t.join(1000); //等待 t 线程，等待时间是1000毫秒。 代码实现： public static void main(String[] args) { method01(); method02(); } /** * 第一种实现方式，顺序写死在线程代码的内部了，有时候不方便 */ private static void method01() { Thread t1 = new Thread(new Runnable() { @Override public void run() { System.out.println(\"t1 is finished\"); } }); Thread t2 = new Thread(new Runnable() { @Override public void run() { try { t1.join(); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(\"t2 is finished\"); } }); Thread t3 = new Thread(new Runnable() { @Override public void run() { try { t2.join(); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(\"t3 is finished\"); } }); t3.start(); t2.start(); t1.start(); } /** * 第二种实现方式，线程执行顺序可以在方法中调换 */ private static void method02(){ Runnable runnable = new Runnable() { @Override public void run() { System.out.println(Thread.currentThread().getName() + \"执行完成\"); } }; Thread t1 = new Thread(runnable, \"t1\"); Thread t2 = new Thread(runnable, \"t2\"); Thread t3 = new Thread(runnable, \"t3\"); try { t1.start(); t1.join(); t2.start(); t2.join(); t3.start(); t3.join(); } catch (InterruptedException e) { e.printStackTrace(); } } 在Java中Lock接口比synchronized块的优势是什么？你需要实现一个高效的缓存，它允许多个用户读，但只允许一个用户写，以此来保持它的完整性，你会怎样去实现它？ 这个题的原答案我认为不是很全面。 Lock接口 和 ReadWriteLock接口 如下： public interface Lock { void lock(); void lockInterruptibly() throws InterruptedException; boolean tryLock(); boolean tryLock(long time, TimeUnit unit) throws InterruptedException; void unlock(); Condition newCondition(); } public interface ReadWriteLock { Lock readLock(); Lock writeLock(); } 整体上来说Lock是synchronized的扩展版，Lock提供了无条件的、可轮询的(tryLock方法)、定时的(tryLock带参方法)、可中断的(lockInterruptibly)、可多条件队列的(newCondition方法)锁操作。另外Lock的实现类基本都支持非公平锁(默认)和公平锁，synchronized只支持非公平锁，当然，在大部分情况下，非公平锁是高效的选择。 ReadWriteLock是对Lock的运用，具体的实现类是 ReentrantReadWriteLock ，下面用这个类来实现读写类型的高效缓存： import java.util.HashMap; import java.util.Map; import java.util.Random; import java.util.concurrent.ExecutorService; import java.util.concurrent.Executors; import java.util.concurrent.locks.Lock; import java.util.concurrent.locks.ReadWriteLock; import java.util.concurrent.locks.ReentrantReadWriteLock; /** * 用ReadWriteLock读写锁来实现一个高效的Map缓存 * Created by LEO on 2017/10/30. */ public class ReaderAndWriter { private final ReadWriteLock lock = new ReentrantReadWriteLock(); private final Lock readLock = lock.readLock(); private final Lock writeLock = lock.writeLock(); private final Map map; public ReaderAndWriter(Map map) { this.map = map; } /************* 这是用lock()方法写的 ********************/ // public V put(K key, V value){ // writeLock.lock(); // try { // return map.put(key, value); // }finally { // writeLock.unlock(); // } // } // public V get(K key){ // readLock.lock(); // try { // return map.get(key); // }finally { // readLock.unlock(); // } // } /************* 这是用tryLock()方法写的 ********************/ public V put(K key, V value){ while (true){ if(writeLock.tryLock()){ try { System.out.println(\"put \"+ key +\" = \" + value); return map.put(key, value); }finally { writeLock.unlock(); } } } } public V get(K key){ while (true){ if (readLock.tryLock()) { try { V v = map.get(key); System.out.println(\"get \"+ key +\" = \" + v); return v; } finally { readLock.unlock(); } } } } /******************** 下面是测试区 *********************************/ public static void main(String[] args) { final ReaderAndWriter rw = new ReaderAndWriter<>(new HashMap<>()); ExecutorService exec = Executors.newCachedThreadPool(); for (int i = 0; i rw; private final String KEY = \"x\"; TestRunnable(ReaderAndWriter rw) { this.rw = rw; } @Override public void run() { Random random = new Random(); int r = random.nextInt(100); //生成随机数，小于30的写入缓存，大于等于30则读取数字 if (r 在java中wait和sleep方法的不同？ 通常会在电话面试中经常被问到的Java线程面试问题。 最大的不同是在等待时wait会释放锁，而sleep一直持有锁。Wait通常被用于线程间交互，sleep通常被用于暂停执行。 此处我想理一下Java多线程的基础知识： - Java的多线程锁是挂在对象上的，并不是在方法上的。即每个对象都有一个锁，当遇到类似synchronized的同步需要时，就会监视(monitor)每个想使用本对象的线程按照一定的规则来访问，规则也就是在同一时间内只能有一个线程能访问此对象。 - Java中获取锁的单位是线程。当线程A获取了对象B的锁，也就是对象B的持有标记上写的是线程A的唯一标识，在需要同步的情况下的话，只有线程A能访问对象B。 - Thread常用方法有：start/stop/yield/sleep/interrupt/join等，他们是线程级别的方法，所以并不会太关心锁的具体逻辑。 - Object的线程有关方法是：wait/wait(事件参数)/notify/notifyAll，他们是对象的方法，所以使用的时候就有点憋屈了，必须当前线程获取了本对象的锁才能使用，否则会报异常。但他们能更细粒度的控制锁，可以释放锁。 用Java实现阻塞队列。 这是一个相对艰难的多线程面试问题，它能达到很多的目的。第一，它可以检测侯选者是否能实际的用Java线程写程序；第二，可以检测侯选者对并发场景的理解，并且你可以根据这个问很多问题。如果他用wait()和notify()方法来实现阻塞队列，你可以要求他用最新的Java 5中的并发类来再写一次。 下面是实现了阻塞的take和put方法的阻塞队列（分别用synchronized 和 wait/notify 实现）： import java.util.LinkedList; import java.util.List; import java.util.Random; import java.util.concurrent.ExecutorService; import java.util.concurrent.Executors; /** * 实现了阻塞的take和put方法的阻塞队列 * 分别用synchronized 和 wait/notify 实现 * @author xuexiaolei * @version 2017年11月01日 */ public class MyBlocingQueue { private final List list; private final int limit;//有大小限制的 public MyBlocingQueue(int limit) { this.limit = limit; this.list = new LinkedList(); } //这是用synchronized写的,在list空或者满的时候效率会低，因为会一直轮询 // public void put(E e){ // while(true){ // synchronized (list){ // if (list.size() 0){ // System.out.println(\"list : \" + list.toString()); // E remove = (E) list.remove(0); // System.out.println(\"take : \" + remove); // return remove; // } // } // } // } //用wait，notify写的,在list空或者满的时候效率会高一点，因为wait释放锁，然后等待唤醒 public synchronized void put(E e){ while (list.size() == limit){ try { wait(); } catch (InterruptedException e1) { e1.printStackTrace(); } } System.out.println(\"list : \" + list.toString()); System.out.println(\"put : \" + e); list.add(e); notifyAll(); } public synchronized E take() { while (list.size() == 0){ try { wait(); } catch (InterruptedException e) { e.printStackTrace(); } } System.out.println(\"list : \" + list.toString()); E remove = (E) list.remove(0); System.out.println(\"take : \" + remove); notifyAll(); return remove; } /******************** 下面是测试区 *********************************/ public static void main(String[] args) { final MyBlocingQueue myBlocingQueue = new MyBlocingQueue(10); ExecutorService exec = Executors.newCachedThreadPool(); for (int i = 0; i myBlocingQueue; TestRunnable(MyBlocingQueue myBlocingQueue) { this.myBlocingQueue = myBlocingQueue; } @Override public void run() { Random random = new Random(); int r = random.nextInt(100); //生成随机数,按照一定比率读取或者放入，可以更改！！！ if (r BlockingQueue介绍： Java5中提供了BlockingQueue的方法，并且有几个实现，在此介绍一下。 BlockingQueue 具有 4 组不同的方法用于插入、移除以及对队列中的元素进行检查。如果请求的操作不能得到立即执行的话，每个方法的表现也不同。这些方法如下： Throws exception Special value Blocks Times out add(e) offer(e) put(e) offer(Object, long, TimeUnit) remove() poll() take() poll(long, TimeUnit) element() peek() - Throws exception 抛异常：如果试图的操作无法立即执行，抛一个异常。 - Special value 特定值：如果试图的操作无法立即执行，返回一个特定的值(常常是 true / false) - Blocks 阻塞：如果试图的操作无法立即执行，该方法调用将会发生阻塞，直到能够执行。 - Times out 超时：如果试图的操作无法立即执行，该方法调用将会发生阻塞，直到能够执行，但等待时间不会超过给定值。返回一个特定值以告知该操作是否成功(典型的是true / false)。 BlockingQueue 的实现类 - ArrayBlockingQueue：ArrayBlockingQueue 是一个有界的阻塞队列，其内部实现是将对象放到一个数组里。有界也就意味着，它不能够存储无限多数量的元素。它有一个同一时间能够存储元素数量的上限。你可以在对其初始化的时候设定这个上限，但之后就无法对这个上限进行修改了(译者注：因为它是基于数组实现的，也就具有数组的特性：一旦初始化，大小就无法修改)。 - DelayQueue：DelayQueue 对元素进行持有直到一个特定的延迟到期。注入其中的元素必须实现 java.util.concurrent.Delayed 接口。 - LinkedBlockingQueue：LinkedBlockingQueue 内部以一个链式结构(链接节点)对其元素进行存储。如果需要的话，这一链式结构可以选择一个上限。如果没有定义上限，将使用 Integer.MAX_VALUE 作为上限。 - PriorityBlockingQueue：PriorityBlockingQueue 是一个无界的并发队列。它使用了和类 java.util.PriorityQueue 一样的排序规则。你无法向这个队列中插入 null 值。所有插入到 PriorityBlockingQueue 的元素必须实现 java.lang.Comparable 接口。因此该队列中元素的排序就取决于你自己的 Comparable 实现。 - SynchronousQueue：SynchronousQueue 是一个特殊的队列，它的内部同时只能够容纳单个元素。如果该队列已有一元素的话，试图向队列中插入一个新元素的线程将会阻塞，直到另一个线程将该元素从队列中抽走。同样，如果该队列为空，试图向队列中抽取一个元素的线程将会阻塞，直到另一个线程向队列中插入了一条新的元素。据此，把这个类称作一个队列显然是夸大其词了。它更多像是一个汇合点。 BlocingQueue的实现大多是通过 lock锁的多条件（condition）阻塞控制，下面我们自己写一个简单版： import java.util.LinkedList; import java.util.List; import java.util.concurrent.locks.Condition; import java.util.concurrent.locks.Lock; import java.util.concurrent.locks.ReentrantLock; /** * 模仿ArrayBlockingQueue实现阻塞队列 * @author xuexiaolei * @version 2017年11月01日 */ public class MyBlocingQueue2 { private final List list; private final int limit;//有大小限制的 private final Lock lock = new ReentrantLock(); private final Condition notFull = lock.newCondition(); private final Condition notEmpty = lock.newCondition(); public MyBlocingQueue2(int limit) { this.limit = limit; this.list = new LinkedList(); } public void put(E e) throws InterruptedException { lock.lock(); try { while (list.size() == limit){ notFull.await(); } list.add(e); notEmpty.signalAll(); }finally { lock.unlock(); } } public E take() throws InterruptedException { lock.lock(); try { while (list.size() == 0){ notEmpty.await(); } E remove = (E) list.remove(0); notFull.signalAll(); return remove; }finally { lock.unlock(); } } } 用Java写代码来解决生产者——消费者问题。 与上面的问题很类似，但这个问题更经典，有些时候面试都会问下面的问题。在Java中怎么解决生产者——消费者问题，当然有很多解决方法，我已经分享了一种用阻塞队列实现的方法。有些时候他们甚至会问怎么实现哲学家进餐问题。 生产者、消费者有很多的实现方法： - 用wait() / notify()方法 - 用Lock的多Condition方法 - BlockingQueue阻塞队列方法 可以发现在上面实现阻塞队列题中，BlockingQueue的实现基本都用到了类似的实现，将BlockingQueue的实现方式稍微包装一下就成了一个生产者-消费者模式了。 import java.util.Random; import java.util.concurrent.ArrayBlockingQueue; import java.util.concurrent.BlockingQueue; /** * 用阻塞队列快速实现生产者-消费者 * @author xuexiaolei * @version 2017年11月01日 */ public class ProduceAndConsumer { public static void main(String[] args) { final BlockingQueue list = new ArrayBlockingQueue(10); Procude procude = new Procude(list); Consumer consumer = new Consumer(list); procude.start(); consumer.start(); } static class Procude extends Thread{ private final BlockingQueue list; Procude(BlockingQueue list) { this.list = list; } @Override public void run() { while(true){ try { Integer take = list.take(); System.out.println(\"消费数据：\" + take); // Thread.sleep(1000); } catch (InterruptedException e) { e.printStackTrace(); } } } } static class Consumer extends Thread{ private final BlockingQueue list; Consumer(BlockingQueue list) { this.list = list; } @Override public void run() { while (true){ try { int i = new Random().nextInt(100); list.put(i); System.out.println(\"生产数据：\" + i); Thread.sleep(1000); } catch (InterruptedException e) { e.printStackTrace(); } } } } } 此处不再详细地写另外几种实现方式了：wait() / notify()方法、Lock的多Condition方法、信号量等，甚至可以考虑用CyclicBarrier、CountDownLatch也可以实现生产者-消费者的，难易程度、效率不一样罢了。 用Java写一个会导致死锁的程序，你将怎么解决？ 这是我最喜欢的Java线程面试问题，因为即使死锁问题在写多线程并发程序时非常普遍，但是很多侯选者并不能写deadlock free code（无死锁代码？），他们很挣扎。只要告诉他们，你有N个资源和N个线程，并且你需要所有的资源来完成一个操作。为了简单这里的n可以替换为2，越大的数据会使问题看起来更复杂。通过避免Java中的死锁来得到关于死锁的更多信息。 /** * 简单死锁程序 * lockA、lockB分别是两个资源，线程A、B必须同是拿到才能工作 * 但A线程先拿lockA、再拿lockB * 线程先拿lockB、再拿lockA * @author xuexiaolei * @version 2017年11月01日 */ public class SimpleDeadLock { public static void main(String[] args) { Object lockA = new Object(); Object lockB = new Object(); A a = new A(lockA, lockB); B b = new B(lockA, lockB); a.start(); b.start(); } static class A extends Thread{ private final Object lockA; private final Object lockB; A(Object lockA, Object lockB) { this.lockA = lockA; this.lockB = lockB; } @Override public void run() { synchronized (lockA){ try { Thread.sleep(1000); synchronized (lockB){ System.out.println(\"Hello A\"); } } catch (InterruptedException e) { e.printStackTrace(); } } } } static class B extends Thread{ private final Object lockA; private final Object lockB; B(Object lockA, Object lockB) { this.lockA = lockA; this.lockB = lockB; } @Override public void run() { synchronized (lockB){ try { Thread.sleep(1000); synchronized (lockA){ System.out.println(\"Hello B\"); } } catch (InterruptedException e) { e.printStackTrace(); } } } } } 产生死锁的四个必要条件： - 互斥条件：一个资源每次只能被一个进程使用。 - 请求与保持条件：一个进程因请求资源而阻塞时，对已获得的资源保持不放。 - 不剥夺条件:进程已获得的资源，在末使用完之前，不能强行剥夺。 - 循环等待条件:若干进程之间形成一种头尾相接的循环等待资源关系。 如何避免死锁？ - 从死锁的四个必要条件来看，破坏其中的任意一个条件就可以避免死锁。但互斥条件是由资源本身决定的，不剥夺条件一般无法破坏，要实现的话得自己写更多的逻辑。 - 避免无限期的等待：用Lock.tryLock(),wait/notify等方法写出请求一定时间后，放弃已经拥有的锁的程序。 - 注意锁的顺序：以固定的顺序获取锁，可以避免死锁。 - 开放调用：即只对有请求的进行封锁。你应当只想你要运行的资源获取封锁，比如在上述程序中我在封锁的完全的对象资源。但是如果我们只对它所属领域中的一个感兴趣，那我们应当封锁住那个特殊的领域而并非完全的对象。 - 最后，如果能避免使用多个锁，甚至写出无锁的线程安全程序是再好不过了。 什么是原子操作，Java中的原子操作是什么？ 非常简单的java线程面试问题，接下来的问题是你是否需要同步一个原子操作。 原子操作是不可分割的操作，一个原子操作中间是不会被其他线程打断的，所以不需要同步一个原子操作。 多个原子操作合并起来后就不是一个原子操作了，就需要同步了。 i++不是一个原子操作，它包含 读取-修改-写入 操作，在多线程状态下是不安全的。 另外，java内存模型允许将64位的读操作或写操作分解为2个32位的操作，所以对long和double类型的单次读写操作并不是原子的，注意使用volitile使他们成为原子操作。 Java中的volatile关键是什么作用？怎样使用它？在Java中它跟synchronized方法有什么不同？ 自从Java 5和Java内存模型改变以后，基于volatile关键字的线程问题越来越流行。应该准备好回答关于volatile变量怎样在并发环境中确保可见性。 volatile关键字的作用是：保证变量的可见性。 在java内存结构中，每个线程都是有自己独立的内存空间(此处指的线程栈)。当需要对一个共享变量操作时，线程会将这个数据从主存空间复制到自己的独立空间内进行操作，然后在某个时刻将修改后的值刷新到主存空间。这个中间时间就会发生许多奇奇怪怪的线程安全问题了，volatile就出来了，它保证读取数据时只从主存空间读取，修改数据直接修改到主存空间中去，这样就保证了这个变量对多个操作线程的可见性了。换句话说，被volatile修饰的变量，能保证该变量的 单次读或者单次写 操作是原子的。 但是线程安全是两方面需要的 原子性(指的是多条操作)和可见性。volatile只能保证可见性，synchronized是两个均保证的。 volatile轻量级，只能修饰变量；synchronized重量级，还可修饰方法。 volatile不会造成线程的阻塞，而synchronized可能会造成线程的阻塞。 什么是竞争条件(race condition)？你怎样发现和解决的？ 这是一道出现在多线程面试的高级阶段的问题。大多数的面试官会问最近你遇到的竞争条件，以及你是怎么解决的。有些时间他们会写简单的代码，然后让你检测出代码的竞争条件。可以参考我之前发布的关于Java竞争条件的文章。在我看来这是最好的java线程面试问题之一，它可以确切的检测候选者解决竞争条件的经验。关于这方面最好的书是《java并发编程实战》。 当多个进程都企图对共享数据进行某种处理，而最后的结果又取决于进程运行的顺序时，则我们认为这发生了竞争条件（race condition）。 下面是个最简单的例子，是一个单例模式实现的错误示范： @NotThreadSafe public class LazyInitRace { private ExpensiveObject instance = null; public ExpensiveObject getInstance() { if (instance == null) instance = new ExpensiveObject(); return instance; } }12345678910 在上述例子中，表现一种很常见的竞态条件类型:“先检查后执行”。根据某个检查结果来执行进一步的操作，但很有可能这个检查结果是失效的！还有很常见的竞态条件“读取-修改-写入”三连，在多线程条件下，三个小操作并不一定会放在一起执行的。 如何对待竞态条件？ 首先，警惕复合操作，当多个原子操作合在一起的时候，并不一定仍然是一个原子操作，此时需要用同步的手段来保证原子性。 另外，使用本身是线程安全的类，这样在很大程度上避免了未知的风险。 Hhread Jump https://www.cnblogs.com/wx170119/p/10451812.html 在UNIX中你可以使用kill -3，然后thread dump将会打印日志，在windows中你可以使用”CTRL+Break”。非常简单和专业的线程面试问题，但是如果他问你怎样分析它，就会很棘手。 SIGQUIT（kill -3 pid）用来打印Java进程trace，并不会影响程序运行，不用担心他把程序杀死了；SIGUSR1（kill -10 pid）可触发进程进行一次强制GC。 java线程的状态转换介绍 后续分析要用到，所以此处穿插一下这个点： 新建状态（New） 用new语句创建的线程处于新建状态，此时它和其他Java对象一样，仅仅在堆区中被分配了内存。 就绪状态（Runnable） 当一个线程对象创建后，其他线程调用它的start()方法，该线程就进入就绪状态，Java虚拟机会为它创建方法调用栈和程序计数器。处于这个状态的线程位于可运行池中，等待获得CPU的使用权。 运行状态（Running） 处于这个状态的线程占用CPU，执行程序代码。只有处于就绪状态的线程才有机会转到运行状态。 阻塞状态（Blocked） 阻塞状态是指线程因为某些原因放弃CPU，暂时停止运行。当线程处于阻塞状态时，Java虚拟机不会给线程分配CPU。直到线程重新进入就绪状态，它才有机会转到运行状态。 阻塞状态可分为以下3种： 位于对象等待池中的阻塞状态（Blocked in object’s wait pool）：当线程处于运行状态时，如果执行了某个对象的wait()方法，Java虚拟机就会把线程放到这个对象的等待池中，这涉及到“线程通信”的内容。 位于对象锁池中的阻塞状态（Blocked in object’s lock pool）：当线程处于运行状态时，试图获得某个对象的同步锁时，如果该对象的同步锁已经被其他线程占用，Java虚拟机就会把这个线程放到这个对象的锁池中，这涉及到“线程同步”的内容。 其他阻塞状态（Otherwise Blocked）：当前线程执行了sleep()方法，或者调用了其他线程的join()方法，或者发出了I/O请求时，就会进入这个状态。 死亡状态（Dead） 当线程退出run()方法时，就进入死亡状态，该线程结束生命周期。 我们运行之前的那个死锁代码SimpleDeadLock.java，然后尝试输出信息(/这是注释，作者自己加的/)： /* 时间，jvm信息 */ 2017-11-01 17:36:28 Full thread dump Java HotSpot(TM) 64-Bit Server VM (25.144-b01 mixed mode): /* 线程名称：DestroyJavaVM 编号：#13 优先级：5 系统优先级：0 jvm内部线程id：0x0000000001c88800 对应系统线程id（NativeThread ID）：0x1c18 线程状态： waiting on condition [0x0000000000000000] （等待某个条件） 线程详细状态：java.lang.Thread.State: RUNNABLE 及之后所有*/ \"DestroyJavaVM\" #13 prio=5 os_prio=0 tid=0x0000000001c88800 nid=0x1c18 waiting on condition [0x0000000000000000] java.lang.Thread.State: RUNNABLE \"Thread-1\" #12 prio=5 os_prio=0 tid=0x0000000018d49000 nid=0x17b8 waiting for monitor entry [0x0000000019d7f000] /* 线程状态：阻塞（在对象同步上） 代码位置：at com.leo.interview.SimpleDeadLock$B.run(SimpleDeadLock.java:56) 等待锁：0x00000000d629b4d8 已经获得锁：0x00000000d629b4e8*/ java.lang.Thread.State: BLOCKED (on object monitor) at com.leo.interview.SimpleDeadLock$B.run(SimpleDeadLock.java:56) - waiting to lock (a java.lang.Object) - locked (a java.lang.Object) \"Thread-0\" #11 prio=5 os_prio=0 tid=0x0000000018d44000 nid=0x1ebc waiting for monitor entry [0x000000001907f000] java.lang.Thread.State: BLOCKED (on object monitor) at com.leo.interview.SimpleDeadLock$A.run(SimpleDeadLock.java:34) - waiting to lock (a java.lang.Object) - locked (a java.lang.Object) \"Service Thread\" #10 daemon prio=9 os_prio=0 tid=0x0000000018ca5000 nid=0x1264 runnable [0x0000000000000000] java.lang.Thread.State: RUNNABLE \"C1 CompilerThread2\" #9 daemon prio=9 os_prio=2 tid=0x0000000018c46000 nid=0xb8c waiting on condition [0x0000000000000000] java.lang.Thread.State: RUNNABLE \"C2 CompilerThread1\" #8 daemon prio=9 os_prio=2 tid=0x0000000018be4800 nid=0x1db4 waiting on condition [0x0000000000000000] java.lang.Thread.State: RUNNABLE \"C2 CompilerThread0\" #7 daemon prio=9 os_prio=2 tid=0x0000000018be3800 nid=0x810 waiting on condition [0x0000000000000000] java.lang.Thread.State: RUNNABLE \"Monitor Ctrl-Break\" #6 daemon prio=5 os_prio=0 tid=0x0000000018bcc800 nid=0x1c24 runnable [0x00000000193ce000] java.lang.Thread.State: RUNNABLE at java.net.SocketInputStream.socketRead0(Native Method) at java.net.SocketInputStream.socketRead(SocketInputStream.java:116) at java.net.SocketInputStream.read(SocketInputStream.java:171) at java.net.SocketInputStream.read(SocketInputStream.java:141) at sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284) at sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326) at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178) - locked (a java.io.InputStreamReader) at java.io.InputStreamReader.read(InputStreamReader.java:184) at java.io.BufferedReader.fill(BufferedReader.java:161) at java.io.BufferedReader.readLine(BufferedReader.java:324) - locked (a java.io.InputStreamReader) at java.io.BufferedReader.readLine(BufferedReader.java:389) at com.intellij.rt.execution.application.AppMainV2$1.run(AppMainV2.java:64) \"Attach Listener\" #5 daemon prio=5 os_prio=2 tid=0x0000000017781800 nid=0x524 runnable [0x0000000000000000] java.lang.Thread.State: RUNNABLE \"Signal Dispatcher\" #4 daemon prio=9 os_prio=2 tid=0x000000001778f800 nid=0x1b08 waiting on condition [0x0000000000000000] java.lang.Thread.State: RUNNABLE \"Finalizer\" #3 daemon prio=8 os_prio=1 tid=0x000000001776a800 nid=0xdac in Object.wait() [0x0000000018b6f000] java.lang.Thread.State: WAITING (on object monitor) at java.lang.Object.wait(Native Method) - waiting on (a java.lang.ref.ReferenceQueue$Lock) at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:143) - locked (a java.lang.ref.ReferenceQueue$Lock) at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:164) at java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:209) \"Reference Handler\" #2 daemon prio=10 os_prio=2 tid=0x0000000017723800 nid=0x1670 in Object.wait() [0x00000000189ef000] java.lang.Thread.State: WAITING (on object monitor) at java.lang.Object.wait(Native Method) - waiting on (a java.lang.ref.Reference$Lock) at java.lang.Object.wait(Object.java:502) at java.lang.ref.Reference.tryHandlePending(Reference.java:191) - locked (a java.lang.ref.Reference$Lock) at java.lang.ref.Reference$ReferenceHandler.run(Reference.java:153) \"VM Thread\" os_prio=2 tid=0x000000001771b800 nid=0x604 runnable \"GC task thread#0 (ParallelGC)\" os_prio=0 tid=0x0000000001c9d800 nid=0x9f0 runnable \"GC task thread#1 (ParallelGC)\" os_prio=0 tid=0x0000000001c9f000 nid=0x154c runnable \"GC task thread#2 (ParallelGC)\" os_prio=0 tid=0x0000000001ca0800 nid=0xcd0 runnable \"GC task thread#3 (ParallelGC)\" os_prio=0 tid=0x0000000001ca2000 nid=0x1e58 runnable \"VM Periodic Task Thread\" os_prio=2 tid=0x0000000018c5a000 nid=0x1b58 waiting on condition JNI global references: 33 /* 此处可以看待死锁的相关信息！ */ Found one Java-level deadlock: ============================= \"Thread-1\": waiting to lock monitor 0x0000000017729fc8 (object 0x00000000d629b4d8, a java.lang.Object), which is held by \"Thread-0\" \"Thread-0\": waiting to lock monitor 0x0000000017727738 (object 0x00000000d629b4e8, a java.lang.Object), which is held by \"Thread-1\" Java stack information for the threads listed above: =================================================== \"Thread-1\": at com.leo.interview.SimpleDeadLock$B.run(SimpleDeadLock.java:56) - waiting to lock (a java.lang.Object) - locked (a java.lang.Object) \"Thread-0\": at com.leo.interview.SimpleDeadLock$A.run(SimpleDeadLock.java:34) - waiting to lock (a java.lang.Object) - locked (a java.lang.Object) Found 1 deadlock. /* 内存使用状况，详情得看JVM方面的书 */ Heap PSYoungGen total 37888K, used 4590K [0x00000000d6100000, 0x00000000d8b00000, 0x0000000100000000) eden space 32768K, 14% used [0x00000000d6100000,0x00000000d657b968,0x00000000d8100000) from space 5120K, 0% used [0x00000000d8600000,0x00000000d8600000,0x00000000d8b00000) to space 5120K, 0% used [0x00000000d8100000,0x00000000d8100000,0x00000000d8600000) ParOldGen total 86016K, used 0K [0x0000000082200000, 0x0000000087600000, 0x00000000d6100000) object space 86016K, 0% used [0x0000000082200000,0x0000000082200000,0x0000000087600000) Metaspace used 3474K, capacity 4500K, committed 4864K, reserved 1056768K class space used 382K, capacity 388K, committed 512K, reserved 1048576K123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132 为什么我们调用start()方法时会执行run()方法，为什么我们不能直接调用run()方法？ 这是另一个非常经典的java多线程面试问题。这也是我刚开始写线程程序时候的困惑。现在这个问题通常在电话面试或者是在初中级Java面试的第一轮被问到。这个问题的回答应该是这样的，当你调用start()方法时你将创建新的线程，并且执行在run()方法里的代码。但是如果你直接调用run()方法，它不会创建新的线程也不会执行调用线程的代码。 简单点来说： new一个Thread，线程进入了新建状态;调用start()方法，线程进入了就绪状态，当分配到时间片后就可以开始运行了。 start()会执行线程的相应准备工作，然后自动执行run()方法的内容。是真正的多线程工作。 而直接执行run()方法，会把run方法当成一个mian线程下的普通方法去执行，并不会在某个线程中执行它，这并不是多线程工作。 Java中你怎样唤醒一个阻塞的线程？ 这是个关于线程和阻塞的棘手的问题，它有很多解决方法。如果线程遇到了IO阻塞，我并且不认为有一种方法可以中止线程。如果线程因为调用wait()、sleep()、或者join()方法而导致的阻塞，你可以中断线程，并且通过抛出InterruptedException来唤醒它。我之前写的《How to deal with blocking methods in java》有很多关于处理线程阻塞的信息。 这个我们先简单粗暴地对某些阻塞方法进行分类： - 会抛出InterruptedException的方法：wait、sleep、join、Lock.lockInterruptibly等，针对这类方法，我们在线程内部处理好异常(要不完全内部处理，要不把这个异常抛出去)，然后就可以实现唤醒。 - 不会抛InterruptedException的方法：Socket的I/O,同步I/O，Lock.lock等。对于I/O类型，我们可以关闭他们底层的通道，比如Socket的I/O，关闭底层套接字，然后抛出异常处理就好了;比如同步I/O，关闭底层Channel然后处理异常。对于Lock.lock方法，我们可以改造成Lock.lockInterruptibly方法去实现。 在Java中CycliBarriar和CountdownLatch有什么区别？ 这个线程问题主要用来检测你是否熟悉JDK5中的并发包。这两个的区别是CyclicBarrier可以重复使用已经通过的障碍，而CountdownLatch不能重复使用。 还要注意一点的区别： CountDownLatch : 一个线程(或者多个)， 等待另外N个线程完成某个事情之后才能执行。 CyclicBarrier : N个线程相互等待，任何一个线程完成之前，所有的线程都必须等待。 这样应该就清楚一点了，对于CountDownLatch来说，重点是那个“一个线程”, 是它在等待，而另外那N的线程在把“某个事情”做完之后可以继续等待，可以终止。而对于CyclicBarrier来说，重点是那N个线程，他们之间任何一个没有完成，所有的线程都必须等待。 从api上理解就是CountdownLatch有主要配合使用两个方法countDown()和await()，countDown()是做事的线程用的方法，await()是等待事情完成的线程用个方法，这两种线程是可以分开的(下面例子:CountdownLatchTest2)，当然也可以是同一组线程(下面例子:CountdownLatchTest);CyclicBarrier只有一个方法await(),指的是做事线程必须大家同时等待，必须是同一组线程的工作。 CountdownLatch例子： import java.util.concurrent.CountDownLatch; import java.util.concurrent.ExecutorService; import java.util.concurrent.Executors; /** * 线程都准备完成后一起执行的例子 * @author xuexiaolei * @version 2017年11月02日 */ public class CountdownLatchTest { private final static int THREAD_NUM = 10; public static void main(String[] args) { CountDownLatch lock = new CountDownLatch(THREAD_NUM); ExecutorService exec = Executors.newCachedThreadPool(); for (int i = 0; i CyclicBarrier例子： import java.util.concurrent.*; /** * * @author xuexiaolei * @version 2017年11月02日 */ public class CyclicBarrierTest { private final static int THREAD_NUM = 10; public static void main(String[] args) { CyclicBarrier lock = new CyclicBarrier(THREAD_NUM, new Runnable() { @Override public void run() { System.out.println(\"大家都准备完成了\"); } }); ExecutorService exec = Executors.newCachedThreadPool(); for (int i = 0; i 什么是不可变对象，它对写并发应用有什么帮助？ 另一个多线程经典面试问题，并不直接跟线程有关，但间接帮助很多。这个java面试问题可以变的非常棘手，如果他要求你写一个不可变对象，或者问你为什么String是不可变的。 immutable Objects(不可变对象)就是那些一旦被创建，它们的状态就不能被改变的Objects，每次对他们的改变都是产生了新的immutable的对象，而mutable Objects(可变对象)就是那些创建后，状态可以被改变的Objects. 如何在Java中写出Immutable的类？ \\1. immutable对象的状态在创建之后就不能发生改变，任何对它的改变都应该产生一个新的对象。 \\2. immutable类的所有的属性都应该是final的。 \\3. 对象必须被正确的创建，比如：对象引用在对象创建过程中不能泄露(leak)。 \\4. 对象应该是final的，以此来限制子类继承父类，以避免子类改变了父类的immutable特性。 \\5. 如果类中包含mutable类对象，那么返回给客户端的时候，返回该对象的一个拷贝，而不是该对象本身（该条可以归为第一条中的一个特例） 使用Immutable类的好处： \\1. Immutable对象是线程安全的，可以不用被synchronize就在并发环境中共享 2.Immutable对象简化了程序开发，因为它无需使用额外的锁机制就可以在线程间共享 \\3. Immutable对象提高了程序的性能，因为它减少了synchroinzed的使用 \\4. Immutable对象是可以被重复使用的，你可以将它们缓存起来重复使用，就像字符串字面量和整型数字一样。你可以使用静态工厂方法来提供类似于valueOf（）这样的方法，它可以从缓存中返回一个已经存在的Immutable对象，而不是重新创建一个。 /** * 不可变对象 * @author xuexiaolei * @version 2017年11月03日 */ public class ImmutableObjectPerson { private final String name; private final String sex; public ImmutableObjectPerson(String name, String sex) { this.name = name; this.sex = sex; } public String getName() { return name; } public String getSex() { return sex; } }123456789101112131415161718192021 你在多线程环境中遇到的常见的问题是什么？你是怎么解决它的？ 多线程和并发程序中常遇到的有Memory-interface、竞争条件、死锁、活锁和饥饿。问题是没有止境的，如果你弄错了，将很难发现和调试。这是大多数基于面试的，而不是基于实际应用的Java线程问题。 此类问题请大家面试的时候提前准备，方便交流，如果实在找不出来，可以想想自己平时解决问题的思路，总结下来告诉考官。 "},"多线程/10面试题二.html":{"url":"多线程/10面试题二.html","title":"13.面试题二.md","keywords":"","body":"什么是线程和进程？并发和并行的区别？为什么使用多线程？使用多线程可能会带来什么问题？说说线程的生命周期？什么是上下文切换？什么是死锁？如何避免死锁？说说Sleep和Wait方法的区别SynchronzedCASThreadLocal并发集合容器并发同步容器阻塞队列线程池Java锁机制 什么是线程和进程？ 进程 进程是程序的一次执行过程，是系统运行程序的基本单位，因此进程是动态的。系统运行一个程序即是一个进程从创建，运行到消亡的过程。 比如：当我们启动 main 函数时其实就是启动了一个 JVM 的进程，而 main 函数所在的线程就是这个进程中的一个线程，也称主线程。 线程 线程是一个比进程更小的执行单位 一个进程在其执行的过程中可以产生多个线程 与进程不同的是同类的多个线程共享进程的堆和方法区资源，但每个线程有自己的程序计数器、虚拟机栈和本地方法栈，所以系统在产生一个线程，或是在各个线程之间作切换工作时，负担要比进程小得多，也正因为如此，线程也被称为轻量级进程 并发和并行的区别？ 并发：同一时间段，多个任务都在执行 (单位时间内不一定同时执行)； 并行：单位时间内，多个任务同时执行。 为什么使用多线程？ 从计算机底层来说： 线程可以比作是轻量级的进程，是程序执行的最小单位,线程间的切换和调度的成本远远小于进程。另外，多核 CPU 时代意味着多个线程可以同时运行，这减少了线程上下文切换的开销。 从当代互联网发展趋势来说：现在的系统动不动就要求百万级甚至千万级的并发量，而多线程并发编程正是开发高并发系统的基础，利用好多线程机制可以大大提高系统整体的并发能力以及性能。 使用多线程可能会带来什么问题？ 可能会带来内存泄漏、上下文切换、死锁有受限于硬件和软件的资源闲置问题。 说说线程的生命周期？ 线程创建之后它将处于New（新建）状态，调用 start() 方法后开始运行，线程这时候处于 READY（可运行） 状态。可运行状态的线程获得了 CPU 时间片（timeslice）后就处于 RUNNING（运行） 状态。 当线程执行 wait()方法之后，线程进入 WAITING（等待） 状态。进入等待状态的线程需要依靠其他线程的通知才能够返回到运行状态，而 TIME_WAITING(超时等待) 状态相当于在等待状态的基础上增加了超时限制，比如通过 sleep（long millis）方法或 wait（long millis）方法可以将 Java 线程置于 TIMED WAITING 状态。当超时时间到达后 Java 线程将会返回到 RUNNABLE 状态。当线程调用同步方法时，在没有获取到锁的情况下，线程将会进入到 BLOCKED（阻塞）状态。线程在执行 Runnable 的run()方法之后将会进入到 TERMINATED（终止） 状态。 什么是上下文切换？ 多线程编程中一般线程的个数都大于 CPU 核心的个数，而一个 CPU 核心在任意时刻只能被一个线程使用，为了让这些线程都能得到有效执行，CPU 采取的策略是为每个线程分配时间片并轮转的形式。当一个线程的时间片用完的时候就会重新处于就绪状态让给其他线程使用，这个过程就属于一次上下文切换。 实际上就是任务从保存到再加载的过程就是一次上下文切换。 上下文切换通常是计算密集型的。也就是说，它需要相当可观的处理器时间，在每秒几十上百次的切换中，每次切换都需要纳秒量级的时间。所以，上下文切换对系统来说意味着消耗大量的 CPU 时间，事实上，可能是操作系统中时间消耗最大的操作。 Linux 相比与其他操作系统（包括其他类 Unix 系统）有很多的优点，其中有一项就是，其上下文切换和模式切换的时间消耗非常少。 什么是死锁？如何避免死锁？ 如下图所示，线程 A 持有资源 2，线程 B 持有资源 1，他们同时都想申请对方的资源，所以这两个线程就会互相等待而进入死锁状态。 学过操作系统的朋友都知道产生死锁必须具备以下四个条件： 互斥条件：该资源任意一个时刻只由一个线程占用。(同一时刻，这个碗是我的，你不能碰) 请求与保持条件：一个进程因请求资源而阻塞时，对已获得的资源保持不放。（我拿着这个碗一直不放） 不剥夺条件:线程已获得的资源在末使用完之前不能被其他线程强行剥夺，只有自己使用完毕后才释放资源。（我碗中的饭没吃完，你不能抢，释放权是我自己的，我想什么时候放就什么时候放） 循环等待条件:若干进程之间形成一种头尾相接的循环等待资源关系。（我拿了A碗，你拿了B碗，但是我还想要你的B碗，你还想我的A碗） public class DeadLockDemo { private static Object resource1 = new Object();//资源 1 private static Object resource2 = new Object();//资源 2 public static void main(String[] args) { new Thread(() -> { synchronized (resource1) { System.out.println(Thread.currentThread() + \"get resource1\"); try { Thread.sleep(1000); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(Thread.currentThread() + \"waiting get resource2\"); synchronized (resource2) { System.out.println(Thread.currentThread() + \"get resource2\"); } } }, \"线程 1\").start(); new Thread(() -> { synchronized (resource2) { System.out.println(Thread.currentThread() + \"get resource2\"); try { Thread.sleep(1000); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(Thread.currentThread() + \"waiting get resource1\"); synchronized (resource1) { System.out.println(Thread.currentThread() + \"get resource1\"); } } }, \"线程 2\").start(); } } Thread[线程 1,5,main]get resource1 Thread[线程 2,5,main]get resource2 Thread[线程 1,5,main]waiting get resource2 Thread[线程 2,5,main]waiting get resource1 线程 A 通过 synchronized (resource1) 获得 resource1 的监视器锁，然后通过Thread.sleep(1000);让线程 A 休眠 1s 为的是让线程 B 得到执行然后获取到 resource2 的监视器锁。线程 A 和线程 B 休眠结束了都开始企图请求获取对方的资源，然后这两个线程就会陷入互相等待的状态，这也就产生了死锁。上面的例子符合产生死锁的四个必要条件。 如何找到死锁 Jstack查看死锁： 同样，也是进入jdk安装目录的bin下面，输入jps，先查看我们要检测死锁的进程： 然后可以看到进程Test的进程号：8384，然后执行：Jstack -l 8384 查看死锁信息： 如何避免与预防死锁 1. 死锁避免 死锁避免的基本思想：系统对进程发出的每一个系统能够满足的资源申请进行动态检查，并根据检查结果决定是否分配资源，如果分配后系统可能发生死锁，则不予分配，否则予以分配，这是一种保证系统不进入死锁状态的动态策略。 如果操作系统能保证所有进程在有限时间内得到需要的全部资源，则系统处于安全状态否则系统是不安全的。 安全状态是指：如果系统存在 由所有的安全序列{P1，P2，…Pn},则系统处于安全状态。一个进程序列是安全的，如果对其中每一个进程Pi(i >=1 && i 不安全状态：如果不存在任何一个安全序列，则系统处于不安全状态。他们之间的对对应关系如下图所示： 下面我们来通过一个例子对安全状态和不安全状态进行更深的了解 如上图所示系统处于安全状态，系统剩余3个资源，可以把其中的2个分配给P3，此时P3已经获得了所有的资源，执行完毕后还能还给系统4个资源，此时系统剩余5个资源所以满足（P2所需的资源不超过系统当前剩余量与P3当前占有资源量之和），同理P1也可以在P2执行完毕后获得自己需要的资源。 如果P1提出再申请一个资源的要求，系统从剩余的资源中分配一个给进程P1，此时系统剩余2个资源，新的状态图如下：那么是否仍是安全序列呢那我们来分析一下 系统当前剩余2个资源，分配给P3后P3执行完毕还给系统4个资源，但是P2需要5个资源，P1需要6个资源，他们都无法获得资源执行完成，因此找不到一个安全序列。此时系统转到了不安全状态。 2. 死锁预防 破坏互斥条件 如果资源不需要互斥访问，就可以破坏互斥条件。 对于某些硬件资源，可以采用特殊技术实现允许同时访问； 对于软件资源，无法实现。 破坏请求和保持条件 在执行时不再提出资源请求 系统要求所有进程要一次性地申请在整个运行过程中所需的全部资源。若系统有足够资源则完全分配。 在等待时不保持任何资源 只要有一个请求的资源不可用，就其它可用资源都不分配给它。 破坏不可剥夺条件 一个已拥有资源的进程，若它再提出新资源要求而不能立即得到满足时，它必须释放已经拥有的所有资源。以后需要时再重新申请。 实现复杂且要付出很大的代价（以前工作的失效，执行的推迟） 破坏环路条件 将系统资源进行统一排队，赋予不同编号；要求进程申请资源时，必须按照资源序号递增的顺序提出。这样在资源分配图中就不可能出现环路，破坏了“环路等待”条件 例如：系统中有下列设备：输入机（1），打印机（2），穿孔机（3），磁带机（4），磁盘（5）。有一进程要先后使用输入机、磁盘、打印机，则它申请设备时要按输入机、打印机、磁盘的顺序申请。 优点：同前两法相比，其资源利用率和系统吞吐量有较明显的改善。 缺点：进程实际需要资源的顺序不一定与资源的编号一致，因此仍会造成资源浪费；资源的序号必须相对稳定，从而限制了新设备的增加。 小结： 预防死锁的原理为设计不同的资源分配算法，来保证不发生死锁。 破坏请求和保持条件的资源分配算法是一种静态资源分配； 而破坏不可剥夺条件和环路条件是动态分配资源算法； 破坏环路条件是相对优异的一种预防死锁算法。 说说Sleep和Wait方法的区别 两者最主要的区别在于：sleep 方法没有释放锁，而 wait 方法释放了锁 。 两者都可以暂停线程的执行。 Wait 通常被用于线程间交互/通信，sleep 通常被用于暂停执行。 wait() 方法被调用后，线程不会自动苏醒，需要别的线程调用同一个对象上的 notify() 或者 notifyAll() 方法。sleep() 方法执行完成后，线程会自动苏醒。或者可以使用wait(long timeout)超时后线程会自动苏醒。 Synchronzed 使用方式 修饰实例方法，作用于当前对象实例加锁，进入同步代码前要获得当前对象实例的锁 修饰静态方法，作用于当前类对象加锁，进入同步代码前要获得当前类对象的锁 。 修饰代码块，指定加锁对象，对给定对象加锁，进入同步代码库前要获得给定对象的锁。 单例 public class Singleton { private volatile static Singleton uniqueInstance; // 第一步 private Singleton() { // 第二步，私有 } public static Singleton getUniqueInstance() { //先判断对象是否已经实例过，没有实例化过才进入加锁代码 if (uniqueInstance == null) { // 双重校验 //类对象加锁 synchronized (Singleton.class) { if (uniqueInstance == null) { uniqueInstance = new Singleton(); } } } return uniqueInstance; } } 注意：uniqueInstance 采用 volatile 关键字修饰也是很有必要 uniqueInstance = new Singleton(); 这段代码其实是分为三步执行： 为 uniqueInstance 分配内存空间 初始化 uniqueInstance 将 uniqueInstance 指向分配的内存地址 但是由于 JVM 具有指令重排的特性，执行顺序有可能变成 1->3->2。指令重排在单线程环境下不会出现问题，但是在多线程环境下会导致一个线程获得还没有初始化的实例。例如，线程 T1 执行了 1 和 3，此时 T2 调用 getUniqueInstance() 后发现 uniqueInstance 不为空，因此返回 uniqueInstance，但此时 uniqueInstance 还未被初始化。 所以采用volatile以保证程序的可见性和有序性 Synchronized 和 ReenTrantLock 的对比 两者都是可重入锁:两者都是可重入锁。“可重入锁”概念是：自己可以再次获取自己的内部锁。比如一个线程获得了某个对象的锁，此时这个对象锁还没有释放，当其再次想要获取这个对象的锁的时候还是可以获取的，如果不可锁重入的话，就会造成死锁。同一个线程每次获取锁，锁的计数器都自增1，所以要等到锁的计数器下降为0时才能释放锁。 Synchronized 依赖于 JVM 而 ReenTrantLock 依赖于 API:synchronized 是依赖于 JVM 实现的，前面我们也讲到了 虚拟机团队在 JDK1.6 为 synchronized 关键字进行了很多优化，但是这些优化都是在虚拟机层面实现的，并没有直接暴露给我们。ReenTrantLock 是 JDK 层面实现的（也就是 API 层面，需要 lock() 和 unlock 方法配合 try/finally 语句块来完成），所以我们可以通过查看它的源代码，来看它是如何实现的。 ReenTrantLock 比 Synchronized 增加了一些高级功能 等待可中断：过lock.lockInterruptibly()来实现这个机制。也就是说正在等待的线程可以选择放弃等待，改为处理其他事情。 可实现公平锁 可实现选择性通知（锁可以绑定多个条件）：线程对象可以注册在指定的Condition中，从而可以有选择性的进行线程通知，在调度线程上更加灵活。 在使用notify/notifyAll()方法进行通知时，被通知的线程是由 JVM 选择的，用ReentrantLock类结合Condition实例可以实现“选择性通知” 性能已不是选择标准：在jdk1.6之前synchronized 关键字吞吐量随线程数的增加，下降得非常严重。1.6之后，synchronized 和 ReenTrantLock 的性能基本是持平了。 底层原理 synchronized 关键字底层原理属于 JVM 层面。 synchronized 同步语句块的情况 synchronized 同步语句块的实现使用的是 monitorenter 和 monitorexit 指令，其中 monitorenter 指令指向同步代码块的开始位置，monitorexit 指令则指明同步代码块的结束位置。 当执行 monitorenter 指令时，线程试图获取锁也就是获取 monitor(monitor对象存在于每个Java对象的对象头中，synchronized 锁便是通过这种方式获取锁的，也是为什么Java中任意对象可以作为锁的原因) 的持有权.当计数器为0则可以成功获取，获取后将锁计数器设为1也就是加1。相应的在执行 monitorexit 指令后，将锁计数器设为0，表明锁被释放。如果获取对象锁失败，那当前线程就要阻塞等待，直到锁被另外一个线程释放为止。 synchronized 修饰方法的的情况 synchronized 修饰的方法并没有 monitorenter 指令和 monitorexit 指令，取得代之的确实是 ACC_SYNCHRONIZED 标识，该标识指明了该方法是一个同步方法，JVM 通过该 ACC_SYNCHRONIZED 访问标志来辨别一个方法是否声明为同步方法，从而执行相应的同步调用。 1.6版本的优化 在 Java 早期版本中，synchronized 属于重量级锁，效率低下，因为监视器锁（monitor）是依赖于底层的操作系统的 Mutex Lock 来实现的，Java 的线程是映射到操作系统的原生线程之上的。如果要挂起或者唤醒一个线程，都需要操作系统帮忙完成，而操作系统实现线程之间的切换时需要从用户态转换到内核态，这个状态之间的转换需要相对比较长的时间，时间成本相对较高，这也是为什么早期的 synchronized 效率低的原因。庆幸的是在 Java 6 之后 Java 官方对从 JVM 层面对synchronized 较大优化，所以现在的 synchronized 锁效率也优化得很不错了。JDK1.6对锁的实现引入了大量的优化，如自旋锁、适应性自旋锁、锁消除、锁粗化、偏向锁、轻量级锁等技术来减少锁操作的开销。 锁主要存在四中状态，依次是：无锁状态、偏向锁状态、轻量级锁状态、重量级锁状态，他们会随着竞争的激烈而逐渐升级。注意锁可以升级不可降级，这种策略是为了提高获得锁和释放锁的效率。 偏向锁 引入偏向锁的目的和引入轻量级锁的目的很像，他们都是为了没有多线程竞争的前提下，减少传统的重量级锁使用操作系统互斥量产生的性能消耗。但是不同是：轻量级锁在无竞争的情况下使用 CAS 操作去代替使用互斥量。而偏向锁在无竞争的情况下会把整个同步都消除掉。 偏向锁的“偏”就是偏心的偏，它的意思是会偏向于第一个获得它的线程，如果在接下来的执行中，该锁没有被其他线程获取，那么持有偏向锁的线程就不需要进行同步！ 但是对于锁竞争比较激烈的场合，偏向锁就失效了，因为这样场合极有可能每次申请锁的线程都是不相同的，因此这种场合下不应该使用偏向锁，否则会得不偿失，需要注意的是，偏向锁失败后，并不会立即膨胀为重量级锁，而是先升级为轻量级锁。 升级过程： 访问Mark Word中偏向锁的标识是否设置成1，锁标识位是否为01，确认偏向状态 如果为可偏向状态，则判断当前线程ID是否为偏向线程 如果偏向线程未当前线程，则通过cas操作竞争锁，如果竞争成功则操作Mark Word中线程ID设置为当前线程ID 如果cas偏向锁获取失败，则挂起当前偏向锁线程，偏向锁升级为轻量级锁。 轻量级锁 倘若偏向锁失败，虚拟机并不会立即升级为重量级锁，它还会尝试使用一种称为轻量级锁的优化手段(1.6之后加入的)。轻量级锁不是为了代替重量级锁，它的本意是在没有多线程竞争的前提下，减少传统的重量级锁使用操作系统互斥量产生的性能消耗，因为使用轻量级锁时，不需要申请互斥量。另外，轻量级锁的加锁和解锁都用到了CAS操作。 轻量级锁能够提升程序同步性能的依据是“对于绝大部分锁，在整个同步周期内都是不存在竞争的”，这是一个经验数据。如果没有竞争，轻量级锁使用 CAS 操作避免了使用互斥操作的开销。但如果存在锁竞争，除了互斥量开销外，还会额外发生CAS操作，因此在有锁竞争的情况下，轻量级锁比传统的重量级锁更慢！如果锁竞争激烈，那么轻量级将很快膨胀为重量级锁！ 升级过程： 线程由偏向锁升级为轻量级锁时，会先把锁的对象头MarkWord复制一份到线程的栈帧中，建立一个名为锁记录空间（Lock Record），用于存储当前Mark Word的拷贝。 虚拟机使用cas操作尝试将对象的Mark Word指向Lock Record的指针，并将Lock record里的owner指针指对象的Mark Word。 如果cas操作成功，则该线程拥有了对象的轻量级锁。第二个线程cas自旋锁等待锁线程释放锁。 如果多个线程竞争锁，轻量级锁要膨胀为重量级锁，Mark Word中存储的就是指向重量级锁（互斥量）的指针。其他等待线程进入阻塞状态。 自旋锁和自适应自旋 轻量级锁失败后，虚拟机为了避免线程真实地在操作系统层面挂起，还会进行一项称为自旋锁的优化手段。 互斥同步对性能最大的影响就是阻塞的实现，因为挂起线程/恢复线程的操作都需要转入内核态中完成（用户态转换到内核态会耗费时间）。 一般线程持有锁的时间都不是太长，所以仅仅为了这一点时间去挂起线程/恢复线程是得不偿失的。 所以，虚拟机的开发团队就这样去考虑：“我们能不能让后面来的请求获取锁的线程等待一会而不被挂起呢？看看持有锁的线程是否很快就会释放锁”。为了让一个线程等待，我们只需要让线程执行一个忙循环（自旋），这项技术就叫做自旋。 在 JDK1.6 中引入了自适应的自旋锁。自适应的自旋锁带来的改进就是：自旋的时间不在固定了，而是和前一次同一个锁上的自旋时间以及锁的拥有者的状态来决定，虚拟机变得越来越“聪明”了。 锁消除 锁消除理解起来很简单，它指的就是虚拟机即使编译器在运行时，如果检测到那些共享数据不可能存在竞争，那么就执行锁消除。锁消除可以节省毫无意义的请求锁的时间。 锁粗化 原则上，我们在编写代码的时候，总是推荐将同步块的作用范围限制得尽量小，——直在共享数据的实际作用域才进行同步，这样是为了使得需要同步的操作数量尽可能变小，如果存在锁竞争，那等待线程也能尽快拿到锁。 总结升级过程： 检测Mark Word里面是不是当前线程的ID，如果是，表示当前线程处于偏向锁 如果不是，则使用CAS将当前线程的ID替换Mard Word，如果成功则表示当前线程获得偏向锁，置偏向标志位1 如果失败，则说明发生竞争，撤销偏向锁，进而升级为轻量级锁。 当前线程使用CAS将对象头的Mark Word替换为锁记录指针，如果成功，当前线程获得锁 如果失败，表示其他线程竞争锁，当前线程便尝试使用自旋来获取锁。 如果自旋成功则依然处于轻量级状态。 如果自旋失败，则升级为重量级锁。 CAS 我们在读Concurrent包下的类的源码时，发现无论是ReenterLock内部的AQS，还是各种Atomic开头的原子类，内部都应用到了CAS public class Test { public AtomicInteger i; public void add() { i.getAndIncrement(); } } 我们来看getAndIncrement的内部： public final int getAndIncrement() { return unsafe.getAndAddInt(this, valueOffset, 1); } 再深入到getAndAddInt(): public final int getAndAddInt(Object var1, long var2, int var4) { int var5; do { var5 = this.getIntVolatile(var1, var2); } while(!this.compareAndSwapInt(var1, var2, var5, var5 + var4)); return var5; } 现在重点来了，compareAndSwapInt（var1, var2, var5, var5 + var4）其实换成compareAndSwapInt（obj, offset, expect, update）比较清楚，意思就是如果obj内的value和expect相等，就证明没有其他线程改变过这个变量，那么就更新它为update，如果这一步的CAS没有成功，那就采用自旋的方式继续进行CAS操作，取出乍一看这也是两个步骤了啊，其实在JNI里是借助于一个CPU指令完成的。所以还是原子操作。 CAS底层 UNSAFE_ENTRY(jboolean, Unsafe_CompareAndSwapInt(JNIEnv *env, jobject unsafe, jobject obj, jlong offset, jint e, jint x)) UnsafeWrapper(\"Unsafe_CompareAndSwapInt\"); oop p = JNIHandles::resolve(obj); jint* addr = (jint *) index_oop_from_field_offset_long(p, offset); return (jint)(Atomic::cmpxchg(x, addr, e)) == e; UNSAFE_END p是取出的对象，addr是p中offset处的地址，最后调用了Atomic::cmpxchg(x, addr, e), 其中参数x是即将更新的值，参数e是原内存的值。代码中能看到cmpxchg有基于各个平台的实现。 ABA问题 描述: 第一个线程取到了变量 x 的值 A，然后巴拉巴拉干别的事，总之就是只拿到了变量 x 的值 A。这段时间内第二个线程也取到了变量 x 的值 A，然后把变量 x 的值改为 B，然后巴拉巴拉干别的事，最后又把变量 x 的值变为 A （相当于还原了）。在这之后第一个线程终于进行了变量 x 的操作，但是此时变量 x 的值还是 A，所以 compareAndSet 操作是成功。 目前在JDK的atomic包里提供了一个类AtomicStampedReference来解决ABA问题。 public class ABADemo { static AtomicInteger atomicInteger = new AtomicInteger(100); static AtomicStampedReference atomicStampedReference = new AtomicStampedReference<>(100, 1); public static void main(String[] args) { System.out.println(\"=====ABA的问题产生=====\"); new Thread(() -> { atomicInteger.compareAndSet(100, 101); atomicInteger.compareAndSet(101, 100); }, \"t1\").start(); new Thread(() -> { // 保证线程1完成一次ABA问题 try { TimeUnit.SECONDS.sleep(1); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(atomicInteger.compareAndSet(100, 2020) + \" \" + atomicInteger.get()); }, \"t2\").start(); try { TimeUnit.SECONDS.sleep(2); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(\"=====解决ABA的问题=====\"); new Thread(() -> { int stamp = atomicStampedReference.getStamp(); // 第一次获取版本号 System.out.println(Thread.currentThread().getName() + \" 第1次版本号\" + stamp); try { TimeUnit.SECONDS.sleep(2); } catch (InterruptedException e) { e.printStackTrace(); } atomicStampedReference.compareAndSet(100, 101, atomicStampedReference.getStamp(), atomicStampedReference.getStamp() + 1); System.out.println(Thread.currentThread().getName() + \"\\t第2次版本号\" + atomicStampedReference.getStamp()); atomicStampedReference.compareAndSet(101, 100, atomicStampedReference.getStamp(), atomicStampedReference.getStamp() + 1); System.out.println(Thread.currentThread().getName() + \"\\t第3次版本号\" + atomicStampedReference.getStamp()); }, \"t3\").start(); new Thread(() -> { int stamp = atomicStampedReference.getStamp(); System.out.println(Thread.currentThread().getName() + \"\\t第1次版本号\" + stamp); try { TimeUnit.SECONDS.sleep(4); } catch (InterruptedException e) { e.printStackTrace(); } boolean result = atomicStampedReference.compareAndSet(100, 2020, stamp, stamp + 1); System.out.println(Thread.currentThread().getName() + \"\\t修改是否成功\" + result + \"\\t当前最新实际版本号：\" + atomicStampedReference.getStamp()); System.out.println(Thread.currentThread().getName() + \"\\t当前最新实际值：\" + atomicStampedReference.getReference()); }, \"t4\").start(); } } ThreadLocal 如果想实现每一个线程都有自己的专属本地变量该如何解决呢？ JDK中提供的ThreadLocal类正是为了解决这样的问题。 ThreadLocal类主要解决的就是让每个线程绑定自己的值，可以将ThreadLocal类形象的比喻成存放数据的盒子，盒子中可以存储每个线程的私有数据。**如果你创建了一个ThreadLocal变量，那么访问这个变量的每个线程都会有这个变量的本地副本，这也是ThreadLocal变量名的由来。他们可以使用 get（） 和 set（） 方法来获取默认值或将其值更改为当前线程所存的副本的值，从而避免了线程安全问题。** 原理 public class Thread implements Runnable { ...... //与此线程有关的ThreadLocal值。由ThreadLocal类维护 ThreadLocal.ThreadLocalMap threadLocals = null; //与此线程有关的InheritableThreadLocal值。由InheritableThreadLocal类维护 ThreadLocal.ThreadLocalMap inheritableThreadLocals = null; ...... } 从上面Thread类 源代码可以看出Thread 类中有一个 threadLocals 和 一个 inheritableThreadLocals 变量，它们都是 ThreadLocalMap 类型的变量,我们可以把 ThreadLocalMap 理解为ThreadLocal 类实现的定制化的 HashMap。默认情况下这两个变量都是null，只有当前线程调用 ThreadLocal 类的 set或get方法时才创建它们，实际上调用这两个方法的时候，我们调用的是ThreadLocalMap类对应的 get()、set()方法。 ThreadLocal类的set()方法 public void set(T value) { Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) map.set(this, value); else createMap(t, value); } ThreadLocalMap getMap(Thread t) { return t.threadLocals; } 最终的变量是放在了当前线程的 ThreadLocalMap 中，并不是存在 ThreadLocal 上，ThreadLocal 可以理解为只是ThreadLocalMap的封装，传递了变量值。 每个Thread中都具备一个ThreadLocalMap，而ThreadLocalMap可以存储以ThreadLocal为key的键值对。 比如我们在同一个线程中声明了两个 ThreadLocal 对象的话，会使用 Thread内部都是使用仅有那个ThreadLocalMap 存放数据的，ThreadLocalMap的 key 就是 ThreadLocal对象，value 就是 ThreadLocal 对象调用set方法设置的值。ThreadLocal 是 map结构是为了让每个线程可以关联多个 ThreadLocal变量。这也就解释了ThreadLocal声明的变量为什么在每一个线程都有自己的专属本地变量。 内存泄露 ThreadLocalMap 中使用的 key 为 ThreadLocal 的弱引用,而 value 是强引用。所以，如果 ThreadLocal 没有被外部强引用的情况下，在垃圾回收的时候会 key 会被清理掉，而 value 不会被清理掉。这样一来，ThreadLocalMap 中就会出现key为null的Entry。假如我们不做任何措施的话，value 永远无法被GC 回收，这个时候就可能会产生内存泄露。ThreadLocalMap实现中已经考虑了这种情况，在调用 set()、get()、remove() 方法的时候，会清理掉 key 为 null 的记录。使用完 ThreadLocal方法后 最好手动调用remove()方法 并发集合容器 为什么说ArrayList线程不安全？ 看add方法的源码 public boolean add(E e) { /** * 添加一个元素时，做了如下两步操作 * 1.判断列表的capacity容量是否足够，是否需要扩容 * 2.真正将元素放在列表的元素数组里面 */ ensureCapacityInternal(size + 1); // Increments modCount!! // 可能因为该操作，导致下一步发生数组越界 elementData[size++] = e; // 可能null值 return true; } 数组越界 列表大小为9，即size=9 线程A开始进入add方法，这时它获取到size的值为9，调用ensureCapacityInternal方法进行容量判断。 线程B此时也进入add方法，它获取到size的值也为9，也开始调用ensureCapacityInternal方法。 线程A发现需求大小为10，而elementData的大小就为10，可以容纳。于是它不再扩容，返回。 线程B也发现需求大小为10，也可以容纳，返回。 线程A开始进行设置值操作， elementData[size++] = e 操作。此时size变为10。 线程B也开始进行设置值操作，它尝试设置elementData[10] = e，而elementData没有进行过扩容，它的下标最大为9。于是此时会报出一个数组越界的异常ArrayIndexOutOfBoundsException. null值情况 elementData[size++] = e不是一个原子操作： elementData[size] = e; size = size + 1; 逻辑： 列表大小为0，即size=0 线程A开始添加一个元素，值为A。此时它执行第一条操作，将A放在了elementData下标为0的位置上。 接着线程B刚好也要开始添加一个值为B的元素，且走到了第一步操作。此时线程B获取到size的值依然为0，于是它将B也放在了elementData下标为0的位置上。 线程A开始将size的值增加为1 线程B开始将size的值增加为2 这样线程AB执行完毕后，理想中情况为size为2，elementData下标0的位置为A，下标1的位置为B。而实际情况变成了size为2，elementData下标为0的位置变成了B，下标1的位置上什么都没有。并且后续除非使用set方法修改此位置的值，否则将一直为null，因为size为2，添加元素时会从下标为2的位置上开始。 解决非安全集合的并发都有哪些？ ArrayList->Vector->SynchronizedList->CopyOnWriteArrayList ArraySet->SynchronizedSet->CopyOnWriteArraySet HashMap->SynchronizedMap->ConcurrentHashMap 并发同步容器 AQS原理 AQS 使用一个 int 成员变量来表示同步状态，通过内置的 FIFO 队列来完成获取资源线程的排队工作。AQS 使用 CAS 对该同步状态进行原子操作实现对其值的修改。 private volatile int state;//共享变量，使用volatile修饰保证线程可见性 状态信息通过 protected 类型的getState，setState，compareAndSetState进行操作 //返回同步状态的当前值 protected final int getState() { return state; } // 设置同步状态的值 protected final void setState(int newState) { state = newState; } //原子地（CAS操作）将同步状态值设置为给定值update如果当前同步状态的值等于expect（期望值） protected final boolean compareAndSetState(int expect, int update) { return unsafe.compareAndSwapInt(this, stateOffset, expect, update); } AQS 定义两种资源共享方式 Exclusive（独占）只有一个线程能执行，如 ReentrantLock。又可分为公平锁和非公平锁,ReentrantLock 同时支持两种锁。 总结：公平锁和非公平锁只有两处不同： 非公平锁在调用 lock 后，首先就会调用 CAS 进行一次抢锁，如果这个时候恰巧锁没有被占用，那么直接就获取到锁返回了。 非公平锁在 CAS 失败后，和公平锁一样都会进入到 tryAcquire 方法，在 tryAcquire 方法中，如果发现锁这个时候被释放了（state == 0），非公平锁会直接 CAS 抢锁，但是公平锁会判断等待队列是否有线程处于等待状态，如果有则不去抢锁，乖乖排到后面。 Share（共享）多个线程可同时执行，如 Semaphore/CountDownLatch。Semaphore、 CyclicBarrier、ReadWriteLock 。 AQS 使用了模板方法模式，自定义同步器时需要重写下面几个 AQS 提供的模板方法： isHeldExclusively()//该线程是否正在独占资源。只有用到condition才需要去实现它。 tryAcquire(int)//独占方式。尝试获取资源，成功则返回true，失败则返回false。 tryRelease(int)//独占方式。尝试释放资源，成功则返回true，失败则返回false。 tryAcquireShared(int)//共享方式。尝试获取资源。负数表示失败；0表示成功，但没有剩余可用资源；正数表示成功，且有剩余资源。 tryReleaseShared(int)//共享方式。尝试释放资源，成功则返回true，失败则返回false。 CountDownLatch CountDownLatch是共享锁的一种实现,它默认构造 AQS 的 state 值为 count。当线程使用countDown方法时,其实使用了tryReleaseShared方法以CAS的操作来减少state,直至state为0就代表所有的线程都调用了countDown方法。当调用await方法的时候，如果state不为0，就代表仍然有线程没有调用countDown方法，那么就把已经调用过countDown的线程都放入阻塞队列Park,并自旋CAS判断state == 0，直至最后一个线程调用了countDown，使得state == 0，于是阻塞的线程便判断成功，全部往下执行。 三种用法： 某一线程在开始运行前等待 n 个线程执行完毕。将 CountDownLatch 的计数器初始化为 n ：new CountDownLatch(n)，每当一个任务线程执行完毕，就将计数器减 1 countdownlatch.countDown()，当计数器的值变为 0 时，在CountDownLatch上 await() 的线程就会被唤醒。一个典型应用场景就是启动一个服务时，主线程需要等待多个组件加载完毕，之后再继续执行。 实现多个线程开始执行任务的最大并行性。注意是并行性，不是并发，强调的是多个线程在某一时刻同时开始执行。类似于赛跑，将多个线程放到起点，等待发令枪响，然后同时开跑。做法是初始化一个共享的 CountDownLatch 对象，将其计数器初始化为 1 ：new CountDownLatch(1)，多个线程在开始执行任务前首先 coundownlatch.await()，当主线程调用 countDown() 时，计数器变为 0，多个线程同时被唤醒。 死锁检测：一个非常方便的使用场景是，你可以使用 n 个线程访问共享资源，在每次测试阶段的线程数目是不同的，并尝试产生死锁。 public class CountDownLatchDemo { public static void main(String[] args) throws InterruptedException { countDownLatchTest(); // general(); } public static void general() { for (int i = 0; i { System.out.println(Thread.currentThread().getName() + \" 上完自习，离开教师\"); }, \"Thread --> \" + i).start(); } while (Thread.activeCount() > 2) { try { TimeUnit.SECONDS.sleep(2); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(Thread.currentThread().getName() + \" ====班长最后走人\"); } } public static void countDownLatchTest() throws InterruptedException { CountDownLatch countDownLatch = new CountDownLatch(6); for (int i = 0; i { System.out.println(Thread.currentThread().getName() + \" 上完自习，离开教师\"); countDownLatch.countDown(); }, \"Thread --> \" + i).start(); } countDownLatch.await(); System.out.println(Thread.currentThread().getName() + \" ====班长最后走人\"); } } CyclicBarrier CyclicBarrier 的字面意思是可循环使用（Cyclic）的屏障（Barrier）。它要做的事情是，让一组线程到达一个屏障（也可以叫同步点）时被阻塞，直到最后一个线程到达屏障时，屏障才会开门，所有被屏障拦截的线程才会继续干活。CyclicBarrier 默认的构造方法是 CyclicBarrier(int parties)，其参数表示屏障拦截的线程数量，每个线程调用await方法告诉 CyclicBarrier 我已经到达了屏障，然后当前线程被阻塞。 public class CyclicBarrierDemo { public static void main(String[] args) { cyclicBarrierTest(); } public static void cyclicBarrierTest() { CyclicBarrier cyclicBarrier = new CyclicBarrier(7, () -> { System.out.println(\"====召唤神龙====\"); }); for (int i = 0; i { System.out.println(Thread.currentThread().getName() + \" 收集到第\" + tempInt + \"颗龙珠\"); try { cyclicBarrier.await(); } catch (InterruptedException e) { e.printStackTrace(); } catch (BrokenBarrierException e) { e.printStackTrace(); } }, \"\" + i).start(); } } } 当调用 CyclicBarrier 对象调用 await() 方法时，实际上调用的是dowait(false, 0L)方法。 await() 方法就像树立起一个栅栏的行为一样，将线程挡住了，当拦住的线程数量达到 parties 的值时，栅栏才会打开，线程才得以通过执行。 // 当线程数量或者请求数量达到 count 时 await 之后的方法才会被执行。上面的示例中 count 的值就为 5。 private int count; /** * Main barrier code, covering the various policies. */ private int dowait(boolean timed, long nanos) throws InterruptedException, BrokenBarrierException, TimeoutException { final ReentrantLock lock = this.lock; // 锁住 lock.lock(); try { final Generation g = generation; if (g.broken) throw new BrokenBarrierException(); // 如果线程中断了，抛出异常 if (Thread.interrupted()) { breakBarrier(); throw new InterruptedException(); } // cout减1 int index = --count; // 当 count 数量减为 0 之后说明最后一个线程已经到达栅栏了，也就是达到了可以执行await 方法之后的条件 if (index == 0) { // tripped boolean ranAction = false; try { final Runnable command = barrierCommand; if (command != null) command.run(); ranAction = true; // 将 count 重置为 parties 属性的初始化值 // 唤醒之前等待的线程 // 下一波执行开始 nextGeneration(); return 0; } finally { if (!ranAction) breakBarrier(); } } // loop until tripped, broken, interrupted, or timed out for (;;) { try { if (!timed) trip.await(); else if (nanos > 0L) nanos = trip.awaitNanos(nanos); } catch (InterruptedException ie) { if (g == generation && ! g.broken) { breakBarrier(); throw ie; } else { // We're about to finish waiting even if we had not // been interrupted, so this interrupt is deemed to // \"belong\" to subsequent execution. Thread.currentThread().interrupt(); } } if (g.broken) throw new BrokenBarrierException(); if (g != generation) return index; if (timed && nanos 总结：CyclicBarrier 内部通过一个 count 变量作为计数器，cout 的初始值为 parties 属性的初始化值，每当一个线程到了栅栏这里了，那么就将计数器减一。如果 count 值为 0 了，表示这是这一代最后一个线程到达栅栏，就尝试执行我们构造方法中输入的任务。 Semaphore synchronized 和 ReentrantLock 都是一次只允许一个线程访问某个资源，Semaphore(信号量)可以指定多个线程同时访问某个资源。 public class SemaphoreDemo { public static void main(String[] args) { Semaphore semaphore = new Semaphore(3);// 模拟三个停车位 for (int i = 0; i { try { semaphore.acquire(); System.out.println(Thread.currentThread().getName() + \" 抢到车位\"); // 停车3s try { TimeUnit.SECONDS.sleep(3); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(Thread.currentThread().getName() + \" 停车3s后离开车位\"); } catch (Exception e) { e.printStackTrace(); } finally { semaphore.release(); } }, \"Car \" + i).start(); } } } 阻塞队列 ArrayBlockingQueue:由数组结构组成的有界阻塞队列. LinkedBlockingQueue:由链表结构组成的有界(但大小默认值Integer>MAX_VALUE)阻塞队列. PriorityBlockingQueue:支持优先级排序的无界阻塞队列. DelayQueue:使用优先级队列实现的延迟无界阻塞队列. SynchronousQueue:不存储元素的阻塞队列,也即是单个元素的队列. LinkedTransferQueue:由链表结构组成的无界阻塞队列. LinkedBlockingDuque:由链表结构组成的双向阻塞队列. 抛出异常方法：add remove 不抛异常：offer poll 阻塞 put take 带时间 offer poll 生产者消费者 synchronized版本的生产者和消费者，比较繁琐 public class ProdConsumerSynchronized { private final LinkedList lists = new LinkedList<>(); public synchronized void put(String s) { while (lists.size() != 0) { // 用while怕有存在虚拟唤醒线程 // 满了， 不生产了 try { this.wait(); } catch (InterruptedException e) { e.printStackTrace(); } } lists.add(s); System.out.println(Thread.currentThread().getName() + \" \" + lists.peekFirst()); this.notifyAll(); // 这里可是通知所有被挂起的线程，包括其他的生产者线程 } public synchronized void get() { while (lists.size() == 0) { try { this.wait(); } catch (InterruptedException e) { e.printStackTrace(); } } System.out.println(Thread.currentThread().getName() + \" \" + lists.removeFirst()); this.notifyAll(); // 通知所有被wait挂起的线程 用notify可能就死锁了。 } public static void main(String[] args) { ProdConsumerSynchronized prodConsumerSynchronized = new ProdConsumerSynchronized(); // 启动消费者线程 for (int i = 0; i { prodConsumerSynchronized.put(\"\" + tempI); }, \"ProdA\" + i).start(); } } } ReentrantLock public class ProdConsumerReentrantLock { private LinkedList lists = new LinkedList<>(); private Lock lock = new ReentrantLock(); private Condition prod = lock.newCondition(); private Condition cons = lock.newCondition(); public void put(String s) { lock.lock(); try { // 1. 判断 while (lists.size() != 0) { // 等待不能生产 prod.await(); } // 2.干活 lists.add(s); System.out.println(Thread.currentThread().getName() + \" \" + lists.peekFirst()); // 3. 通知 cons.signalAll(); } catch (Exception e) { e.printStackTrace(); } finally { lock.unlock(); } } public void get() { lock.lock(); try { // 1. 判断 while (lists.size() == 0) { // 等待不能消费 cons.await(); } // 2.干活 System.out.println(Thread.currentThread().getName() + \" \" + lists.removeFirst()); // 3. 通知 prod.signalAll(); } catch (Exception e) { e.printStackTrace(); } finally { lock.unlock(); } } public static void main(String[] args) { ProdConsumerReentrantLock prodConsumerReentrantLock = new ProdConsumerReentrantLock(); for (int i = 0; i { prodConsumerReentrantLock.put(tempI + \"\"); }, \"ProdA\" + i).start(); } for (int i = 0; i BlockingQueue public class ProdConsumerBlockingQueue { private volatile boolean flag = true; private AtomicInteger atomicInteger = new AtomicInteger(); BlockingQueue blockingQueue = null; public ProdConsumerBlockingQueue(BlockingQueue blockingQueue) { this.blockingQueue = blockingQueue; } public void myProd() throws Exception { String data = null; boolean retValue; while (flag) { data = atomicInteger.incrementAndGet() + \"\"; retValue = blockingQueue.offer(data, 2, TimeUnit.SECONDS); if (retValue) { System.out.println(Thread.currentThread().getName() + \" 插入队列\" + data + \" 成功\"); } else { System.out.println(Thread.currentThread().getName() + \" 插入队列\" + data + \" 失败\"); } TimeUnit.SECONDS.sleep(1); } System.out.println(Thread.currentThread().getName() + \" 大老板叫停了，flag=false，生产结束\"); } public void myConsumer() throws Exception { String result = null; while (flag) { result = blockingQueue.poll(2, TimeUnit.SECONDS); if (null == result || result.equalsIgnoreCase(\"\")) { flag = false; System.out.println(Thread.currentThread().getName() + \" 超过2s没有取到蛋糕，消费退出\"); return; } System.out.println(Thread.currentThread().getName() + \" 消费队列\" + result + \"成功\"); } } public void stop() { flag = false; } public static void main(String[] args) { ProdConsumerBlockingQueue prodConsumerBlockingQueue = new ProdConsumerBlockingQueue(new ArrayBlockingQueue<>(10)); new Thread(() -> { System.out.println(Thread.currentThread().getName() + \" 生产线程启动\"); try { prodConsumerBlockingQueue.myProd(); } catch (Exception e) { e.printStackTrace(); } }, \"Prod\").start(); new Thread(() -> { System.out.println(Thread.currentThread().getName() + \" 消费线程启动\"); try { prodConsumerBlockingQueue.myConsumer(); } catch (Exception e) { e.printStackTrace(); } }, \"Consumer\").start(); try { TimeUnit.SECONDS.sleep(5); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(\"5s后main叫停，线程结束\"); prodConsumerBlockingQueue.stop(); } } 线程池 线程池的好处 降低资源消耗。通过重复利用已创建的线程降低线程创建和销毁造成的消耗。 提高响应速度。当任务到达时，任务可以不需要的等到线程创建就能立即执行。 提高线程的可管理性。线程是稀缺资源，如果无限制的创建，不仅会消耗系统资源，还会降低系统的稳定性，使用线程池可以进行统一的分配，调优和监控。 FixedThreadPool FixedThreadPool 被称为可重用固定线程数的线程池。通过 Executors 类中的相关源代码来看一下相关实现： /** * 创建一个可重用固定数量线程的线程池 */ public static ExecutorService newFixedThreadPool(int nThreads, ThreadFactory threadFactory) { return new ThreadPoolExecutor(nThreads, nThreads, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue(), threadFactory); } 从上面源代码可以看出新创建的 FixedThreadPool 的 corePoolSize 和 maximumPoolSize 都被设置为 nThreads，这个 nThreads 参数是我们使用的时候自己传递的。 如果当前运行的线程数小于 corePoolSize， 如果再来新任务的话，就创建新的线程来执行任务； 当前运行的线程数等于 corePoolSize 后， 如果再来新任务的话，会将任务加入 LinkedBlockingQueue； 线程池中的线程执行完 手头的任务后，会在循环中反复从 LinkedBlockingQueue 中获取任务来执行； 不推荐使用 FixedThreadPool 使用无界队列 LinkedBlockingQueue（队列的容量为 Intger.MAX_VALUE）作为线程池的工作队列会对线程池带来如下影响 ： 当线程池中的线程数达到 corePoolSize 后，新任务将在无界队列中等待，因此线程池中的线程数不会超过 corePoolSize； 由于使用无界队列时 maximumPoolSize 将是一个无效参数，因为不可能存在任务队列满的情况。所以，通过创建 FixedThreadPool的源码可以看出创建的 FixedThreadPool 的 corePoolSize 和 maximumPoolSize 被设置为同一个值。 由于 1 和 2，使用无界队列时 keepAliveTime 将是一个无效参数； 运行中的 FixedThreadPool（未执行 shutdown()或 shutdownNow()）不会拒绝任务，在任务比较多的时候会导致 OOM（内存溢出）。 SingleThreadExecutor /** *返回只有一个线程的线程池 */ public static ExecutorService newSingleThreadExecutor(ThreadFactory threadFactory) { return new FinalizableDelegatedExecutorService (new ThreadPoolExecutor(1, 1, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue(), threadFactory)); } 和上面一个差不多，只不过core和max都被设置为1 CachedThreadPool /** * 创建一个线程池，根据需要创建新线程，但会在先前构建的线程可用时重用它。 */ public static ExecutorService newCachedThreadPool(ThreadFactory threadFactory) { return new ThreadPoolExecutor(0, Integer.MAX_VALUE, 60L, TimeUnit.SECONDS, new SynchronousQueue(), threadFactory); } CachedThreadPool 的corePoolSize 被设置为空（0），maximumPoolSize被设置为 Integer.MAX.VALUE，即它是无界的，这也就意味着如果主线程提交任务的速度高于 maximumPool 中线程处理任务的速度时，CachedThreadPool 会不断创建新的线程。极端情况下，这样会导致耗尽 cpu 和内存资源。 首先执行 SynchronousQueue.offer(Runnable task) 提交任务到任务队列。如果当前 maximumPool 中有闲线程正在执行 SynchronousQueue.poll(keepAliveTime,TimeUnit.NANOSECONDS)，那么主线程执行 offer 操作与空闲线程执行的 poll 操作配对成功，主线程把任务交给空闲线程执行，execute()方法执行完成，否则执行下面的步骤 2； 当初始 maximumPool 为空，或者 maximumPool 中没有空闲线程时，将没有线程执行 SynchronousQueue.poll(keepAliveTime,TimeUnit.NANOSECONDS)。这种情况下，步骤 1 将失败，此时 CachedThreadPool 会创建新线程执行任务，execute 方法执行完成； ScheduledThreadPoolExecutor省略，基本不会用 ThreadPoolExecutor（重点） /** * 用给定的初始参数创建一个新的ThreadPoolExecutor。 */ public ThreadPoolExecutor(int corePoolSize,//线程池的核心线程数量 int maximumPoolSize,//线程池的最大线程数 long keepAliveTime,//当线程数大于核心线程数时，多余的空闲线程存活的最长时间 TimeUnit unit,//时间单位 BlockingQueue workQueue,//任务队列，用来储存等待执行任务的队列 ThreadFactory threadFactory,//线程工厂，用来创建线程，一般默认即可 RejectedExecutionHandler handler//拒绝策略，当提交的任务过多而不能及时处理时，我们可以定制策略来处理任务 ) { if (corePoolSize ThreadPoolExecutor 3 个最重要的参数： corePoolSize : 核心线程数线程数定义了最小可以同时运行的线程数量。 maximumPoolSize : 当队列中存放的任务达到队列容量的时候，当前可以同时运行的线程数量变为最大线程数。 workQueue: 当新任务来的时候会先判断当前运行的线程数量是否达到核心线程数，如果达到的话，信任就会被存放在队列中。 ThreadPoolExecutor其他常见参数: keepAliveTime:当线程池中的线程数量大于 corePoolSize 的时候，如果这时没有新的任务提交，核心线程外的线程不会立即销毁，而是会等待，直到等待的时间超过了 keepAliveTime才会被回收销毁； unit : keepAliveTime 参数的时间单位。 threadFactory :executor 创建新线程的时候会用到。 handler :饱和策略。关于饱和策略下面单独介绍一下. 如果当前同时运行的线程数量达到最大线程数量并且队列也已经被放满了任时，ThreadPoolTaskExecutor 定义一些策略: ThreadPoolExecutor.AbortPolicy：抛出 RejectedExecutionException来拒绝新任务的处理。 ThreadPoolExecutor.CallerRunsPolicy：调用执行自己的线程运行任务。您不会任务请求。但是这种策略会降低对于新任务提交速度，影响程序的整体性能。另外，这个策略喜欢增加队列容量。如果您的应用程序可以承受此延迟并且你不能任务丢弃任何一个任务请求的话，你可以选择这个策略。 ThreadPoolExecutor.DiscardPolicy： 不处理新任务，直接丢弃掉。 ThreadPoolExecutor.DiscardOldestPolicy： 此策略将丢弃最早的未处理的任务请求。 Spring 通过 ThreadPoolTaskExecutor 或者我们直接通过 ThreadPoolExecutor 的构造函数创建线程池的时候，当我们不指定 RejectedExecutionHandler 饱和策略的话来配置线程池的时候默认使用的是 ThreadPoolExecutor.AbortPolicy。在默认情况下，ThreadPoolExecutor 将抛出 RejectedExecutionException 来拒绝新来的任务 ，这代表你将丢失对这个任务的处理。 对于可伸缩的应用程序，建议使用 ThreadPoolExecutor.CallerRunsPolicy。当最大池被填满时，此策略为我们提供可伸缩队列。（这个直接查看 ThreadPoolExecutor 的构造函数源码就可以看出，比较简单的原因，这里就不贴代码了。） Executors 返回线程池对象的弊端如下： FixedThreadPool 和 SingleThreadExecutor ： 允许请求的队列长度为 Integer.MAX_VALUE,可能堆积大量的请求，从而导致 OOM。 CachedThreadPool 和 ScheduledThreadPool ： 允许创建的线程数量为 Integer.MAX_VALUE ，可能会创建大量线程，从而导致 OOM。 public class ThreadPoolExecutorDemo { public static void main(String[] args) { ExecutorService threadpools = new ThreadPoolExecutor( 3, 5, 1l, TimeUnit.SECONDS, new LinkedBlockingDeque<>(3), Executors.defaultThreadFactory(), new ThreadPoolExecutor.AbortPolicy()); //new ThreadPoolExecutor.AbortPolicy(); //new ThreadPoolExecutor.CallerRunsPolicy(); //new ThreadPoolExecutor.DiscardOldestPolicy(); //new ThreadPoolExecutor.DiscardPolicy(); try { for (int i = 0; i { System.out.println(Thread.currentThread().getName() + \"\\t办理业务\"); }); } } catch (Exception e) { e.printStackTrace(); } finally { threadpools.shutdown(); } } } Java锁机制 公平锁/非公平锁 公平锁指多个线程按照申请锁的顺序来获取锁。非公平锁指多个线程获取锁的顺序并不是按照申请锁的顺序，有可能后申请的线程比先申请的线程优先获取锁。有可能，会造成优先级反转或者饥饿现象（很长时间都没获取到锁-非洲人…），ReentrantLock，了解一下。 可重入锁 可重入锁又名递归锁，是指在同一个线程在外层方法获取锁的时候，在进入内层方法会自动获取锁，典型的synchronized，了解一下 synchronized void setA() throws Exception { Thread.sleep(1000); setB(); // 因为获取了setA()的锁，此时调用setB()将会自动获取setB()的锁，如果不自动获取的话方法B将不会执行 } synchronized void setB() throws Exception { Thread.sleep(1000); } 独享锁/共享锁 独享锁：是指该锁一次只能被一个线程所持有。 共享锁：是该锁可被多个线程所持有。 互斥锁/读写锁 上面讲的独享锁/共享锁就是一种广义的说法，互斥锁/读写锁就是其具体的实现 乐观锁/悲观锁 乐观锁与悲观锁不是指具体的什么类型的锁，而是指看待兵法同步的角度。 悲观锁认为对于同一个人数据的并发操作，一定是会发生修改的，哪怕没有修改，也会认为修改。因此对于同一个数据的并发操作，悲观锁采取加锁的形式。悲观的认为，不加锁的并发操作一定会出现问题。 乐观锁则认为对于同一个数据的并发操作，是不会发生修改的。在更新数据的时候，会采用尝试更新，不断重新的方式更新数据。乐观的认为，不加锁的并发操作时没有事情的。 悲观锁适合写操作非常多的场景，乐观锁适合读操作非常多的场景，不加锁带来大量的性能提升。 悲观锁在Java中的使用，就是利用各种锁。乐观锁在Java中的使用，是无锁编程，常常采用的是CAS算法，典型的例子就是原子类，通过CAS自旋实现原子类操作的更新。重量级锁是悲观锁的一种，自旋锁、轻量级锁与偏向锁属于乐观锁 分段锁 分段锁其实是一种锁的设计，并不是具体的一种锁，对于ConcurrentHashMap而言，其并发的实现就是通过分段锁的形式来哦实现高效的并发操作。 以ConcurrentHashMap来说一下分段锁的含义以及设计思想，ConcurrentHashMap中的分段锁称为Segment，它即类似于HashMap（JDK7与JDK8中HashMap的实现）的结构，即内部拥有一个Entry数组，数组中的每个元素又是一个链表；同时又是ReentrantLock（Segment继承了ReentrantLock） 当需要put元素的时候，并不是对整个hashmap进行加锁，而是先通过hashcode来知道他要放在那一个分段中，然后对这个分段进行加锁，所以当多线程put的时候，只要不是放在一个分段中，就实现了真正的并行的插入。但是，在统计size的时候，可就是获取hashmap全局信息的时候，就需要获取所有的分段锁才能统计。 分段锁的设计目的是细化锁的粒度，当操作不需要更新整个数组的时候，就仅仅针对数组中的一项进行加锁操作。 偏向锁/轻量级锁/重量级锁 这三种锁是锁的状态，并且是针对Synchronized。在Java5通过引入锁升级的机制来实现高效Synchronized。这三种锁的状态是通过对象监视器在对象头中的字段来表明的。偏向锁是指一段同步代码一直被一个线程所访问，那么该线程会自动获取锁。降低获取锁的代价。 偏向锁的适用场景：始终只有一个线程在执行代码块，在它没有执行完释放锁之前，没有其它线程去执行同步快，在锁无竞争的情况下使用，一旦有了竞争就升级为轻量级锁，升级为轻量级锁的时候需要撤销偏向锁，撤销偏向锁的时候会导致stop the word操作；在有锁竞争时，偏向锁会多做很多额外操作，尤其是撤销偏向锁的时候会导致进入安全点，安全点会导致stw，导致性能下降，这种情况下应当禁用。 轻量级锁是指当锁是偏向锁的时候，被另一个线程锁访问，偏向锁就会升级为轻量级锁，其他线程会通过自选的形式尝试获取锁，不会阻塞，提高性能。 重量级锁是指当锁为轻量级锁的时候，另一个线程虽然是自旋，但自旋不会一直持续下去，当自旋一定次数的时候，还没有获取到锁，就会进入阻塞，该锁膨胀为重量级锁。重量级锁会让其他申请的线程进入阻塞，性能降低。 自旋锁 在Java中，自旋锁是指尝试获取锁的线程不会立即阻塞，而是采用循环的方式去尝试获取锁，这样的好处是减少线程上下文切换的消耗，缺点是循环会消耗CPU。 自旋锁原理非常简单，如果持有锁的线程能在很短时间内释放锁资源，那么那些等待竞争锁的线程就不需要做内核态和用户态之间的切换进入阻塞挂起状态，它们只需要等一等（自旋），等持有锁的线程释放锁后即可立即获取锁，这样就避免用户线程和内核的切换的消耗。 自旋锁尽可能的减少线程的阻塞，适用于锁的竞争不激烈，且占用锁时间非常短的代码来说性能能大幅度的提升，因为自旋的消耗会小于线程阻塞挂起再唤醒的操作的消耗。 但是如果锁的竞争激烈，或者持有锁的线程需要长时间占用锁执行同步块，这时候就不适用使用自旋锁了，因为自旋锁在获取锁前一直都是占用cpu做无用功，同时有大量线程在竞争一个锁，会导致获取锁的时间很长，线程自旋的消耗大于线程阻塞挂起操作的消耗，其它需要cpu的线程又不能获取到cpu，造成cpu的浪费。 Java锁总结 Java锁机制可归为Sychornized锁和Lock锁两类。Synchronized是基于JVM来保证数据同步的，而Lock则是硬件层面，依赖特殊的CPU指令来实现数据同步的。 Synchronized是一个非公平、悲观、独享、互斥、可重入的重量级锁。 ReentrantLock是一个默认非公平但可实现公平的、悲观、独享、互斥、可重入、重量级锁。 ReentrantReadWriteLock是一个默认非公平但可实现公平的、悲观、写独享、读共享、读写、可重入、重量级锁。 "},"basic/collection/1.java集合.html":{"url":"basic/collection/1.java集合.html","title":"1.集合架构","keywords":"","body":"概述:注意:总结:Connection接口:Map接口:重点问题重点分析:(一).TreeSet, LinkedHashSet and HashSet 的区别(二).TreeSet的两种排序方式比较(三). 性能测试概述: List , Set, Map都是接口，前两个继承至Collection接口，Map为独立接口 Set下有HashSet，LinkedHashSet，TreeSet List下有ArrayList，Vector，LinkedList Map下有Hashtable，LinkedHashMap，HashMap，TreeMap Collection接口下还有个Queue接口，有PriorityQueue类 注意: Queue接口与List、Set同一级别，都是继承了Collection接口。 看图你会发现,LinkedList既可以实现Queue接口,也可以实现List接口.只不过呢, LinkedList实现了Queue接口。Queue接口窄化了对LinkedList的方法的访问权限（即在方法中的参数类型如果是Queue时，就完全只能访问Queue接口所定义的方法 了，而不能直接访问 LinkedList的非Queue的方法），以使得只有恰当的方法才可以使用。 SortedSet是个接口，它里面的（只有TreeSet这一个实现可用）中的元素一定是有序的。 总结: Connection接口: — List 有序,可重复 ArrayList 优点: 底层数据结构是数组，查询快，增删慢。 缺点: 线程不安全，效率高 Vector 优点: 底层数据结构是数组，查询快，增删慢。 缺点: 线程安全，效率低 LinkedList 优点: 底层数据结构是链表，查询慢，增删快。 缺点: 线程不安全，效率高 —Set 无序,唯一 HashSet 底层数据结构是哈希表。(无序,唯一) 如何来保证元素唯一性? 1.依赖两个方法：hashCode()和equals() equals()返回True,hashCode返回相同 LinkedHashSet 底层数据结构是链表和哈希表。(FIFO插入有序,唯一) 1.由链表保证元素有序 2.由哈希表保证元素唯一 TreeSet 底层数据结构是红黑树。(唯一，有序) 如何保证元素排序的呢? 自然排序 比较器排序 2.如何保证元素唯一性的呢? 根据比较的返回值是否是0来决定 针对Collection集合我们到底使用谁呢?(掌握) 唯一吗? 是：Set 排序吗? 是：TreeSet或LinkedHashSet 否：HashSet 如果你知道是Set，但是不知道是哪个Set，就用HashSet。 否：List 要安全吗? 是：Vector 否：ArrayList或者LinkedList 查询多：ArrayList 增删多：LinkedList 如果你知道是List，但是不知道是哪个List，就用ArrayList。 如果你知道是Collection集合，但是不知道使用谁，就用ArrayList。 如果你知道用集合，就用ArrayList。 说完了Collection,来简单说一下Map. Map接口: 上图: Map接口有三个比较重要的实现类，分别是HashMap、TreeMap和HashTable。 TreeMap是有序的，HashMap和HashTable是无序的。 Hashtable的方法是同步的，HashMap的方法不是同步的。这是两者最主要的区别。 这就意味着: Hashtable是线程安全的，HashMap不是线程安全的。 HashMap效率较高，Hashtable效率较低。 如果对同步性或与遗留代码的兼容性没有任何要求，建议使用HashMap。 查看Hashtable的源代码就可以发现，除构造函数外，Hashtable的所有 public 方法声明中都有 synchronized关键字，而HashMap的源码中则没有。 Hashtable不允许null值，HashMap允许null值（key和value都允许） 父类不同：Hashtable的父类是Dictionary，HashMap的父类是AbstractMap 重点问题重点分析: (一).TreeSet, LinkedHashSet and HashSet 的区别 1. 介绍 TreeSet, LinkedHashSet and HashSet 在java中都是实现Set的数据结构 TreeSet的主要功能用于排序 LinkedHashSet的主要功能用于保证FIFO即有序的集合(先进先出) HashSet只是通用的存储数据的集合 2. 相同点 Duplicates elements: 因为三者都实现Set interface，所以三者都不包含duplicate elements Thread safety: 三者都不是线程安全的，如果要使用线程安全可以Collections.synchronizedSet() 3. 不同点 Performance and Speed: HashSet插入数据最快，其次LinkHashSet，最慢的是TreeSet因为内部实现排序 Ordering: HashSet不保证有序，LinkHashSet保证FIFO即按插入顺序排序，TreeSet安装内部实现排序，也可以自定义排序规则 null:HashSet和LinkHashSet允许存在null数据，但是TreeSet中插入null数据时会报NullPointerException 4. 代码比较 public static void main(String args[]) { HashSet hashSet = new HashSet<>(); LinkedHashSet linkedHashSet = new LinkedHashSet<>(); TreeSet treeSet = new TreeSet<>(); for (String data : Arrays.asList(\"B\", \"E\", \"D\", \"C\", \"A\")) { hashSet.add(data); linkedHashSet.add(data); treeSet.add(data); } //不保证有序 System.out.println(\"Ordering in HashSet :\" + hashSet); //FIFO保证安装插入顺序排序 System.out.println(\"Order of element in LinkedHashSet :\" + linkedHashSet); //内部实现排序 System.out.println(\"Order of objects in TreeSet :\" + treeSet); } 12345678910111213141516171819202122 运行结果: Ordering in HashSet :[A, B, C, D, E] (无顺序) Order of element in LinkedHashSet :[B, E, D, C, A] (FIFO插入有序) Order of objects in TreeSet :[A, B, C, D, E] (排序) (二).TreeSet的两种排序方式比较 1.排序的引入(以基本数据类型的排序为例) 由于TreeSet可以实现对元素按照某种规则进行排序，例如下面的例子 public class MyClass { public static void main(String[] args) { // 创建集合对象 // 自然顺序进行排序 TreeSet ts = new TreeSet(); // 创建元素并添加 // 20,18,23,22,17,24,19,18,24 ts.add(20); ts.add(18); ts.add(23); ts.add(22); ts.add(17); ts.add(24); ts.add(19); ts.add(18); ts.add(24); // 遍历 for (Integer i : ts) { System.out.println(i); } } } 运行结果: 17 18 19 20 22 23 24 2.如果是引用数据类型呢,比如自定义对象,又该如何排序呢? 测试类: public class MyClass { public static void main(String[] args) { TreeSet ts=new TreeSet(); //创建元素对象 Student s1=new Student(\"zhangsan\",20); Student s2=new Student(\"lis\",22); Student s3=new Student(\"wangwu\",24); Student s4=new Student(\"chenliu\",26); Student s5=new Student(\"zhangsan\",22); Student s6=new Student(\"qianqi\",24); //将元素对象添加到集合对象中 ts.add(s1); ts.add(s2); ts.add(s3); ts.add(s4); ts.add(s5); ts.add(s6); //遍历 for(Student s:ts){ System.out.println(s.getName()+\"-----------\"+s.getAge()); } } } Student.java: public class Student { private String name; private int age; public Student() { super(); // TODO Auto-generated constructor stub } public Student(String name, int age) { super(); this.name = name; this.age = age; } public String getName() { return name; } public void setName(String name) { this.name = name; } public int getAge() { return age; } public void setAge(int age) { this.age = age; } } 1234567891011121314151617181920212223242526272829303132 结果报错: 原因分析： 由于不知道该安照那一中排序方式排序，所以会报错。 解决方法： 1.自然排序 2.比较器排序 (1).自然排序 自然排序要进行一下操作： 1.Student类中实现 Comparable接口 2.重写Comparable接口中的Compareto方法 compareTo(T o) 比较此对象与指定对象的顺序。 1 public class Student implements Comparable{ private String name; private int age; public Student() { super(); // TODO Auto-generated constructor stub } public Student(String name, int age) { super(); this.name = name; this.age = age; } public String getName() { return name; } public void setName(String name) { this.name = name; } public int getAge() { return age; } public void setAge(int age) { this.age = age; } @Override public int compareTo(Student s) { //return -1; //-1表示放在红黑树的左边,即逆序输出 //return 1; //1表示放在红黑树的右边，即顺序输出 //return o; //表示元素相同，仅存放第一个元素 //主要条件 姓名的长度,如果姓名长度小的就放在左子树，否则放在右子树 int num=this.name.length()-s.name.length(); //姓名的长度相同，不代表内容相同,如果按字典顺序此 String 对象位于参数字符串之前，则比较结果为一个负整数。 //如果按字典顺序此 String 对象位于参数字符串之后，则比较结果为一个正整数。 //如果这两个字符串相等，则结果为 0 int num1=num==0?this.name.compareTo(s.name):num; //姓名的长度和内容相同，不代表年龄相同，所以还要判断年龄 int num2=num1==0?this.age-s.age:num1; return num2; } } 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748 运行结果: lis-----------22 qianqi-----------24 wangwu-----------24 chenliu-----------26 zhangsan-----------20 zhangsan-----------22 (2).比较器排序 比较器排序步骤： 1.单独创建一个比较类，这里以MyComparator为例，并且要让其继承Comparator接口 2.重写Comparator接口中的Compare方法 compare(T o1,T o2) 比较用来排序的两个参数。 1 3.在主类中使用下面的 构造方法 TreeSet(Comparator comparator) 构造一个新的空 TreeSet，它根据指定比较器进行排序。 12 测试类: public class MyClass { public static void main(String[] args) { //创建集合对象 //TreeSet(Comparator comparator) 构造一个新的空 TreeSet，它根据指定比较器进行排序。 TreeSet ts=new TreeSet(new MyComparator()); //创建元素对象 Student s1=new Student(\"zhangsan\",20); Student s2=new Student(\"lis\",22); Student s3=new Student(\"wangwu\",24); Student s4=new Student(\"chenliu\",26); Student s5=new Student(\"zhangsan\",22); Student s6=new Student(\"qianqi\",24); //将元素对象添加到集合对象中 ts.add(s1); ts.add(s2); ts.add(s3); ts.add(s4); ts.add(s5); ts.add(s6); //遍历 for(Student s:ts){ System.out.println(s.getName()+\"-----------\"+s.getAge()); } } } 123456789101112131415161718192021222324252627282930 Student.java: public class Student { private String name; private int age; public Student() { super(); // TODO Auto-generated constructor stub } public Student(String name, int age) { super(); this.name = name; this.age = age; } public String getName() { return name; } public void setName(String name) { this.name = name; } public int getAge() { return age; } public void setAge(int age) { this.age = age; } } 123456789101112131415161718192021222324252627282930313233 MyComparator类： public class MyComparator implements Comparator { @Override public int compare(Student s1,Student s2) { // 姓名长度 int num = s1.getName().length() - s2.getName().length(); // 姓名内容 int num2 = num == 0 ? s1.getName().compareTo(s2.getName()) : num; // 年龄 int num3 = num2 == 0 ? s1.getAge() - s2.getAge() : num2; return num3; } } 1234567891011121314 运行结果: lis-----------22 qianqi-----------24 wangwu-----------24 chenliu-----------26 zhangsan-----------20 zhangsan-----------22 (三). 性能测试 对象类: class Dog implements Comparable { int size; public Dog(int s) { size = s; } public String toString() { return size + \"\"; } @Override public int compareTo(Dog o) { //数值大小比较 return size - o.size; } } 1234567891011121314 主类: public class MyClass { public static void main(String[] args) { Random r = new Random(); HashSet hashSet = new HashSet(); TreeSet treeSet = new TreeSet(); LinkedHashSet linkedSet = new LinkedHashSet(); // start time long startTime = System.nanoTime(); for (int i = 0; i 运行结果: HashSet: 1544313 TreeSet: 2066049 LinkedHashSet: 629826 虽然测试不够准确,但能反映得出，TreeSet要慢得多,因为它是有序的。 "},"basic/collection/2.HashMap源码分析.html":{"url":"basic/collection/2.HashMap源码分析.html","title":"2.hashMap底层原理","keywords":"","body":"一. 简介二.数据结构2.1 主要介绍2.2 存储流程2.3 数组元素&链表节点的实现类2.4 红黑树节点 实现类三 .具体使用四 基础知识:HashMap中的重要参数(变量)HashMap1.8如何解决闭环问题九 HashMap常见面试题一. 简介 public class HashMap extends AbstractMap implements Map, Cloneable, Serializable 二.数据结构 2.1 主要介绍 2.2 存储流程 2.3 数组元素&链表节点的实现类 HashMap中的数组元素 & 链表节点 采用 Node类 实现 与 JDK 1.7 的对比（Entry类），仅仅只是换了名字 该类的源码分析如下 /** * Node = HashMap的内部类，实现了Map.Entry接口，本质是 = 一个映射(键值对) * 实现了getKey()、getValue()、equals(Object o)和hashCode()等方法 **/ static class Node implements Map.Entry { final int hash; // 哈希值，HashMap根据该值确定记录的位置 final K key; // key V value; // value Node next;// 链表下一个节点 // 构造方法 Node(int hash, K key, V value, Node next) { this.hash = hash; this.key = key; this.value = value; this.next = next; } public final K getKey() { return key; } // 返回 与 此项 对应的键 public final V getValue() { return value; } // 返回 与 此项 对应的值 public final String toString() { return key + \"=\" + value; } public final V setValue(V newValue) { V oldValue = value; value = newValue; return oldValue; } /** * hashCode（） */ public final int hashCode() { return Objects.hashCode(key) ^ Objects.hashCode(value); } /** * equals（） * 作用：判断2个Entry是否相等，必须key和value都相等，才返回true */ public final boolean equals(Object o) { if (o == this) return true; if (o instanceof Map.Entry) { Map.Entry e = (Map.Entry)o; if (Objects.equals(key, e.getKey()) && Objects.equals(value, e.getValue())) return true; } return false; } } 2.4 红黑树节点 实现类 HashMap中的红黑树节点 采用 TreeNode 类 实现 /** * 红黑树节点 实现类：继承自LinkedHashMap.Entry类 */ static final class TreeNode extends LinkedHashMap.Entry { // 属性 = 父节点、左子树、右子树、删除辅助节点 + 颜色 TreeNode parent; TreeNode left; TreeNode right; TreeNode prev; boolean red; // 构造函数 TreeNode(int hash, K key, V val, Node next) { super(hash, key, val, next); } // 返回当前节点的根节点 final TreeNode root() { for (TreeNode r = this, p;;) { if ((p = r.parent) == null) return r; r = p; } } 三 .具体使用 V get(Object key); // 获得指定键的值 V put(K key, V value); // 添加键值对 void putAll(Map m); // 将指定Map中的键值对 复制到 此Map中 V remove(Object key); // 删除该键值对 boolean containsKey(Object key); // 判断是否存在该键的键值对；是 则返回true boolean containsValue(Object value); // 判断是否存在该值的键值对；是 则返回true Set keySet(); // 单独抽取key序列，将所有key生成一个Set Collection values(); // 单独value序列，将所有value生成一个Collection void clear(); // 清除哈希表中的所有键值对 int size(); // 返回哈希表中所有 键值对的数量 = 数组中的键值对 + 链表中的键值对 boolean isEmpty(); // 判断HashMap是否为空；size == 0时 表示为 空 四 基础知识:HashMap中的重要参数(变量) 在进行真正的源码分析前，先讲解HashMap中的重要参数（变量） HashMap中的主要参数 同 JDK 1.7 ，即：容量、加载因子、扩容阈值 但由于数据结构中引入了 红黑树，故加入了 与红黑树相关的参数。具体介绍如下： ``` /** 主要参数 同 JDK 1.7 即：容量、加载因子、扩容阈值（要求、范围均相同） */ // 1. 容量（capacity）： 必须是2的幂 & // 2. 加载因子(Load factor)：HashMap在其容量自动增加前可达到多满的一种尺度 final float loadFactor; // 实际加载因子 static final float DEFAULT_LOAD_FACTOR = 0.75f; // 默认加载因子 = 0.75 // 3. 扩容阈值（threshold）：当哈希表的大小 ≥ 扩容阈值时，就会扩容哈希表（即扩充HashMap的容量） // a. 扩容 = 对哈希表进行resize操作（即重建内部数据结构），从而哈希表将具有大约两倍的桶数 // b. 扩容阈值 = 容量 x 加载因子 int threshold; // 4. 其他 transient Node[] table; // 存储数据的Node类型 数组，长度 = 2的幂；数组的每个元素 = 1个单链表 transient int size;// HashMap的大小，即 HashMap中存储的键值对的数量 /** * 与红黑树相关的参数 */ // 1. 桶的树化阈值：即 链表转成红黑树的阈值，在存储数据时，当链表长度 > 该值时，则将链表转换成红黑树 static final int TREEIFY_THRESHOLD = 8; // 2. 桶的链表还原阈值：即 红黑树转为链表的阈值，当在扩容（resize（））时（此时HashMap的数据存储位置会重新计算），在重新计算存储位置后，当原有的红黑树内数量 该值时，才允许树形化链表 （即 将链表 转换成红黑树） // 否则，若桶内元素太多时，则直接扩容，而不是树形化 // 为了避免进行扩容、树形化选择的冲突，这个值不能小于 4 * TREEIFY_THRESHOLD static final int MIN_TREEIFY_CAPACITY = 64; 加载因子: ![59547126049](https://gitee.com/zisuu/picture/raw/master/img/20201221222556.png) ![59547157269](https://gitee.com/zisuu/picture/raw/master/img/20201221222603.png) ## 五 . 源码分析 ![示意图](https://gitee.com/zisuu/picture/raw/master/img/20201221222609.png) ### 5.1 声明一个HashMap /** 函数使用原型 */ Map map = new HashMap(); /** 源码分析：主要是HashMap的构造函数 = 4个 仅贴出关于HashMap构造函数的源码 */ public class HashMap extends AbstractMap implements Map, Cloneable, Serializable{ // 省略上节阐述的参数 /** * 构造函数1：默认构造函数（无参） * 加载因子 & 容量 = 默认 = 0.75、16 */ public HashMap() { this.loadFactor = DEFAULT_LOAD_FACTOR; } /** * 构造函数2：指定“容量大小”的构造函数 * 加载因子 = 默认 = 0.75 、容量 = 指定大小 */ public HashMap(int initialCapacity) { // 实际上是调用指定“容量大小”和“加载因子”的构造函数 // 只是在传入的加载因子参数 = 默认加载因子 this(initialCapacity, DEFAULT_LOAD_FACTOR); } /** * 构造函数3：指定“容量大小”和“加载因子”的构造函数 * 加载因子 & 容量 = 自己指定 */ public HashMap(int initialCapacity, float loadFactor) { // 指定初始容量必须非负，否则报错 if (initialCapacity 最大容量 if (initialCapacity > MAXIMUM_CAPACITY) initialCapacity = MAXIMUM_CAPACITY; // 填充比必须为正 if (loadFactor 传入容量大小的最小的2的幂，该阈值后面会重新计算 // 下面会详细讲解 ->> 分析1 this.threshold = tableSizeFor(initialCapacity); } /** * 构造函数4：包含“子Map”的构造函数 * 即 构造出来的HashMap包含传入Map的映射关系 * 加载因子 & 容量 = 默认 */ public HashMap(Map m) { // 设置容量大小 & 加载因子 = 默认 this.loadFactor = DEFAULT_LOAD_FACTOR; // 将传入的子Map中的全部元素逐个添加到HashMap中 putMapEntries(m, false); } } /** * 分析1：tableSizeFor(initialCapacity) * 作用：将传入的容量大小转化为：>传入容量大小的最小的2的幂 * 与JDK 1.7对比：类似于JDK 1.7 中 inflateTable()里的 roundUpToPowerOf2(toSize) */ static final int tableSizeFor(int cap) { int n = cap - 1; n |= n >>> 1; n |= n >>> 2; n |= n >>> 4; n |= n >>> 8; n |= n >>> 16; return (n = MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + 1; } 注：（同JDK 1.7类似） 此处仅用于接收初始容量大小（capacity）、加载因子(Load factor)，但仍无真正初始化哈希表，即初始化存储数组table 此处先给出结论：真正初始化哈希表（初始化存储数组table）是在第1次添加键值对时，即第1次调用put（）时。下面会详细说明 至此，关于HashMap的构造函数讲解完毕。 ### 5.2 填键值对 ![示意图](https://gitee.com/zisuu/picture/raw/master/img/20201221222619.png) ![](https://gitee.com/zisuu/picture/raw/master/img/20201221222629.png) /** 函数使用原型 */ map.put(\"Android\", 1); map.put(\"Java\", 2); map.put(\"iOS\", 3); map.put(\"数据挖掘\", 4); map.put(\"产品经理\", 5); /** 源码分析：主要分析HashMap的put函数 */ public V put(K key, V value) { // 1. 对传入数组的键Key计算Hash值 ->>分析1 // 2. 再调用putVal（）添加数据进去 ->>分析2 return putVal(hash(key), key, value, false, true); } ##### 5.2.1 hash /** * 分析1：hash(key) * 作用：计算传入数据的哈希码（哈希值、Hash值） * 该函数在JDK 1.7 和 1.8 中的实现不同，但原理一样 = 扰动函数 = 使得根据key生成的哈希码（hash值）分布更加均匀、更具备随机性，避免出现hash值冲突（即指不同key但生成同1个hash值） * JDK 1.7 做了9次扰动处理 = 4次位运算 + 5次异或运算 * JDK 1.8 简化了扰动函数 = 只做了2次扰动 = 1次位运算 + 1次异或运算 */ // JDK 1.7实现：将 键key 转换成 哈希码（hash值）操作 = 使用hashCode() + 4次位运算 + 5次异或运算（9次扰动） static final int hash(int h) { h ^= k.hashCode(); h ^= (h >>> 20) ^ (h >>> 12); return h ^ (h >>> 7) ^ (h >>> 4); } // JDK 1.8实现：将 键key 转换成 哈希码（hash值）操作 = 使用hashCode() + 1次位运算 + 1次异或运算（2次扰动） // 1. 取hashCode值： h = key.hashCode() // 2. 高位参与低位的运算：h ^ (h >>> 16) static final int hash(Object key) { int h; return (key == null) ? 0 : (h = key.hashCode()) ^ (h >>> 16); // a. 当key = null时，hash值 = 0，所以HashMap的key 可为null // 注：对比HashTable，HashTable对key直接hashCode（），若key为null时，会抛出异常，所以HashTable的key不可为null // b. 当key ≠ null时，则通过先计算出 key的 hashCode()（记为h），然后 对哈希码进行 扰动处理： 按位 异或（^） 哈希码自身右移16位后的二进制 } /** * 计算存储位置的函数分析：indexFor(hash, table.length) * 注：该函数仅存在于JDK 1.7 ，JDK 1.8中实际上无该函数（直接用1条语句判断写出），但原理相同 * 为了方便讲解，故提前到此讲解 */ static int indexFor(int h, int length) { return h & (length-1); // 将对哈希码扰动处理后的结果 与运算(&) （数组长度-1），最终得到存储在数组table的位置（即数组下标、索引） } - 总结 计算存放在数组 table 中的位置（即数组下标、索引）的过程 > 1. 此处与 `JDK 1.7`的区别在于：`hash`值的求解过程中 哈希码的二次处理方式（扰动处理） > 2. 步骤1、2 = `hash`值的求解过程 ![示意图](https://gitee.com/zisuu/picture/raw/master/img/20201221222638.png) ![示意图](https://gitee.com/zisuu/picture/raw/master/img/20201221222642.png) ![59547226412](D:\\Typora\\java核心\\容器\\assets\\1595472264128.png) ##### 5.2.2 为什么不直接采用hashcode的哈希码? ![示意图](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWdjb252ZXJ0LmNzZG5pbWcuY24vYUhSMGNITTZMeTlwYldkamIyNTJaWEowTG1OelpHNXBiV2N1WTI0dllVaFNNR05FYjNaTU0xWjNZa2M1YUZwRE1YQmlWMFp1V2xoTmRXRnRiR2hpYms1dlpGTTFjR0o1T1RGalIzaDJXVmRTWm1GWE1XaGFNbFo2VEhwck1FNUVUVEpPVXpGclRWUm9iRnBVUVRKUFZHUm9UVmRGZUZscVZYcE1ia0oxV25vNWNHSlhSbTVhVlRGMldqTkplVXd5UmpGa1J6aDBZak5LY0ZwWE5UQk1NMDR3WTIxc2QwcFVaRVJoVnpGb1dqSldWMkZYVmpOTmFUaDVURE5qZGsxVVNUQk5RUQ?x-oss-process=image/format,png) 为了解决 “哈希码与数组大小范围不匹配” 的问题，`HashMap`给出了解决方案：**哈希码 与运算（&） （数组长度-1）**，即问题3 ![示意图](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWdjb252ZXJ0LmNzZG5pbWcuY24vYUhSMGNITTZMeTlwYldkamIyNTJaWEowTG1OelpHNXBiV2N1WTI0dllVaFNNR05FYjNaTU0xWjNZa2M1YUZwRE1YQmlWMFp1V2xoTmRXRnRiR2hpYms1dlpGTTFjR0o1T1RGalIzaDJXVmRTWm1GWE1XaGFNbFo2VEhwck1FNUVUVEpPVXpCM1RrUlpNVTlFWXpWTk1rcG9XbFJHYkZwRWEzZE1ia0oxV25vNWNHSlhSbTVhVlRGMldqTkplVXd5UmpGa1J6aDBZak5LY0ZwWE5UQk1NMDR3WTIxc2QwcFVaRVJoVnpGb1dqSldWMkZYVmpOTmFUaDVURE5qZGsxVVNUQk5RUQ?x-oss-process=image/format,png) ##### 5.2.3 为什么要做二次扰动? 加大哈希码低位的随机性，使得分布更均匀，从而提高对应数组存储下标位置的随机性 & 均匀性，最终减少Hash冲突 ### 5.3 putVal(hash(key), key, value, false, true) ##### 5.3.1 计算完存储位置,如何存放到哈希表 ![示意图](https://gitee.com/zisuu/picture/raw/master/img/20201221222649.png) /** * 分析2：putVal(hash(key), key, value, false, true) */ final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) { Node[] tab; Node p; int n, i; // 1. 若哈希表的数组tab为空，则 通过resize() 创建 // 所以，初始化哈希表的时机 = 第1次调用put函数时，即调用resize() 初始化创建 // 关于resize（）的源码分析将在下面讲解扩容时详细分析，此处先跳过 if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; // 2. 计算插入存储的数组索引i：根据键值key计算的hash值 得到 // 此处的数组下标计算方式 = i = (n - 1) & hash，同JDK 1.7中的indexFor（），上面已详细描述 // 3. 插入时，需判断是否存在Hash冲突： // 若不存在（即当前table[i] == null），则直接在该数组位置新建节点，插入完毕 // 否则，代表存在Hash冲突，即当前存储位置已存在节点，则依次往下判断：a. 当前位置的key是否与需插入的key相同、b. 判断需插入的数据结构是否为红黑树 or 链表 if ((p = tab[i = (n - 1) & hash]) == null) tab[i] = newNode(hash, key, value, null); // newNode(hash, key, value, null)的源码 = new Node<>(hash, key, value, next) else { Node e; K k; // a. 判断 table[i]的元素的key是否与 需插入的key一样，若相同则 直接用新value 覆盖 旧value // 判断原则：equals（） if (p.hash == hash && ((k = p.key) == key || (key != null && key.equals(k)))) e = p; // b. 继续判断：需插入的数据结构是否为红黑树 or 链表 // 若是红黑树，则直接在树中插入 or 更新键值对 else if (p instanceof TreeNode) e = ((TreeNode)p).putTreeVal(this, tab, hash, key, value); ->>分析3 // 若是链表,则在链表中插入 or 更新键值对 // i. 遍历table[i]，判断Key是否已存在：采用equals（） 对比当前遍历节点的key 与 需插入数据的key：若已存在，则直接用新value 覆盖 旧value // ii. 遍历完毕后仍无发现上述情况，则直接在链表尾部插入数据 // 注：新增节点后，需判断链表长度是否>8（8 = 桶的树化阈值）：若是，则把链表转换为红黑树 else { for (int binCount = 0; ; ++binCount) { // 对于ii：若数组的下1个位置，表示已到表尾也没有找到key值相同节点，则新建节点 = 插入节点 // 注：此处是从链表尾插入，与JDK 1.7不同（从链表头插入，即永远都是添加到数组的位置，原来数组位置的数据则往后移） if ((e = p.next) == null) { p.next = newNode(hash, key, value, null); // 插入节点后，若链表节点>数阈值，则将链表转换为红黑树 if (binCount >= TREEIFY_THRESHOLD - 1) treeifyBin(tab, hash); // 树化操作 break; } // 对于i if (e.hash == hash && ((k = e.key) == key || (key != null && key.equals(k)))) break; // 更新p指向下一个节点，继续遍历 p = e; } } // 对i情况的后续操作：发现key已存在，直接用新value 覆盖 旧value & 返回旧value if (e != null) { V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); // 替换旧值时会调用的方法（默认实现为空） return oldValue; } } ++modCount; // 插入成功后，判断实际存在的键值对数量size > 最大容量threshold // 若 > ，则进行扩容 ->>分析4（但单独讲解，请直接跳出该代码块） if (++size > threshold) resize(); afterNodeInsertion(evict);// 插入成功时会调用的方法（默认实现为空） return null; } /** * 分析3：putTreeVal(this, tab, hash, key, value) * 作用：向红黑树插入 or 更新数据（键值对） * 过程：遍历红黑树判断该节点的key是否与需插入的key 相同： * a. 若相同，则新value覆盖旧value * b. 若不相同，则插入 */ final TreeNode putTreeVal(HashMap map, Node[] tab, int h, K k, V v) { Class kc = null; boolean searched = false; TreeNode root = (parent != null) ? root() : this; for (TreeNode p = root;;) { int dir, ph; K pk; if ((ph = p.hash) > h) dir = -1; else if (ph q, ch; searched = true; if (((ch = p.left) != null && (q = ch.find(h, k, kc)) != null) || ((ch = p.right) != null && (q = ch.find(h, k, kc)) != null)) return q; } dir = tieBreakOrder(k, pk); } TreeNode xp = p; if ((p = (dir xpn = xp.next; TreeNode x = map.newTreeNode(h, k, v, xpn); if (dir )xpn).prev = x; moveRootToFront(tab, balanceInsertion(root, x)); return null; } } } ![示意图](https://gitee.com/zisuu/picture/raw/master/img/20201221222655.png) ##### 5.3.2 扩容机制 ![示意图](https://gitee.com/zisuu/picture/raw/master/img/20201221222702.png) /** * 分析4：resize（） * 该函数有2种使用情况：1.初始化哈希表 2.当前数组容量过小，需扩容 */ final Node[] resize() { Node[] oldTab = table; // 扩容前的数组（当前数组） int oldCap = (oldTab == null) ? 0 : oldTab.length; // 扩容前的数组的容量 = 长度 int oldThr = threshold;// 扩容前的数组的阈值 int newCap, newThr = 0; // 针对情况2：若扩容前的数组容量超过最大值，则不再扩充 if (oldCap > 0) { if (oldCap >= MAXIMUM_CAPACITY) { threshold = Integer.MAX_VALUE; return oldTab; } // 针对情况2：若无超过最大值，就扩充为原来的2倍 else if ((newCap = oldCap = DEFAULT_INITIAL_CAPACITY) newThr = oldThr 0) // initial capacity was placed in threshold newCap = oldThr; else { // zero initial threshold signifies using defaults newCap = DEFAULT_INITIAL_CAPACITY; newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY); } // 计算新的resize上限 if (newThr == 0) { float ft = (float)newCap * loadFactor; newThr = (newCap [] newTab = (Node[])new Node[newCap]; table = newTab; if (oldTab != null) { // 把每个bucket都移动到新的buckets中 for (int j = 0; j e; if ((e = oldTab[j]) != null) { oldTab[j] = null; if (e.next == null) newTab[e.hash & (newCap - 1)] = e; else if (e instanceof TreeNode) ((TreeNode)e).split(this, newTab, j, oldCap); else { // 链表优化重hash的代码块 Node loHead = null, loTail = null; Node hiHead = null, hiTail = null; Node next; do { next = e.next; // 原索引 if ((e.hash & oldCap) == 0) { if (loTail == null) loHead = e; else loTail.next = e; loTail = e; } // 原索引 + oldCap else { if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; } } while ((e = next) != null); // 原索引放到bucket里 if (loTail != null) { loTail.next = null; newTab[j] = loHead; } // 原索引+oldCap放到bucket里 if (hiTail != null) { hiTail.next = null; newTab[j + oldCap] = hiHead; } } } } } return newTab; } ![示意图](https://gitee.com/zisuu/picture/raw/master/img/20201221222710.png) ##### 5.3.3 扩容位置重新计算方法 ![示意图](https://gitee.com/zisuu/picture/raw/master/img/20201221222719.png) ![示意图](https://gitee.com/zisuu/picture/raw/master/img/20201221222723.png) - `JDK 1.8`根据此结论作出的新元素存储位置计算规则 非常简单，提高了扩容效率，具体如下图 > 这与 `JDK 1.7`在计算新元素的存储位置有很大区别：`JDK 1.7`在扩容后，都需按照原来方法重新计算，即 > `hashCode（）`->> 扰动处理 ->>`（h & length-1）`） ### 5.4 get ![示意图](https://gitee.com/zisuu/picture/raw/master/img/20201221222730.png) ## 6.源码总结 下面，用3个图总结整个源码内容： > 总结内容 = 数据结构、主要参数、添加 & 查询数据流程、扩容机制 - 数据结构 & 主要参数 ![示意图](https://gitee.com/zisuu/picture/raw/master/img/20201221222733.png) - 添加 & 查询数据流程 ![示意图](https://gitee.com/zisuu/picture/raw/master/img/20201221222739.png) 扩容机制: ![示意图](https://gitee.com/zisuu/picture/raw/master/img/20201221222745.png) ## 七.与 Jdk1.7的区别 ### 7.1 数据结构 ![示意图](https://gitee.com/zisuu/picture/raw/master/img/20201221222751.png) ### 7.2 获取数据时（获取数据 类似） ![示意图](https://gitee.com/zisuu/picture/raw/master/img/20201221222754.png) ### 7.3 扩容机制 ![示意图](https://gitee.com/zisuu/picture/raw/master/img/20201221222758.png) ## 八 HashMap 1.7闭环问题 ### 为何出现死循环简要说明 　　HashMap是非线程安全的，在并发场景中如果不保持足够的同步，就有可能在执行HashMap.get时进入死循环，将CPU的消耗到100%。 　　HashMap采用链表解决Hash冲突。因为是链表结构，那么就很容易形成闭合的链路，这样在循环的时候只要有线程对这个HashMap进行get操作就会产生死循环， 　　单线程情况下，只有一个线程对HashMap的数据结构进行操作，是不可能产生闭合的回路的。 　　只有在多线程并发的情况下才会出现这种情况，那就是在put操作的时候，如果size>initialCapacity*loadFactor，hash表进行扩容，那么这时候HashMap就会进行rehash操作，随之HashMap的结构就会很大的变化。很有可能就是在两个线程在这个时候同时触发了rehash操作，产生了闭合的回路。 　　所以多线程并发的情况下推荐使用currentHashMap。 ### 多线程下[HashMap]的问题 　　1、多线程put操作后，get操作导致死循环。 　　2、多线程put非NULL元素后，get操作得到NULL值。 　　3、多线程put操作，导致元素丢失。 ### **HashMap闭环的详细原因** 　　Java的HashMap是非线程安全的，所以在并发下必然出现问题，以下做详细的解释： #### 正常的扩容的过程 　　画了个图做了个演示。 - 我假设了我们的hash算法就是简单的用key mod 一下表的大小（也就是数组的长度）。 - 最上面的是old hash 表，其中的Hash表的size=2, 所以key = 3, 7, 5，在mod 2以后都冲突在table[1]这里了。 - 接下来的三个步骤是Hash表 resize成4，然后所有的 重新rehash的过程 　　　　　　![img](http://s9.51cto.com/wyfs01/M00/0E/D4/wKioJlGwIqaBMCL9AACSa9G7jXM118.jpg) #### **并发下的扩容** 　　**1）假设我们有两个线程。**我用红色和浅蓝色标注了一下。 　　我们再回头看一下我们的 transfer代码中的这个细节： do { Entry next = e.next; // 　　而我们的线程二执行完成了。于是我们有下面的这个样子。 　　　　　　 　　注意，因为Thread1的 e 指向了key(3)，而next指向了key(7)，其在线程二rehash后，指向了线程二重组后的链表。我们可以看到链表的顺序被反转后。 　　2）线程一被调度回来执行。 先是执行 newTalbe[i] = e; 然后是e = next，导致了e指向了key(7)， 而下一次循环的next = e.next导致了next指向了key(3) 　　　　　　 　　3）一切安好。 　　线程一接着工作。把key(7)摘下来，放到newTable[i]的第一个，然后把e和next往下移。 　　　　　　 　　4）环形链接出现。 　　e.next = newTable[i] 导致 key(3).next 指向了 key(7) 　　注意：此时的key(7).next 已经指向了key(3)， 环形链表就这样出现了。 　　　　　　 　　于是，当我们的线程一调用到，HashTable.get(11)时，悲剧就出现了——Infinite Loop。 Stay hungry，stay foolish ！ HashMap1.8如何解决闭环问题 尾插法 九 HashMap常见面试题 1.谈一下HashMap的特性？ 1.HashMap存储键值对实现快速存取，允许为null。key值不可重复，若key值重复则覆盖。 2.非同步，线程不安全。 3.底层是hash表，不保证有序(比如插入的顺序) 2.谈一下HashMap的底层原理是什么？ 基于hashing的原理，jdk8后采用数组+链表+红黑树的数据结构。我们通过put和get存储和获取对象。当我们给put()方法传递键和值时，先对键做一个hashCode()的计算来得到它在bucket数组中的位置来存储Entry对象。当获取对象时，通过get获取到bucket的位置，再通过键对象的equals()方法找到正确的键值对，然后在返回值对象。 3.谈一下hashMap中put是如何实现的？ 1.计算关于key的hashcode值（与Key.hashCode的高16位做异或运算） 2.如果散列表为空时，调用resize()初始化散列表 3.如果没有发生碰撞，直接添加元素到散列表中去 4.如果发生了碰撞(hashCode值相同)，进行三种判断 4.1:若key地址相同或者equals后内容相同，则替换旧值 4.2:如果是红黑树结构，就调用树的插入方法 4.3：链表结构，循环遍历直到链表中某个节点为空，尾插法进行插入，插入之后判断链表个数是否到达变成红黑树的阙值8；也可以遍历到有节点与插入元素的哈希值和内容相同，进行覆盖。 5.如果桶满了大于阀值，则resize进行扩容 4.谈一下hashMap中什么时候需要进行扩容，扩容resize()又是如何实现的？ 调用场景： 1.初始化数组table 2.当数组table的size达到阙值时即++size > load factor * capacity 时，也是在putVal函数中 实现过程：(细讲) 1.通过判断旧数组的容量是否大于0来判断数组是否初始化过 否：进行初始化 判断是否调用无参构造器， 是:使用默认的大小和阙值 否:使用构造函数中初始化的容量，当然这个容量是经过tableSizefor计算后的2的次幂数 是，进行扩容，扩容成两倍(小于最大值的情况下)，之后在进行将元素重新进行与运算复制到新的散列表中 概括的讲：扩容需要重新分配一个新数组，新数组是老数组的2倍长，然后遍历整个老结构，把所有的元素挨个重新hash分配到新结构中去。 PS：可见底层数据结构用到了数组，到最后会因为容量问题都需要进行扩容操作 5.谈一下hashMap中get是如何实现的？ 对key的hashCode进行hashing，与运算计算下标获取bucket位置，如果在桶的首位上就可以找到就直接返回，否则在树中找或者链表中遍历找，如果有hash冲突，则利用equals方法去遍历链表查找节点。 6.谈一下HashMap中hash函数是怎么实现的？还有哪些hash函数的实现方式？ 对key的hashCode做hash操作，与高16位做异或运算 还有平方取中法，除留余数法，伪随机数法 7.为什么不直接将kedy作为哈希值而是与高16位做异或运算？ 因为数组位置的确定用的是与运算，仅仅最后四位有效，设计者将key的哈希值与高16为做异或运算使得在做&运算确定数组的插入位置时，此时的低位实际是高位与低位的结合，增加了随机性，减少了哈希碰撞的次数。 HashMap默认初始化长度为16，并且每次自动扩展或者是手动初始化容量时，必须是2的幂。 8.为什么是16？为什么必须是2的幂？如果输入值不是2的幂比如10会怎么样？ HashMap为了存取高效，要尽量较少碰撞，就是要尽量把数据分配均匀，每个链表长度大致相同，这个实现就在把数据存到哪个链表中的算法； 这个算法实际就是取模，hash%length，计算机中直接求余效率不如位移运算，源码中做了优化hash&(length-1)， hash%length==hash&(length-1)的前提是length是2的n次方； 为什么这样能均匀分布减少碰撞呢？2的n次方实际就是1后面n个0，2的n次方-1 实际就是n个1； 例如长度为9时候，3&(9-1)=0 2&(9-1)=0 ，都在0上，碰撞了； 例如长度为8时候，3&(8-1)=3 2&(8-1)=2 ，不同位置上，不碰撞； 其实就是按位“与”的时候，每一位都能 &1 ，也就是和1111……1111111进行与运算 0000 0011 3 & 0000 1000 8 = 0000 0000 0 0000 0010 2 & 0000 1000 8 = 0000 0000 0 0000 0011 3 & 0000 0111 7 = 0000 0011 3 0000 0010 2 & 0000 0111 7 = 0000 0010 2 1.为了数据的均匀分布，减少哈希碰撞。因为确定数组位置是用的位运算，若数据不是2的次幂则会增加哈希碰撞的次数和浪费数组空间。(PS:其实若不考虑效率，求余也可以就不用位运算了也不用长度必需为2的幂次) 2.输入数据若不是2的幂，HashMap通过一通位移运算和或运算得到的肯定是2的幂次数，并且是离那个数最近的数字 9.谈一下当两个对象的hashCode相等时会怎么样？ 会产生哈希碰撞，若key值相同则替换旧值，不然链接到链表后面，链表长度超过阙值8就转为红黑树存储 10.如果两个键的hashcode相同，你如何获取值对象？ HashCode相同，通过equals比较内容获取值对象 11.\"如果HashMap的大小超过了负载因子(load factor)定义的容量，怎么办？ 超过阙值会进行扩容操作，概括的讲就是扩容后的数组大小是原数组的2倍，将原来的元素重新hashing放入到新的散列表中去。 12.HashMap和HashTable的区别 相同点：都是存储key-value键值对的 不同点： HashMap允许Key-value为null，hashTable不允许； hashMap没有考虑同步，是线程不安全的。hashTable是线程安全的，给api套上了一层synchronized修饰; HashMap继承于AbstractMap类，hashTable继承与Dictionary类。 迭代器(Iterator)。HashMap的迭代器(Iterator)是fail-fast迭代器，而Hashtable的enumerator迭代器不是fail-fast的。所以当有其它线程改变了HashMap的结构（增加或者移除元素），将会抛出ConcurrentModificationException。 容量的初始值和增加方式都不一样：HashMap默认的容量大小是16；增加容量时，每次将容量变为\"原始容量x2\"。Hashtable默认的容量大小是11；增加容量时，每次将容量变为\"原始容量x2 + 1\"； 添加key-value时的hash值算法不同：HashMap添加元素时，是使用自定义的哈希算法。Hashtable没有自定义哈希算法，而直接采用的key的hashCode()。 13.请解释一下HashMap的参数loadFactor，它的作用是什么？ loadFactor表示HashMap的拥挤程度，影响hash操作到同一个数组位置的概率。默认loadFactor等于0.75，当HashMap里面容纳的元素已经达到HashMap数组长度的75%时，表示HashMap太挤了，需要扩容，在HashMap的构造器中可以定制loadFactor。 14.传统hashMap的缺点(为什么引入红黑树？)： JDK 1.8 以前 HashMap 的实现是 数组+链表，即使哈希函数取得再好，也很难达到元素百分百均匀分布。当 HashMap 中有大量的元素都存放到同一个桶中时，这个桶下有一条长长的链表，这个时候 HashMap 就相当于一个单链表，假如单链表有 n 个元素，遍历的时间复杂度就是 O(n)，完全失去了它的优势。针对这种情况，JDK 1.8 中引入了 红黑树（查找时间复杂度为 O(logn)）来优化这个问题。 15. 平时在使用HashMap时一般使用什么类型的元素作为Key？ 选择Integer，String这种不可变的类型，像对String的一切操作都是新建一个String对象，对新的对象进行拼接分割等，这些类已经很规范的覆写了hashCode()以及equals()方法。作为不可变类天生是线程安全的， "},"basic/collection/3.ConcurrentHashMap分析.html":{"url":"basic/collection/3.ConcurrentHashMap分析.html","title":"3.ConcurrentHashMap分析","keywords":"","body":"前言ConcurrentHashMapJDK1.7的实现JDK1.8的实现总结与思考前言 我们都知道HashMap在多线程情况下，在put的时候，插入的元素超过了容量（由负载因子决定）的范围就会触发扩容操作，就是rehash，这个会重新将原数组的内容重新hash到新的扩容数组中，在多线程的环境下，存在同时其他的元素也在进行put操作，如果hash值相同，可能出现同时在同一数组下用链表表示，造成闭环，导致在get时会出现死循环，所以HashMap是线程不安全的。 我们来了解另一个键值存储集合HashTable，它是线程安全的，它在所有涉及到多线程操作的都加上了synchronized关键字来锁住整个table，这就意味着所有的线程都在竞争一把锁，在多线程的环境下，它是安全的，但是无疑是效率低下的。 其实HashTable有很多的优化空间，锁住整个table这么粗暴的方法可以变相的柔和点，比如在多线程的环境下，对不同的数据集进行操作时其实根本就不需要去竞争一个锁，因为他们不同hash值，不会因为rehash造成线程不安全，所以互不影响，这就是锁分离技术，将锁的粒度降低，利用多个锁来控制多个小的table ConcurrentHashMap JDK1.7的实现 在JDK1.7版本中，ConcurrentHashMap的数据结构是由一个Segment数组和多个HashEntry组成，如下图所示： Segment数组的意义就是将一个大的table分割成多个小的table来进行加锁，也就是上面的提到的锁分离技术,也即所谓的分段锁，而每一个Segment元素存储的是HashEntry数组+链表，这个和HashMap的数据存储结构一样 初始化 ConcurrentHashMap的初始化是会通过位与运算来初始化Segment的大小，用size来表示，如下所示 int size =1; while(size 如上所示，因为size用位于运算来计算（ size 每一个Segment元素下的HashEntry的初始化也是按照位于运算来计算，用cap来表示，如下所示 int cap =1; while(cap 如上所示，HashEntry大小的计算也是2的N次方（cap put操作 对于ConcurrentHashMap的数据插入，这里要进行两次Hash去定位数据的存储位置 static class Segment extends ReentrantLock implements Serializable { } 从上Segment的继承体系可以看出，Segment实现了ReentrantLock,也就带有锁的功能，当执行put操作时，会进行第一次key的hash来定位Segment的位置，如果该Segment还没有初始化，即通过CAS操作进行赋值，然后进行第二次hash操作，找到相应的HashEntry的位置，这里会利用继承过来的锁的特性，在将数据插入指定的HashEntry位置时（链表的尾端），会通过继承ReentrantLock的tryLock（）方法尝试去获取锁，如果获取成功就直接插入相应的位置，如果已经有线程获取该Segment的锁，那当前线程会以自旋的方式去继续的调用tryLock（）方法去获取锁，超过指定次数就挂起，等待唤醒 get操作 ConcurrentHashMap的get操作跟HashMap类似，只是ConcurrentHashMap第一次需要经过一次hash定位到Segment的位置，然后再hash定位到指定的HashEntry，遍历该HashEntry下的链表进行对比，成功就返回，不成功就返回null size操作 计算ConcurrentHashMap的元素大小是一个有趣的问题，因为他是并发操作的，就是在你计算size的时候，他还在并发的插入数据，可能会导致你计算出来的size和你实际的size有相差（在你return size的时候，插入了多个数据），要解决这个问题，JDK1.7版本用两种方案 第一种方案他会使用不加锁的模式去尝试多次计算ConcurrentHashMap的size，最多三次，比较前后两次计算的结果，结果一致就认为当前没有元素加入，计算的结果是准确的 第二种方案是如果第一种方案不符合，他就会给每个Segment加上锁，然后计算ConcurrentHashMap的size返回 JDK1.8的实现 JDK1.8的实现已经摒弃了Segment的概念，而是直接用Node数组+链表+红黑树的数据结构来实现，并发控制使用Synchronized和CAS来操作，整个看起来就像是优化过且线程安全的HashMap，虽然在JDK1.8中还能看到Segment的数据结构，但是已经简化了属性，只是为了兼容旧版本 说明：ConcurrentHashMap的数据结构（数组+链表+红黑树），桶中的结构可能是链表，也可能是红黑树，红黑树是为了提高查找效率。 在深入JDK1.8的put和get实现之前要知道一些常量设计和数据结构，这些是构成ConcurrentHashMap实现结构的基础，下面看一下基本属性： // node数组最大容量：2^30=1073741824 private static final int MAXIMUM_CAPACITY = 1 8 链表转换为红黑树 static final int TREEIFY_THRESHOLD = 8 ; //树转链表阀值，小于等于6（tranfer时，lc、hc=0两个计数器分别++记录原bin、新binTreeNode数量，[] table; /*控制标识符，用来控制table的初始化和扩容的操作，不同的值有不同的含义 *当为负数时：- 1 代表正在初始化，-N代表有N- 1 个线程正在 进行扩容 *当为 0 时：代表当时的table还没有被初始化 *当为正数时：表示初始化或者下一次进行扩容的大小 */ private transient volatile int sizeCtl; 基本属性定义了ConcurrentHashMap的一些边界以及操作时的一些控制，下面看一些内部的一些结构组成，这些是整个ConcurrentHashMap整个数据结构的核心 Node Node是ConcurrentHashMap存储结构的基本单元，继承于HashMap中的Entry，用于存储数据，源代码如下 static class Node implements Map.Entry { //链表的数据结构 final int hash; final K key; //val和next都会在扩容时发生变化，所以加上volatile来保持可见性和禁止重排序 volatile V val; volatile Node next; Node( int hash, K key, V val, Node next) { this .hash = hash; this .key = key; this .val = val; this .next = next; } public final K getKey() { return key; } public final V getValue() { return val; } public final int hashCode() { return key.hashCode() ^ val.hashCode(); } public final String toString(){ return key + \"=\" + val; } //不允许更新value public final V setValue(V value) { throw new UnsupportedOperationException(); } public final boolean equals(Object o) { Object k, v, u; Map.Entry e; return ((o instanceof Map.Entry) && (k = (e = (Map.Entry)o).getKey()) != null && (v = e.getValue()) != null && (k == key || k.equals(key)) && (v == (u = val) || v.equals(u))); } //用于map中的get（）方法，子类重写 Node find( int h, Object k) { Node e = this ; if (k != null ) { do { K ek; if (e.hash == h && ((ek = e.key) == k || (ek != null && k.equals(ek)))) return e; } while ((e = e.next) != null ); } return null ; } } Node数据结构很简单，从上可知，就是一个链表，但是只允许对数据进行查找，不允许进行修改 TreeNode TreeNode继承与Node，但是数据结构换成了二叉树结构，它是红黑树的数据的存储结构，用于红黑树中存储数据，当链表的节点数大于8时会转换成红黑树的结构，他就是通过TreeNode作为存储结构代替Node来转换成黑红树源代码如下 static final class TreeNode extends Node { //树形结构的属性定义 TreeNode parent; // red-black tree links TreeNode left; TreeNode right; TreeNode prev; // needed to unlink next upon deletion boolean red; //标志红黑树的红节点 TreeNode( int hash, K key, V val, Node next, TreeNode parent) { super (hash, key, val, next); this .parent = parent; } Node find( int h, Object k) { return findTreeNode(h, k, null ); } //根据key查找 从根节点开始找出相应的TreeNode， final TreeNode findTreeNode( int h, Object k, Class kc) { if (k != null ) { TreeNode p = this ; do { int ph, dir; K pk; TreeNode q; TreeNode pl = p.left, pr = p.right; if ((ph = p.hash) > h) p = pl; else if (ph TreeBin TreeBin从字面含义中可以理解为存储树形结构的容器，而树形结构就是指TreeNode，所以TreeBin就是封装TreeNode的容器，它提供转换黑红树的一些条件和锁的控制，部分源码结构如下 static final class TreeBin extends Node { //指向TreeNode列表和根节点 TreeNode root; volatile TreeNode first; volatile Thread waiter; volatile int lockState; // 读写锁状态 static final int WRITER = 1 ; // 获取写锁的状态 static final int WAITER = 2 ; // 等待写锁的状态 static final int READER = 4 ; // 增加数据时读锁的状态 /** * 初始化红黑树 */ TreeBin(TreeNode b) { super (TREEBIN, null , null , null ); this .first = b; TreeNode r = null ; for (TreeNode x = b, next; x != null ; x = next) { next = (TreeNode)x.next; x.left = x.right = null ; if (r == null ) { x.parent = null ; x.red = false ; r = x; } else { K k = x.key; int h = x.hash; Class kc = null ; for (TreeNode p = r;;) { int dir, ph; K pk = p.key; if ((ph = p.hash) > h) dir = - 1 ; else if (ph xp = p; if ((p = (dir 介绍了ConcurrentHashMap主要的属性与内部的数据结构，现在通过一个简单的例子以debug的视角看看ConcurrentHashMap的具体操作细节 public class TestConcurrentHashMap{ public static void main(String[] args){ ConcurrentHashMap map = new ConcurrentHashMap(); //初始化ConcurrentHashMap //新增个人信息 map.put( \"id\" , \"1\" ); map.put( \"name\" , \"andy\" ); map.put( \"sex\" , \"男\" ); //获取姓名 String name = map.get( \"name\" ); Assert.assertEquals(name, \"andy\" ); //计算大小 int size = map.size(); Assert.assertEquals(size, 3 ); } } 我们先通过 new ConcurrentHashMap() 来进行初始化 public ConcurrentHashMap() { } 由上你会发现ConcurrentHashMap的初始化其实是一个空实现，并没有做任何事，这里后面会讲到，这也是和其他的集合类有区别的地方，初始化操作并不是在构造函数实现的，而是在put操作中实现，当然ConcurrentHashMap还提供了其他的构造函数，有指定容量大小或者指定负载因子，跟HashMap一样，这里就不做介绍了 put操作 在上面的例子中我们新增个人信息会调用put方法，我们来看下 public V put(K key, V value) { return putVal(key, value, false ); } /** Implementation for put and putIfAbsent */ final V putVal(K key, V value, boolean onlyIfAbsent) { if (key == null || value == null ) throw new NullPointerException(); int hash = spread(key.hashCode()); //两次hash，减少hash冲突，可以均匀分布 int binCount = 0 ; for (Node[] tab = table;;) { //对这个table进行迭代 Node f; int n, i, fh; //这里就是上面构造方法没有进行初始化，在这里进行判断，为null就调用initTable进行初始化，属于懒汉模式初始化 if (tab == null || (n = tab.length) == 0 ) tab = initTable(); else if ((f = tabAt(tab, i = (n - 1 ) & hash)) == null ) { //如果i位置没有数据，就直接无锁插入 if (casTabAt(tab, i, null , new Node(hash, key, value, null ))) break ; // no lock when adding to empty bin } else if ((fh = f.hash) == MOVED) //如果在进行扩容，则先进行扩容操作 tab = helpTransfer(tab, f); else { V oldVal = null ; //如果以上条件都不满足，那就要进行加锁操作，也就是存在hash冲突，锁住链表或者红黑树的头结点 synchronized (f) { if (tabAt(tab, i) == f) { if (fh >= 0 ) { //表示该节点是链表结构 binCount = 1 ; for (Node e = f;; ++binCount) { K ek; //这里涉及到相同的key进行put就会覆盖原先的value if (e.hash == hash && ((ek = e.key) == key || (ek != null && key.equals(ek)))) { oldVal = e.val; if (!onlyIfAbsent) e.val = value; break ; } Node pred = e; if ((e = e.next) == null ) { //插入链表尾部 pred.next = new Node(hash, key, value, null ); break ; } } } else if (f instanceof TreeBin) { //红黑树结构 Node p; binCount = 2 ; //红黑树结构旋转插入 if ((p = ((TreeBin)f).putTreeVal(hash, key, value)) != null ) { oldVal = p.val; if (!onlyIfAbsent) p.val = value; } } } } if (binCount != 0 ) { //如果链表的长度大于8时就会进行红黑树的转换 if (binCount >= TREEIFY_THRESHOLD) treeifyBin(tab, i); if (oldVal != null ) return oldVal; break ; } } } addCount(1L, binCount); //统计size，并且检查是否需要扩容 return null ; } 这个put的过程很清晰，对当前的table进行无条件自循环直到put成功，可以分成以下六步流程来概述 如果没有初始化就先调用initTable（）方法来进行初始化过程 如果没有hash冲突就直接CAS插入 如果还在进行扩容操作就先进行扩容 如果存在hash冲突，就加锁来保证线程安全，这里有两种情况，一种是链表形式就直接遍历到尾端插入，一种是红黑树就按照红黑树结构插入， 最后一个如果Hash冲突时会形成Node链表，在链表长度超过8，Node数组超过64时会将链表结构转换为红黑树的结构，break再一次进入循环 如果添加成功就调用addCount（）方法统计size，并且检查是否需要扩容 现在我们来对每一步的细节进行源码分析，在第一步中，符合条件会进行初始化操作，我们来看看initTable（）方法 /** * Initializes table, using the size recorded in sizeCtl. */ private final Node[] initTable() { Node[] tab; int sc; while ((tab = table) == null || tab.length == 0 ) { //空的table才能进入初始化操作 if ((sc = sizeCtl) 0 ) ? sc : DEFAULT_CAPACITY; @SuppressWarnings ( \"unchecked\" ) Node[] nt = (Node[]) new Node[n]; //初始化 table = tab = nt; sc = n - (n >>> 2 ); //记录下次扩容的大小 } } finally { sizeCtl = sc; } break ; } } return tab; } 在第二步中没有hash冲突就直接调用Unsafe的方法CAS插入该元素，进入第三步如果容器正在扩容，则会调用helpTransfer（）方法帮助扩容，现在我们跟进helpTransfer（）方法看看 /** *帮助从旧的table的元素复制到新的table中 */ final Node[] helpTransfer(Node[] tab, Node f) { Node[] nextTab; int sc; if (tab != null && (f instanceof ForwardingNode) && (nextTab = ((ForwardingNode)f).nextTable) != null ) { //新的table nextTba已经存在前提下才能帮助扩容 int rs = resizeStamp(tab.length); while (nextTab == nextTable && table == tab && (sc = sizeCtl) >> RESIZE_STAMP_SHIFT) != rs || sc == rs + 1 || sc == rs + MAX_RESIZERS || transferIndex 其实helpTransfer（）方法的目的就是调用多个工作线程一起帮助进行扩容，这样的效率就会更高，而不是只有检查到要扩容的那个线程进行扩容操作，其他线程就要等待扩容操作完成才能工作 既然这里涉及到扩容的操作，我们也一起来看看扩容方法transfer（） private final void transfer(Node[] tab, Node[] nextTab) { int n = tab.length, stride; // 每核处理的量小于16，则强制赋值16 if ((stride = (NCPU > 1 ) ? (n >>> 3 ) / NCPU : n) [] nt = (Node[]) new Node[n fwd = new ForwardingNode(nextTab); // 当advance == true时，表明该节点已经处理过了 boolean advance = true ; boolean finishing = false ; // to ensure sweep before committing nextTab for ( int i = 0 , bound = 0 ;;) { Node f; int fh; // 控制 --i ,遍历原hash表中的节点 while (advance) { int nextIndex, nextBound; if (--i >= bound || finishing) advance = false ; else if ((nextIndex = transferIndex) stride ? nextIndex - stride : 0 ))) { bound = nextBound; i = nextIndex - 1 ; advance = false ; } } if (i = n || i + n >= nextn) { int sc; // 已经完成所有节点复制了 if (finishing) { nextTable = null ; table = nextTab; // table 指向nextTable sizeCtl = (n >> 1 ); // sizeCtl阈值为原来的1.5倍 return ; // 跳出死循环， } // CAS 更扩容阈值，在这里面sizectl值减一，说明新加入一个线程参与到扩容操作 if (U.compareAndSwapInt( this , SIZECTL, sc = sizeCtl, sc - 1 )) { if ((sc - 2 ) != resizeStamp(n) ln, hn; // fh >= 0 ,表示为链表节点 if (fh >= 0 ) { // 构造两个链表 一个是原链表 另一个是原链表的反序排列 int runBit = fh & n; Node lastRun = f; for (Node p = f.next; p != null ; p = p.next) { int b = p.hash & n; if (b != runBit) { runBit = b; lastRun = p; } } if (runBit == 0 ) { ln = lastRun; hn = null ; } else { hn = lastRun; ln = null ; } for (Node p = f; p != lastRun; p = p.next) { int ph = p.hash; K pk = p.key; V pv = p.val; if ((ph & n) == 0 ) ln = new Node(ph, pk, pv, ln); else hn = new Node(ph, pk, pv, hn); } // 在nextTable i 位置处插上链表 setTabAt(nextTab, i, ln); // 在nextTable i + n 位置处插上链表 setTabAt(nextTab, i + n, hn); // 在table i 位置处插上ForwardingNode 表示该节点已经处理过了 setTabAt(tab, i, fwd); // advance = true 可以执行--i动作，遍历节点 advance = true ; } // 如果是TreeBin，则按照红黑树进行处理，处理逻辑与上面一致 else if (f instanceof TreeBin) { TreeBin t = (TreeBin)f; TreeNode lo = null , loTail = null ; TreeNode hi = null , hiTail = null ; int lc = 0 , hc = 0 ; for (Node e = t.first; e != null ; e = e.next) { int h = e.hash; TreeNode p = new TreeNode (h, e.key, e.val, null , null ); if ((h & n) == 0 ) { if ((p.prev = loTail) == null ) lo = p; else loTail.next = p; loTail = p; ++lc; } else { if ((p.prev = hiTail) == null ) hi = p; else hiTail.next = p; hiTail = p; ++hc; } } // 扩容后树节点个数若(lo) : t; hn = (hc (hi) : t; setTabAt(nextTab, i, ln); setTabAt(nextTab, i + n, hn); setTabAt(tab, i, fwd); advance = true ; } } } } } } 其实helpTransfer（）方法的目的就是调用多个工作线程一起帮助进行扩容，这样的效率就会更高，而不是只有检查到要扩容的那个线程进行扩容操作，其他线程就要等待扩容操作完成才能工作 既然这里涉及到扩容的操作，我们也一起来看看扩容方法transfer（） private final void transfer(Node[] tab, Node[] nextTab) { int n = tab.length, stride; // 每核处理的量小于16，则强制赋值16 if ((stride = (NCPU > 1 ) ? (n >>> 3 ) / NCPU : n) [] nt = (Node[]) new Node[n fwd = new ForwardingNode(nextTab); // 当advance == true时，表明该节点已经处理过了 boolean advance = true ; boolean finishing = false ; // to ensure sweep before committing nextTab for ( int i = 0 , bound = 0 ;;) { Node f; int fh; // 控制 --i ,遍历原hash表中的节点 while (advance) { int nextIndex, nextBound; if (--i >= bound || finishing) advance = false ; else if ((nextIndex = transferIndex) stride ? nextIndex - stride : 0 ))) { bound = nextBound; i = nextIndex - 1 ; advance = false ; } } if (i = n || i + n >= nextn) { int sc; // 已经完成所有节点复制了 if (finishing) { nextTable = null ; table = nextTab; // table 指向nextTable sizeCtl = (n >> 1 ); // sizeCtl阈值为原来的1.5倍 return ; // 跳出死循环， } // CAS 更扩容阈值，在这里面sizectl值减一，说明新加入一个线程参与到扩容操作 if (U.compareAndSwapInt( this , SIZECTL, sc = sizeCtl, sc - 1 )) { if ((sc - 2 ) != resizeStamp(n) ln, hn; // fh >= 0 ,表示为链表节点 if (fh >= 0 ) { // 构造两个链表 一个是原链表 另一个是原链表的反序排列 int runBit = fh & n; Node lastRun = f; for (Node p = f.next; p != null ; p = p.next) { int b = p.hash & n; if (b != runBit) { runBit = b; lastRun = p; } } if (runBit == 0 ) { ln = lastRun; hn = null ; } else { hn = lastRun; ln = null ; } for (Node p = f; p != lastRun; p = p.next) { int ph = p.hash; K pk = p.key; V pv = p.val; if ((ph & n) == 0 ) ln = new Node(ph, pk, pv, ln); else hn = new Node(ph, pk, pv, hn); } // 在nextTable i 位置处插上链表 setTabAt(nextTab, i, ln); // 在nextTable i + n 位置处插上链表 setTabAt(nextTab, i + n, hn); // 在table i 位置处插上ForwardingNode 表示该节点已经处理过了 setTabAt(tab, i, fwd); // advance = true 可以执行--i动作，遍历节点 advance = true ; } // 如果是TreeBin，则按照红黑树进行处理，处理逻辑与上面一致 else if (f instanceof TreeBin) { TreeBin t = (TreeBin)f; TreeNode lo = null , loTail = null ; TreeNode hi = null , hiTail = null ; int lc = 0 , hc = 0 ; for (Node e = t.first; e != null ; e = e.next) { int h = e.hash; TreeNode p = new TreeNode (h, e.key, e.val, null , null ); if ((h & n) == 0 ) { if ((p.prev = loTail) == null ) lo = p; else loTail.next = p; loTail = p; ++lc; } else { if ((p.prev = hiTail) == null ) hi = p; else hiTail.next = p; hiTail = p; ++hc; } } // 扩容后树节点个数若(lo) : t; hn = (hc (hi) : t; setTabAt(nextTab, i, ln); setTabAt(nextTab, i + n, hn); setTabAt(tab, i, fwd); advance = true ; } } } } } } 扩容过程有点复杂，这里主要涉及到多线程并发扩容,ForwardingNode的作用就是支持扩容操作，将已处理的节点和空节点置为ForwardingNode，并发处理时多个线程经过ForwardingNode就表示已经遍历了，就往后遍历，下图是多线程合作扩容的过程： image 介绍完扩容过程，我们再次回到put流程，在第四步中是向链表或者红黑树里加节点，到第五步，会调用treeifyBin（）方法进行链表转红黑树的过程 private final void treeifyBin(Node[] tab, int index) { Node b; int n, sc; if (tab != null ) { //如果整个table的数量小于64，就扩容至原来的一倍，不转红黑树了 //因为这个阈值扩容可以减少hash冲突，不必要去转红黑树 if ((n = tab.length) = 0 ) { synchronized (b) { if (tabAt(tab, index) == b) { TreeNode hd = null , tl = null ; for (Node e = b; e != null ; e = e.next) { //封装成TreeNode TreeNode p = new TreeNode(e.hash, e.key, e.val, null , null ); if ((p.prev = tl) == null ) hd = p; else tl.next = p; tl = p; } //通过TreeBin对象对TreeNode转换成红黑树 setTabAt(tab, index, new TreeBin(hd)); } } } } } 到第六步表示已经数据加入成功了，现在调用addCount()方法计算ConcurrentHashMap的size，在原来的基础上加一，现在来看看addCount()方法 private final void addCount( long x, int check) { CounterCell[] as; long b, s; //更新baseCount，table的数量，counterCells表示元素个数的变化 if ((as = counterCells) != null || !U.compareAndSwapLong( this , BASECOUNT, b = baseCount, s = b + x)) { CounterCell a; long v; int m; boolean uncontended = true ; //如果多个线程都在执行，则CAS失败，执行fullAddCount，全部加入count if (as == null || (m = as.length - 1 ) =0表示需要进行扩容操作 if (check >= 0 ) { Node[] tab, nt; int n, sc; while (s >= ( long )(sc = sizeCtl) && (tab = table) != null && (n = tab.length) >> RESIZE_STAMP_SHIFT) != rs || sc == rs + 1 || sc == rs + MAX_RESIZERS || (nt = nextTable) == null || transferIndex put的流程现在已经分析完了，你可以从中发现，他在并发处理中使用的是乐观锁，当有冲突的时候才进行并发处理，而且流程步骤很清晰，但是细节设计的很复杂，毕竟多线程的场景也复杂 get操作 我们现在要回到开始的例子中，我们对个人信息进行了新增之后，我们要获取所新增的信息，使用String name = map.get(“name”)获取新增的name信息，现在我们依旧用debug的方式来分析下ConcurrentHashMap的获取方法get() public V get(Object key) { Node[] tab; Node e, p; int n, eh; K ek; int h = spread(key.hashCode()); //计算两次hash if ((tab = table) != null && (n = tab.length) > 0 && (e = tabAt(tab, (n - 1 ) & h)) != null ) { //读取首节点的Node元素 if ((eh = e.hash) == h) { //如果该节点就是首节点就返回 if ((ek = e.key) == key || (ek != null && key.equals(ek))) return e.val; } //hash值为负值表示正在扩容，这个时候查的是ForwardingNode的find方法来定位到nextTable来 //查找，查找到就返回 else if (eh ConcurrentHashMap的get操作的流程很简单，也很清晰，可以分为三个步骤来描述 计算hash值，定位到该table索引位置，如果是首节点符合就返回 如果遇到扩容的时候，会调用标志正在扩容节点ForwardingNode的find方法，查找该节点，匹配就返回 以上都不符合的话，就往下遍历节点，匹配就返回，否则最后就返回null size操作 最后我们来看下例子中最后获取size的方式int size = map.size();，现在让我们看下size()方法 public int size() { long n = sumCount(); return ((n ( long )Integer.MAX_VALUE) ? Integer.MAX_VALUE : ( int )n); } final long sumCount() { CounterCell[] as = counterCells; CounterCell a; //变化的数量 long sum = baseCount; if (as != null ) { for ( int i = 0 ; i 在JDK1.8版本中，对于size的计算，在扩容和addCount()方法就已经有处理了，JDK1.7是在调用size()方法才去计算，其实在并发集合中去计算size是没有多大的意义的，因为size是实时在变的，只能计算某一刻的大小，但是某一刻太快了，人的感知是一个时间段，所以并不是很精确 总结与思考 其实可以看出JDK1.8版本的ConcurrentHashMap的数据结构已经接近HashMap，相对而言，ConcurrentHashMap只是增加了同步的操作来控制并发，从JDK1.7版本的ReentrantLock+Segment+HashEntry，到JDK1.8版本中synchronized+CAS+HashEntry+红黑树,相对而言，总结如下思考 JDK1.8的实现降低锁的粒度，JDK1.7版本锁的粒度是基于Segment的，包含多个HashEntry，而JDK1.8锁的粒度就是HashEntry（首节点） JDK1.8版本的数据结构变得更加简单，使得操作也更加清晰流畅，因为已经使用synchronized来进行同步，所以不需要分段锁的概念，也就不需要Segment这种数据结构了，由于粒度的降低，实现的复杂度也增加了 JDK1.8使用红黑树来优化链表，基于长度很长的链表的遍历是一个很漫长的过程，而红黑树的遍历效率是很快的，代替一定阈值的链表，这样形成一个最佳拍档 JDK1.8为什么使用内置锁synchronized来代替重入锁ReentrantLock，我觉得有以下几点 因为粒度降低了，在相对而言的低粒度加锁方式，synchronized并不比ReentrantLock差，在粗粒度加锁中ReentrantLock可能通过Condition来控制各个低粒度的边界，更加的灵活，而在低粒度中，Condition的优势就没有了 JVM的开发团队从来都没有放弃synchronized，而且基于JVM的synchronized优化空间更大，使用内嵌的关键字比使用API更加自然 在大量的数据操作下，对于JVM的内存压力，基于API的ReentrantLock会开销更多的内存，虽然不是瓶颈，但是也是一个选择依据 "},"数据库/mysql/1.b树b+树的底层原理.html":{"url":"数据库/mysql/1.b树b+树的底层原理.html","title":"1.mysql索引基础之b树与b+树⭐⭐","keywords":"","body":"写在前面B-树B+ 树B-树和B+树的区别拓展：MySQL为什么使用B-Tree（B+Tree）&& 存储知识写在前面 B树和B+树是MySQL索引使用的数据结构，对于索引优化和原理理解都非常重要，下面我的写文章就是要把B树，B+树的神秘面纱揭开，让大家在面试的时候碰到这个知识点一往无前，不再成为你的知识盲点！ B-树 B-树概述 B-树,这里的 B 表示 balance( 平衡的意思),B-树是一种多路自平衡的搜索树（B树是一颗多路平衡查找树） 它类似普通的平衡二叉树，不同的一点是B-树允许每个节点有更多的子节点。下图是 B-树的简化图. B-树有如下特点: 所有键值分布在整颗树中（索引值和具体data都在每个节点里）； 任何一个关键字出现且只出现在一个结点中； 搜索有可能在非叶子结点结束（最好情况O(1)就能找到数据）； 在关键字全集内做一次查找,性能逼近二分查找； B树深入 B树由来 定义：B-树是一类树，包括B-树、B+树、B树等，是一棵自平衡的搜索树，它类似普通的平衡二叉树，不同的一点是B-树允许每个节点有更多的子节点。 *B-树是专门为外部存储器设计的，如磁盘，它对于读取和写入大块数据有良好的性能，所以一般被用在文件系统及数据库中。 定义只需要知道B-树允许每个节点有更多的子节点即可（多叉树）。子节点数量一般在上千，具体数量依赖外部存储器的特性。 先来看看为什么会出现B-树这类数据结构。 传统用来搜索的平衡二叉树有很多，如 AVL 树，红黑树等。这些树在一般情况下查询性能非常好，但当数据非常大的时候它们就无能为力了。原因当数据量非常大时，内存不够用，大部分数据只能存放在磁盘上，只有需要的数据才加载到内存中。一般而言内存访问的时间约为 50 ns，而磁盘在 10 ms 左右。速度相差了近 5 个数量级，磁盘读取时间远远超过了数据在内存中比较的时间。这说明程序大部分时间会阻塞在磁盘 IO 上。那么我们如何提高程序性能？减少磁盘 IO 次数，像 AVL 树，红黑树这类平衡二叉树从设计上无法“迎合”磁盘。 上图是一颗简单的平衡二叉树，平衡二叉树是通过旋转来保持平衡的，而旋转是对整棵树的操作，若部分加载到内存中则无法完成旋转操作。其次平衡二叉树的高度相对较大为 log n（底数为2），这样逻辑上很近的节点实际可能非常远，无法很好的利用磁盘预读（局部性原理），所以这类平衡二叉树在数据库和文件系统上的选择就被 pass 了。 空间局部性原理：如果一个存储器的某个位置被访问，那么将它附近的位置也会被访问。 我们从“迎合”磁盘的角度来看看B-树的设计。 索引的效率依赖与磁盘 IO 的次数，快速索引需要有效的减少磁盘 IO 次数，如何快速索引呢？索引的原理其实是不断的缩小查找范围，就如我们平时用字典查单词一样，先找首字母缩小范围，再第二个字母等等。平衡二叉树是每次将范围分割为两个区间。为了更快，B-树每次将范围分割为多个区间，区间越多，定位数据越快越精确。那么如果节点为区间范围，每个节点就较大了。所以新建节点时，直接申请页大小的空间（磁盘存储单位是按 block 分的，一般为 512 Byte。磁盘 IO 一次读取若干个 block，我们称为一页，具体大小和操作系统有关，一般为 4 k，8 k或 16 k），计算机内存分配是按页对齐的，这样就实现了一个节点只需要一次 IO。 上图是一棵简化的B-树，多叉的好处非常明显，有效的降低了B-树的高度，为底数很大的 log n，底数大小与节点的子节点数目有关，一般一棵B-树的高度在 3 层左右。层数低，每个节点区确定的范围更精确，范围缩小的速度越快（比二叉树深层次的搜索肯定快很多）。上面说了一个节点需要进行一次 IO，那么总 IO 的次数就缩减为了 log n 次。B-树的每个节点是 n 个有序的序列(a1,a2,a3…an)，并将该节点的子节点分割成 n+1 个区间来进行索引(X1 an)。 点评：B树的每个节点，都是存多个值的，不像二叉树那样，一个节点就一个值，B树把每个节点都给了一点的范围区间，区间更多的情况下，搜索也就更快了，比如：有1-100个数，二叉树一次只能分两个范围，0-50和51-100，而B树，分成4个范围 1-25， 25-50，51-75，76-100一次就能筛选走四分之三的数据。所以作为多叉树的B树是更快的 B-树的查找 我们来看看B-树的查找，假设每个节点有 n 个 key值，被分割为 n+1 个区间，注意，每个 key 值紧跟着 data 域，这说明B-树的 key 和 data 是聚合在一起的。一般而言，根节点都在内存中，B-树以每个节点为一次磁盘 IO，比如上图中，若搜索 key 为 25 节点的 data，首先在根节点进行二分查找（因为 keys 有序，二分最快），判断 key 25 小于 key 50，所以定位到最左侧的节点，此时进行一次磁盘 IO，将该节点从磁盘读入内存，接着继续进行上述过程，直到找到该 key 为止。 查找伪代码： Data* BTreeSearch(Root *node, Key key) { Data* data; if(root == NULL) return NULL; data = BinarySearch(node); if(data->key == key) { return data; }else{ node = ReadDisk(data->next); BTreeSearch(node, key); } } B+ 树 B+树概述 B+树是B-树的变体，也是一种多路搜索树, 它与 B- 树的不同之处在于: 所有关键字存储在叶子节点出现,内部节点(非叶子节点并不存储真正的 data) 为所有叶子结点增加了一个链指针 简化 B+树 如下图 因为内节点并不存储 data，所以一般B+树的叶节点和内节点大小不同，而B-树的每个节点大小一般是相同的，为一页。 为了增加 区间访问性，一般会对B+树做一些优化。 如下图带顺序访问的B+树。 B-树和B+树的区别 1.B+树内节点不存储数据，所有 data 存储在叶节点导致查询时间复杂度固定为 log n。而B-树查询时间复杂度不固定，与 key 在树中的位置有关，最好为O(1)。 如下所示B-树/B+树查询节点 key 为 50 的 data。 B-树： 从上图可以看出，key 为 50 的节点就在第一层，B-树只需要一次磁盘 IO 即可完成查找。所以说B-树的查询最好时间复杂度是 O(1)。 B+树： 由于B+树所有的 data 域都在根节点，所以查询 key 为 50的节点必须从根节点索引到叶节点，时间复杂度固定为 O(log n)。 点评：B树的由于每个节点都有key和data，所以查询的时候可能不需要O(logn)的复杂度，甚至最好的情况是O(1)就可以找到数据，而B+树由于只有叶子节点保存了data，所以必须经历O(logn)复杂度才能找到数据 2. B+树叶节点两两相连可大大增加区间访问性，可使用在范围查询等，而B-树每个节点 key 和 data 在一起，则无法区间查找。 根据空间局部性原理：如果一个存储器的某个位置被访问，那么将它附近的位置也会被访问。 B+树可以很好的利用局部性原理，若我们访问节点 key为 50，则 key 为 55、60、62 的节点将来也可能被访问，我们可以利用磁盘预读原理提前将这些数据读入内存，减少了磁盘 IO 的次数。 当然B+树也能够很好的完成范围查询。比如查询 key 值在 50-70 之间的节点。 点评：由于B+树的叶子节点的数据都是使用链表连接起来的，而且他们在磁盘里是顺序存储的，所以当读到某个值的时候，磁盘预读原理就会提前把这些数据都读进内存，使得范围查询和排序都很快 3.B+树更适合外部存储。由于内节点无 data 域，每个节点能索引的范围更大更精确 这个很好理解，由于B-树节点内部每个 key 都带着 data 域，而B+树节点只存储 key 的副本，真实的 key 和 data 域都在叶子节点存储。前面说过磁盘是分 block 的，一次磁盘 IO 会读取若干个 block，具体和操作系统有关，那么由于磁盘 IO 数据大小是固定的，在一次 IO 中，单个元素越小，量就越大。这就意味着B+树单次磁盘 IO 的信息量大于B-树，从这点来看B+树相对B-树磁盘 IO 次数少。 点评：由于B树的节点都存了key和data，而B+树只有叶子节点存data，非叶子节点都只是索引值，没有实际的数据，这就时B+树在一次IO里面，能读出的索引值更多。从而减少查询时候需要的IO次数！ 从上图可以看出相同大小的区域，B-树仅有 2 个 key，而B+树有 3 个 key。 拓展：MySQL为什么使用B-Tree（B+Tree）&& 存储知识 上文说过，红黑树等数据结构也可以用来实现索引，但是文件系统及数据库系统普遍采用B-/+Tree作为索引结构，这一节将结合计算机组成原理相关知识讨论B-/+Tree作为索引的理论基础。 一般来说，索引本身也很大，不可能全部存储在内存中，因此索引往往以索引文件的形式存储的磁盘上。这样的话，索引查找过程中就要产生磁盘I/O消耗，相对于内存存取，I/O存取的消耗要高几个数量级，所以评价一个数据结构作为索引的优劣最重要的指标就是在查找过程中磁盘I/O操作次数的渐进复杂度。换句话说，索引的结构组织要尽量减少查找过程中磁盘I/O的存取次数。下面先介绍内存和磁盘存取原理，然后再结合这些原理分析B-/+Tree作为索引的效率。 存储数据最小单元 我们都知道计算机在存储数据的时候，有最小存储单元，这就好比我们今天进行现金的流通最小单位是一毛。 在计算机中磁盘存储数据最小单元是扇区，一个扇区的大小是512字节，而文件系统（例如XFS/EXT4）他的最小单元是块，一个块的大小是4k 而对于我们的InnoDB存储引擎也有自己的最小储存单元——页（Page），一个页的大小是16K。 下面几张图可以帮你理解最小存储单元： 文件系统中一个文件大小只有1个字节，但不得不占磁盘上4KB的空间。 磁盘扇区、文件系统、InnoDB存储引擎都有各自的最小存储单元。 在MySQL中我们的InnoDB页的大小默认是16k，当然也可以通过参数设置： 数据表中的数据都是存储在页中的，所以一个页中能存储多少行数据呢？假设一行数据的大小是1k，那么一个页可以存放16行这样的数据。 主存存取原理 目前计算机使用的主存基本都是随机读写存储器（RAM），现代RAM的结构和存取原理比较复杂，这里本文抛却具体差别，抽象出一个十分简单的存取模型来说明RAM的工作原理。 从抽象角度看，主存是一系列的存储单元组成的矩阵，每个存储单元存储固定大小的数据。每个存储单元有唯一的地址，现代主存的编址规则比较复杂，这里将其简化成一个二维地址：通过一个行地址和一个列地址可以唯一定位到一个存储单元。图5展示了一个4 x 4的主存模型。 主存的存取过程如下： 当系统需要读取主存时，则将地址信号放到地址总线上传给主存，主存读到地址信号后，解析信号并定位到指定存储单元，然后将此存储单元数据放到数据总线上，供其它部件读取。 写主存的过程类似，系统将要写入单元地址和数据分别放在地址总线和数据总线上，主存读取两个总线的内容，做相应的写操作。 这里可以看出，主存存取的时间仅与存取次数呈线性关系，因为不存在机械操作，两次存取的数据的“距离”不会对时间有任何影响，例如，先取A0再取A1和先取A0再取D3的时间消耗是一样的。 磁盘存取原理 上文说过，索引一般以文件形式存储在磁盘上，索引检索需要磁盘I/O操作。与主存不同，磁盘I/O存在机械运动耗费，因此磁盘I/O的时间消耗是巨大的。 图6是磁盘的整体结构示意图。 一个磁盘由大小相同且同轴的圆形盘片组成，磁盘可以转动（各个磁盘必须同步转动）。在磁盘的一侧有磁头支架，磁头支架固定了一组磁头，每个磁头负责存取一个磁盘的内容。磁头不能转动，但是可以沿磁盘半径方向运动（实际是斜切向运动），每个磁头同一时刻也必须是同轴的，即从正上方向下看，所有磁头任何时候都是重叠的（不过目前已经有多磁头独立技术，可不受此限制）。 图7是磁盘结构的示意图。 盘片被划分成一系列同心环，圆心是盘片中心，每个同心环叫做一个磁道，所有半径相同的磁道组成一个柱面。磁道被沿半径线划分成一个个小的段，每个段叫做一个扇区，每个扇区是磁盘的最小存储单元。为了简单起见，我们下面假设磁盘只有一个盘片和一个磁头。 当需要从磁盘读取数据时，系统会将数据逻辑地址传给磁盘，磁盘的控制电路按照寻址逻辑将逻辑地址翻译成物理地址，即确定要读的数据在哪个磁道，哪个扇区。为了读取这个扇区的数据，需要将磁头放到这个扇区上方，为了实现这一点，磁头需要移动对准相应磁道，这个过程叫做寻道，所耗费时间叫做寻道时间，然后磁盘旋转将目标扇区旋转到磁头下，这个过程耗费的时间叫做旋转时间。 局部性原理与磁盘预读 由于存储介质的特性，磁盘本身存取就比主存慢很多，再加上机械运动耗费，磁盘的存取速度往往是主存的几百分分之一，因此为了提高效率，要尽量减少磁盘I/O。为了达到这个目的，磁盘往往不是严格按需读取，而是每次都会预读，即使只需要一个字节，磁盘也会从这个位置开始，顺序向后读取一定长度的数据放入内存。这样做的理论依据是计算机科学中著名的局部性原理： 当一个数据被用到时，其附近的数据也通常会马上被使用。 程序运行期间所需要的数据通常比较集中。 由于磁盘顺序读取的效率很高（不需要寻道时间，只需很少的旋转时间），因此对于具有局部性的程序来说，预读可以提高I/O效率。 预读的长度一般为页（page）的整倍数。页是计算机管理存储器的逻辑块，硬件及操作系统往往将主存和磁盘存储区分割为连续的大小相等的块，每个存储块称为一页（在许多操作系统中，页得大小通常为4k），主存和磁盘以页为单位交换数据。当程序要读取的数据不在主存中时，会触发一个缺页异常，此时系统会向磁盘发出读盘信号，磁盘会找到数据的起始位置并向后连续读取一页或几页载入内存中，然后异常返回，程序继续运行。 所以IO一次就是读一页的大小 总结 从上面介绍的原理可以得出,为了迎合磁盘存储的特征,以及局部性原理 我们希望一种数据结构的高度要尽量小,数据要相对集中,每个节点的区间范围能尽量大 那么,B+树显然是最合适的 "},"数据库/mysql/2.mysql索引.html":{"url":"数据库/mysql/2.mysql索引.html","title":"2.mysql索引及优化⭐⭐⭐⭐","keywords":"","body":"一、初识索引1.1 为什么要有索引？1.2 什么是索引？1.3 你是否对索引存在误解？二、索引的原理2.1 索引原理2.2 磁盘IO与预读三、索引的数据结构四、MySQL索引管理4.1 功能4.2 MySQL常用的索引4.3 各个索引应用场景4.4 索引的两大类型hash与btree4.5 创建/删除索引的语法4.6 示例五、测试索引5.1 数据准备5.2 小结六、正确使用索引6.1 索引查询原则1、范围问题2. 选择区分度高的列建索引3. 索引列不能参与运算4、and/or5、最左前缀匹配原则6、其他情况6.2 其他注意事项七、联合索引和覆盖索引7.1 联合索引7.2 覆盖索引7.3 合并索引八、查询优化神器-explainExplain基础Explain进阶九、慢查询优化的基本步骤十、慢日志管理https://snailclimb.gitee.io/javaguide/#/docs/database/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B4%A2%E5%BC%95?id=%e4%b8%8d%e5%90%88%e9%80%82%e5%88%9b%e5%bb%ba%e7%b4%a2%e5%bc%95%e7%9a%84%e5%ad%97%e6%ae%b5 一、初识索引 1.1 为什么要有索引？ 一般的应用系统，读写比例在10:1左右，而且插入操作和一般的更新操作很少出现性能问题，在生产环境中，我们遇到最多的，也是最容易出问题的，还是一些复杂的查询操作，因此对查询语句的优化显然是重中之重。说起加速查询，就不得不提到索引了。 1.2 什么是索引？ 索引在MySQL中也叫是一种“键”，是存储引擎用于快速找到记录的一种数据结构。索引对于良好的性能非常关键，尤其是当表中的数据量越来越大时，索引对于性能的影响愈发重要。 索引优化应该是对查询性能优化最有效的手段了。索引能够轻易将查询性能提高好几个数量级。 索引相当于字典的音序表，如果要查某个字，如果不使用音序表，则需要从几百页中逐页去查。 1.3 你是否对索引存在误解？ 索引是应用程序设计和开发的一个重要方面。若索引太多，应用程序的性能可能会受到影响。而索引太少，对查询性能又会产生影响，要找到一个平衡点，这对应用程序的性能至关重要。一些开发人员总是在事后才想起添加索引----我一直认为，这源于一种错误的开发模式。如果知道数据的使用，从一开始就应该在需要处添加索引。开发人员往往对数据库的使用停留在应用的层面，比如编写SQL语句、存储过程之类，他们甚至可能不知道索引的存在，或认为事后让相关DBA加上即可。DBA往往不够了解业务的数据流，而添加索引需要通过监控大量的SQL语句进而从中找到问题，这个步骤所需的时间肯定是远大于初始添加索引所需的时间，并且可能会遗漏一部分的索引。当然索引也并不是越多越好，我曾经遇到过这样一个问题：某台MySQL服务器iostat显示磁盘使用率一直处于100%，经过分析后发现是由于开发人员添加了太多的索引，在删除一些不必要的索引之后，磁盘使用率马上下降为20%。可见索引的添加也是非常有技术含量的。 二、索引的原理 2.1 索引原理 索引的目的在于提高查询效率，与我们查阅图书所用的目录是一个道理：先定位到章，然后定位到该章下的一个小节，然后找到页数。相似的例子还有：查字典，查火车车次，飞机航班等 本质都是：通过不断地缩小想要获取数据的范围来筛选出最终想要的结果，同时把随机的事件变成顺序的事件，也就是说，有了这种索引机制，我们可以总是用同一种查找方式来锁定数据。 数据库也是一样，但显然要复杂的多，因为不仅面临着等值查询，还有范围查询(>、 2.2 磁盘IO与预读 前面提到了访问磁盘，那么这里先简单介绍一下磁盘IO和预读，磁盘读取数据靠的是机械运动，每次读取数据花费的时间可以分为寻道时间、旋转延迟、传输时间三个部分，寻道时间指的是磁臂移动到指定磁道所需要的时间，主流磁盘一般在5ms以下；旋转延迟就是我们经常听说的磁盘转速，比如一个磁盘7200转，表示每分钟能转7200次，也就是说1秒钟能转120次，旋转延迟就是1/120/2 = 4.17ms；传输时间指的是从磁盘读出或将数据写入磁盘的时间，一般在零点几毫秒，相对于前两个时间可以忽略不计。那么访问一次磁盘的时间，即一次磁盘IO的时间约等于5+4.17 = 9ms左右，听起来还挺不错的，但要知道一台500 -MIPS（Million Instructions Per Second）的机器每秒可以执行5亿条指令，因为指令依靠的是电的性质，换句话说执行一次IO的时间可以执行约450万条指令，数据库动辄十万百万乃至千万级数据，每次9毫秒的时间，显然是个灾难。下图是计算机硬件延迟的对比图，供大家参考： 考虑到磁盘IO是非常高昂的操作，计算机操作系统做了一些优化，当一次IO时，不光把当前磁盘地址的数据，而是把相邻的数据也都读取到内存缓冲区内，因为局部预读性原理告诉我们，当计算机访问一个地址的数据的时候，与其相邻的数据也会很快被访问到。每一次IO读取的数据我们称之为一页(page)。具体一页有多大数据跟操作系统有关，一般为4k或8k，也就是我们读取一页内的数据时候，实际上才发生了一次IO，这个理论对于索引的数据结构设计非常有帮助。 三、索引的数据结构 可参考上一篇文章 四、MySQL索引管理 4.1 功能 索引的功能就是加速查找 mysql中的primary key，unique，联合唯一也都是索引，这些索引除了加速查找以外，还有约束的功能 4.2 MySQL常用的索引 普通索引INDEX：加速查找 唯一索引： 主键索引PRIMARY KEY：加速查找+约束（不为空、不能重复） 唯一索引UNIQUE:加速查找+约束（不能重复） 联合索引： PRIMARY KEY(id,name):联合主键索引 UNIQUE(id,name):联合唯一索引 INDEX(id,name):联合普通索引 4.3 各个索引应用场景 举个例子来说，比如你在为某商场做一个会员卡的系统。 这个系统有一个会员表 有下列字段： 会员编号 INT 会员姓名 VARCHAR(10) 会员身份证号码 VARCHAR(18) 会员电话 VARCHAR(10) 会员住址 VARCHAR(50) 会员备注信息 TEXT 那么这个 会员编号，作为主键，使用 PRIMARY 会员姓名 如果要建索引的话，那么就是普通的 INDEX 会员身份证号码 如果要建索引的话，那么可以选择 UNIQUE （唯一的，不允许重复） # 除此之外还有全文索引，即FULLTEXT 会员备注信息 ， 如果需要建索引的话，可以选择全文搜索。 用于搜索很长一篇文章的时候，效果最好。 用在比较短的文本，如果就一两行字的，普通的 INDEX 也可以。 但其实对于全文搜索，我们并不会使用MySQL自带的该索引，而是会选择第三方软件如Sphinx，专门来做全文搜索。 # 其他的如空间索引SPATIAL，了解即可，几乎不用 各个索引的应用场景 各个索引的应用场景 4.4 索引的两大类型hash与btree 我们可以在创建上述索引的时候，为其指定索引类型，分两类： hash类型的索引：查询单条快，范围查询慢 btree类型的索引：b+树，层数越多，数据量指数级增长（我们就用它，因为innodb默认支持它） 不同的存储引擎支持的索引类型也不一样： InnoDB 支持事务，支持行级别锁定，支持 B-tree、Full-text 等索引，不支持 Hash 索引； MyISAM 不支持事务，支持表级别锁定，支持 B-tree、Full-text 等索引，不支持 Hash 索引； Memory 不支持事务，支持表级别锁定，支持 B-tree、Hash 等索引，不支持 Full-text 索引； NDB 支持事务，支持行级别锁定，支持 Hash 索引，不支持 B-tree、Full-text 等索引； Archive 不支持事务，支持表级别锁定，不支持 B-tree、Hash、Full-text 等索引； 4.5 创建/删除索引的语法 # 方法一：创建表时 　　CREATE TABLE 表名 ( 字段名1 数据类型 [完整性约束条件…], 字段名2 数据类型 [完整性约束条件…], [UNIQUE | FULLTEXT | SPATIAL ] INDEX | KEY [索引名] (字段名[(长度)] [ASC |DESC]) ); # 方法二：CREATE在已存在的表上创建索引 CREATE [UNIQUE | FULLTEXT | SPATIAL ] INDEX 索引名 ON 表名 (字段名[(长度)] [ASC |DESC]) ; # 方法三：ALTER TABLE在已存在的表上创建索引 ALTER TABLE 表名 ADD [UNIQUE | FULLTEXT | SPATIAL ] INDEX 索引名 (字段名[(长度)] [ASC |DESC]) ; # 删除索引：DROP INDEX 索引名 ON 表名字; 4.6 示例 # 方式一 create table t1( id int, name char, age int, sex enum('male','female'), unique key uni_id(id), index ix_name(name) # index没有key ); create table t1( id int, name char, age int, sex enum('male','female'), unique key uni_id(id), index(name) # index没有key ); # 方式二 create index ix_age on t1(age); # 方式三 alter table t1 add index ix_sex(sex); alter table t1 add index(sex); # 查看 mysql> show create table t1; | t1 | CREATE TABLE `t1` ( `id` int(11) DEFAULT NULL, `name` char(1) DEFAULT NULL, `age` int(11) DEFAULT NULL, `sex` enum('male','female') DEFAULT NULL, UNIQUE KEY `uni_id` (`id`), KEY `ix_name` (`name`), KEY `ix_age` (`age`), KEY `ix_sex` (`sex`) ) ENGINE=InnoDB DEFAULT CHARSET=latin1 五、测试索引 5.1 数据准备 # 1. 准备表 create table s1( id int, name varchar(20), gender char(6), email varchar(50) ); # 2. 创建存储过程，实现批量插入记录 delimiter $$ # 声明存储过程的结束符号为$$ create procedure auto_insert1() BEGIN declare i int default 1; while(i1、在没有索引的前提下测试查询速度 无索引：mysql根本就不知道到底是否存在id等于333333333的记录，只能把数据表从头到尾扫描一遍，此时有多少个磁盘块就需要进行多少IO操作，所以查询速度很慢 mysql> select * from s1 where id=333333333; Empty set (0.33 sec) 2、在表中已经存在大量数据的前提下，为某个字段段建立索引，建立速度会很慢 3、在索引建立完毕后，以该字段为查询条件时，查询速度提升明显 create index id_index on s1(id) 注意： mysql先去索引表里根据b+树的搜索原理很快搜索到id等于333333333的记录不存在，IO大大降低，因而速度明显提升 5.2 小结 一定是为搜索条件的字段创建索引，比如select * from s1 where id = 333;就需要为id加上索引 在表中已经有大量数据的情况下，建索引会很慢，且占用硬盘空间，建完后查询速度加快，比如create index idx on s1(id);会扫描表中所有的数据，然后以id为数据项，创建索引结构，存放于硬盘的表中。建完以后，再查询就会很快了。 需要注意的是：innodb表的索引会存放于s1.ibd文件中，而myisam表的索引则会有单独的索引文件table1.MYI MySAM索引文件和数据文件是分离的，索引文件仅保存数据记录的地址。而在innodb中，表数据文件本身就是按照B+Tree（BTree即Balance True）组织的一个索引结构，这棵树的叶节点data域保存了完整的数据记录。这个索引的key是数据表的主键，因此innodb表数据文件本身就是主索引。 因为inndob的数据文件要按照主键聚集，所以innodb要求表必须要有主键（Myisam可以没有），如果没有显式定义，则mysql系统会自动选择一个可以唯一标识数据记录的列作为主键，如果不存在这种列，则mysql会自动为innodb表生成一个隐含字段作为主键，这字段的长度为6个字节，类型为长整型. 六、正确使用索引 6.1 索引查询原则 并不是说我们创建了索引就一定会加快查询速度，若想利用索引达到预想的提高查询速度的效果，我们在添加索引时，必须遵循以下问题： 1、范围问题 1、范围问题，或者说条件不明确，条件中出现这些符号或关键字：>、>=、 不等于！= between ...and... like 2. 选择区分度高的列建索引 尽量选择区分度高的列作为索引,区分度的公式是count(distinct col)/count(*)，表示字段不重复的比例，比例越大我们扫描的记录数越少，唯一键的区分度是1，而一些状态、性别字段可能在大数据面前区分度就是0，那可能有人会问，这个比例有什么经验值吗？使用场景不同，这个值也很难确定，一般需要join的字段我们都要求是0.1以上，即平均1条扫描10条记录。 先把表中的索引都删除，让我们专心研究区分度的问题： mysql> desc s1; +--------+-------------+------+-----+---------+-------+ | Field | Type | Null | Key | Default | Extra | +--------+-------------+------+-----+---------+-------+ | id | int(11) | YES | MUL | NULL | | | name | varchar(20) | YES | | NULL | | | gender | char(5) | YES | | NULL | | | email | varchar(50) | YES | MUL | NULL | | +--------+-------------+------+-----+---------+-------+ rows in set (0.00 sec) mysql> drop index a on s1; Query OK, 0 rows affected (0.20 sec) Records: 0 Duplicates: 0 Warnings: 0 mysql> drop index d on s1; Query OK, 0 rows affected (0.18 sec) Records: 0 Duplicates: 0 Warnings: 0 mysql> desc s1; +--------+-------------+------+-----+---------+-------+ | Field | Type | Null | Key | Default | Extra | +--------+-------------+------+-----+---------+-------+ | id | int(11) | YES | | NULL | | | name | varchar(20) | YES | | NULL | | | gender | char(5) | YES | | NULL | | | email | varchar(50) | YES | | NULL | | +--------+-------------+------+-----+---------+-------+ rows in set (0.00 sec) 分析原因： 我们编写存储过程为表s1批量添加记录，name字段的值均为egon，也就是说name这个字段的区分度很低（gender字段也是一样的，我们稍后再搭理它） 回忆b+树的结构，查询的速度与树的高度成反比，要想将树的高低控制的很低，需要保证：在某一层内数据项均是按照从左到右，从小到大的顺序依次排开，即左1 而对于区分度低的字段，无法找到大小关系，因为值都是相等的，毫无疑问，还想要用b+树存放这些等值的数据，只能增加树的高度，字段的区分度越低，则树的高度越高。极端的情况，索引字段的值都一样，那么b+树几乎成了一根棍。本例中就是这种极端的情况，name字段所有的值均为'nick' 现在我们得出一个结论：为区分度低的字段建立索引，索引树的高度会很高，然而这具体会带来什么影响呢？？？ 如果条件是name='xxxx',那么肯定是可以第一时间判断出'xxxx'是不在索引树中的（因为树中所有的值均为'nick’），所以查询速度很快 如果条件正好是name='nick',查询时，我们永远无法从树的某个位置得到一个明确的范围，只能往下找，往下找，往下找。。。这与全表扫描的IO次数没有多大区别，所以速度很慢 3. 索引列不能参与运算 索引列不能在条件中参与计算,原因很简单 4、and/or and与or的逻辑 条件1 and 条件2：所有条件都成立才算成立，但凡要有一个条件不成立则最终结果不成立 条件1 or 条件2：只要有一个条件成立则最终结果就成立 and的工作原理 条件：a = 10 and b = 'xxx' and c > 3 and d =4 索引：制作联合索引(d,a,b,c) 工作原理：对于连续多个and：mysql会按照联合索引，从左到右的顺序找一个区分度高的索引字段(这样便可以快速锁定很小的范围)，加速查询，即按照d—>a->b->c的顺序 or的工作原理 条件：a = 10 or b = 'xxx' or c > 3 or d =4 索引：制作联合索引(d,a,b,c) 工作原理：对于连续多个or：mysql会按照条件的顺序，从左到右依次判断，即a->b->c->d 在左边条件成立但是索引字段的区分度低的情况下（name与gender均属于这种情况），会依次往右找到一个区分度高的索引字段，加速查询。 经过分析，在条件为name='nick' and gender='male' and id>333 and email='xxx'的情况下，我们完全没必要为前三个条件的字段加索引，因为只能用上email字段的索引，前三个字段的索引反而会降低我们的查询效率 5、最左前缀匹配原则 非常重要的原则，对于组合索引mysql会一直向右匹配直到遇到范围查询(>、 3 and d = 4 如果建立(a,b,c,d)顺序的索引，d是用不到索引的，如果建立(a,b,d,c)的索引则都可以用到，a,b,d的顺序可以任意调整。 6、其他情况 - 使用函数 select * from tb1 where reverse(email) = 'nick'; - 类型不一致 如果列是字符串类型，传入条件是必须用引号引起来，不然... select * from tb1 where email = 999; #排序条件为索引，则select字段必须也是索引字段，否则无法命中 - order by select name from s1 order by email desc; 当根据索引排序时候，select查询的字段如果不是索引，则速度仍然很慢 select email from s1 order by email desc; 特别的：如果对主键排序，则还是速度很快： select * from tb1 order by nid desc; - 组合索引最左前缀 如果组合索引为：(name,email) name and email -- 命中索引 name -- 命中索引 email -- 未命中索引 - count(1)或count(列)代替count(*)在mysql中没有差别了 - create index xxxx on tb(title(19)) #text类型，必须制定长度 6.2 其他注意事项 避免使用select * 使用count(*) 创建表时尽量使用 char 代替 varchar 表的字段顺序固定长度的字段优先 组合索引代替多个单列索引（由于mysql中每次只能使用一个索引，所以经常使用多个条件查询时更适合使用组合索引） 尽量使用短索引 使用连接（JOIN）来代替子查询(Sub-Queries) 连表时注意条件类型需一致 索引散列值（重复少）不适合建索引，例：性别不适合 七、联合索引和覆盖索引 7.1 联合索引 联合索引是指对表上的多个列合起来做一个索引。联合索引的创建方法与单个索引的创建方法一样，不同之处仅在于有多个索引列，如下： mysql> create table t( -> a int, -> b int, -> primary key(a), -> key idx_a_b(a,b) -> ); Query OK, 0 rows affected (0.11 sec) 那么何时需要使用联合索引呢？在讨论这个问题之前，先来看一下联合索引内部的结果。从本质上来说，联合索引就是一棵B+树，不同的是联合索引的键值得数量不是1，而是>=2。接着来讨论两个整型列组成的联合索引，假定两个键值得名称分别为a、b如图： 可以看到这与我们之前看到的单个键的B+树并没有什么不同，键值都是排序的，通过叶子结点可以逻辑上顺序地读出所有数据，就上面的例子来说，即（1,1），（1,2），（2,1），（2,4），（3,1），（3,2），数据按（a,b）的顺序进行了存放。 因此，对于查询select from table where a=xxx and b=xxx, 显然是可以使用(a,b) 这个联合索引的，对于单个列a的查询select from table where a=xxx,也是可以使用（a,b）这个索引的。 但对于b列的查询select * from table where b=xxx,则不可以使用（a,b） 索引，其实你不难发现原因，叶子节点上b的值为1、2、1、4、1、2显然不是排序的，因此对于b列的查询使用不到(a,b) 索引 联合索引的第二个好处是在第一个键相同的情况下，已经对第二个键进行了排序处理，例如在很多情况下应用程序都需要查询某个用户的购物情况，并按照时间进行排序，最后取出最近三次的购买记录，这时使用联合索引可以帮我们避免多一次的排序操作，因为索引本身在叶子节点已经排序了，如下 # ===========准备表============== create table buy_log( userid int unsigned not null, buy_date date ); insert into buy_log values (1,'2009-01-01'), (2,'2009-01-01'), (3,'2009-01-01'), (1,'2009-02-01'), (3,'2009-02-01'), (1,'2009-03-01'), (1,'2009-04-01'); alter table buy_log add key(userid); alter table buy_log add key(userid,buy_date); # ===========验证============== mysql> show create table buy_log; | buy_log | CREATE TABLE `buy_log` ( `userid` int(10) unsigned NOT NULL, `buy_date` date DEFAULT NULL, KEY `userid` (`userid`), KEY `userid_2` (`userid`,`buy_date`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8 | # 可以看到possible_keys在这里有两个索引可以用，分别是单个索引userid与联合索引userid_2,但是优化器最终选择了使用的key是userid因为该索引的叶子节点包含单个键值，所以理论上一个页能存放的记录应该更多 mysql> explain select * from buy_log where userid=2; +----+-------------+---------+------+-----------------+--------+---------+-------+------+-------+ | id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra | +----+-------------+---------+------+-----------------+--------+---------+-------+------+-------+ | 1 | SIMPLE | buy_log | ref | userid,userid_2 | userid | 4 | const | 1 | | +----+-------------+---------+------+-----------------+--------+---------+-------+------+-------+ row in set (0.00 sec) # 接着假定要取出userid为1的最近3次的购买记录，用的就是联合索引userid_2了，因为在这个索引中，在userid=1的情况下，buy_date都已经排序好了 mysql> explain select * from buy_log where userid=1 order by buy_date desc limit 3; +--+-----------+-------+----+---------------+--------+-------+-----+----+------------------------+ |id|select_type|table |type|possible_keys | key |key_len|ref |rows| Extra | +--+-----------+-------+----+---------------+--------+-------+-----+----+------------------------+ | 1|SIMPLE |buy_log|ref |userid,userid_2|userid_2| 4 |const| 4 |Using where; Using index| +--+-----------+-------+----+---------------+--------+-------+-----+----+------------------------+ row in set (0.00 sec) # ps：如果extra的排序显示是Using filesort，则意味着在查出数据后需要二次排序(如下查询语句，没有先用where userid=3先定位范围，于是即便命中索引也没用，需要二次排序) mysql> explain select * from buy_log order by buy_date desc limit 3; +--+-----------+-------+-----+-------------+--------+-------+----+----+---------------------------+ |id|select_type| table |type |possible_keys|key |key_len|ref |rows|Extra | +--+-----------+-------+-----+-------------+--------+-------+----+----+---------------------------+ | 1|SIMPLE |buy_log|index| NULL |userid_2| 8 |NULL| 7 |Using index; Using filesort| +--+-----------+-------+-----+-------------+--------+-------+----+----+---------------------------+ # 对于联合索引（a,b）,下述语句可以直接使用该索引，无需二次排序 select ... from table where a=xxx order by b; # 然后对于联合索引(a,b,c)来首，下列语句同样可以直接通过索引得到结果 select ... from table where a=xxx order by b; select ... from table where a=xxx and b=xxx order by c; # 但是对于联合索引(a,b,c)，下列语句不能通过索引直接得到结果，还需要自己执行一次filesort操作，因为索引（a，c)并未排序 select ... from table where a=xxx order by c; 7.2 覆盖索引 InnoDB存储引擎支持覆盖索引（covering index，或称索引覆盖），即从辅助索引中就可以得到查询记录，而不需要查询聚集索引中的记录。 使用覆盖索引的一个好处是：辅助索引不包含整行记录的所有信息，故其大小要远小于聚集索引(主键索引)，因此可以减少大量的IO操作。 覆盖索引原理基于以下: 辅助索引的叶子节点除了存储该辅助索引的数据外,还会存储主键索引 若select的并不在指定的辅助索引代表的列的范围内,就会根据主键索引回表到主键B+树进行二次查询 若select的在该范围内,就无需二次回表,也即所谓的覆盖的含义 注意：覆盖索引技术最早是在InnoDB Plugin中完成并实现，这意味着对于InnoDB版本小于1.0的，或者MySQL数据库版本为5.0以下的，InnoDB存储引擎不支持覆盖索引特性。 对于InnoDB存储引擎的辅助索引而言，由于其包含了主键信息，因此其叶子节点存放的数据为（primary key1，priamey key2，...,key1，key2，...）。例如： select age from s1 where id=123 and name = 'nick'; #id字段有索引，但是name字段没有索引,该sql命中了索引，但未覆盖，需要去聚集索引中再查找详细信息。 最牛逼的情况是，索引字段覆盖了所有，那全程通过索引来加速查询以及获取结果就ok了 mysql> desc s1; +--------+-------------+------+-----+---------+-------+ | Field | Type | Null | Key | Default | Extra | +--------+-------------+------+-----+---------+-------+ | id | int(11) | NO | | NULL | | | name | varchar(20) | YES | | NULL | | | gender | char(6) | YES | | NULL | | | email | varchar(50) | YES | | NULL | | +--------+-------------+------+-----+---------+-------+ rows in set (0.21 sec) mysql> explain select name from s1 where id=1000; #没有任何索引 +--+-----------+-----+----------+----+-------------+----+-------+----+-------+--------+-----------+ |id|select_type|table|partitions|type|possible_keys|key |key_len|ref | rows |filtered| Extra | +--+-----------+-----+----------+----+-------------+----+-------+----+-------+--------+-----------+ | 1| SIMPLE | s1 | NULL |ALL | NULL |NULL| NULL |NULL|2688336| 10.00 |Using where| +--+-----------+-----+----------+----+-------------+----+-------+----+-------+--------+-----------+ row in set, 1 warning (0.00 sec) mysql> create index idx_id on s1(id); #创建索引 Query OK, 0 rows affected (4.16 sec) Records: 0 Duplicates: 0 Warnings: 0 mysql> explain select name from s1 where id=1000; #命中辅助索引，但是未覆盖索引，还需要从聚集索引中查找name +--+-----------+-----+----------+----+-------------+------+-------+-----+----+--------+-----+ |id|select_type|table|partitions|type|possible_keys| key|key_len| ref |rows|filtered|Extra| +--+-----------+-----+----------+----+-------------+------+-------+-----+----+--------+-----+ | 1| SIMPLE | s1 | NULL | ref| idx_id |idx_id| 4 |const| 1 | 100.00 | NULL| +--+-----------+-----+----------+----+-------------+------+-------+-----+----+--------+-----+ row in set, 1 warning (0.08 sec) mysql> explain select id from s1 where id=1000; #在辅助索引中就找到了全部信息，Using index代表覆盖索引 +--+-----------+-----+----------+----+-------------+------+-------+-------+------+----------+-----+ |id|select_type|table|partitions|type|possible_keys| key |key_len| ref |rows|filtered| Extra | +--+-----------+-----+----------+----+--------------------+-------+-------+------+----------+-----+ | 1| SIMPLE | s1 | NULL | ref| idx_id |idx_id| 4 |const| 1 | 100.00 |Using index| +--+-----------+-----+----------+----+-------------+------+-------+-----+----+--------+-----------+ row in set, 1 warning (0.03 sec) 覆盖索引的另外一个好处是对某些统计问题而言的。基于上一小结创建的表buy_log，查询计划如下： mysql> explain select count(*) from buy_log; +--+-----------+-------+-----+-------------+------+-------+----+----+-----------+ |id|select_type|table | type|possible_keys|key |key_len|ref |rows|Extra | +--+-----------+-------+-----+-------------+------+-------+----+----+-----------+ | 1| SIMPLE |buy_log|index| NULL |userid| 4 |NULL| 7 |Using index| +--+-----------+-------+-----+-------------+------+-------+----+----+-----------+ row in set (0.00 sec) innodb存储引擎并不会选择通过查询聚集索引来进行统计。由于buy_log表有辅助索引，而辅助索引远小于聚集索引，选择辅助索引可以减少IO操作，故优化器的选择如上key为userid辅助索引 对于（a,b）形式的联合索引，一般是不可以选择b中所谓的查询条件。但如果是统计操作，并且是覆盖索引，则优化器还是会选择使用该索引，如下： # 联合索引userid_2（userid,buy_date）,一般情况，我们按照buy_date是无法使用该索引的，但特殊情况下：查询语句是统计操作，且是覆盖索引，则按照buy_date当做查询条件时，也可以使用该联合索引 mysql> explain select count(*) from buy_log where buy_date >= '2011-01-01' and buy_date 7.3 合并索引 我们的 where 中可能有多个条件(或者join)涉及到多个字段，它们之间进行 AND 或者 OR，那么此时就有可能会使用到 index merge 技术。index merge 技术如果简单的说，其实就是：对多个索引分别进行条件扫描，然后将它们各自的结果进行合并(intersect/union)。 MySQL5.0之前，一个表一次只能使用一个索引，无法同时使用多个索引分别进行条件扫描。但是从5.1开始，引入了 index merge 优化技术，对同一个表可以使用多个索引分别进行条件扫描。 mysql> explain select count(email) from index_t where id = 1000000 or email='eva100000@oldboy'; +--+-----------+------+--------------+--------------------------------+---------------+--------+-----+----+-----------------------------------------+ | id | select_type| table | type | possible_keys | key | key_len | ref |rows | Extra | +--+-----------+------+--------------+--------------------------------+---------------+--------+-----+----+-----------------------------------------+ | 1 | SIMPLE | index_t| index_merge | PRIMARY,email,ind_id,ind_email | PRIMARY,email | 4,51 |NULL| 2 |Using union(PRIMARY,email); Using where | +--+-----------+------+--------------+--------------------------------+---------------+--------+-----+----+-----------------------------------------+ row in set (0.01 sec) 八、查询优化神器-explain Explain基础 关于explain命令相信大家并不陌生，具体用法和字段含义可以参考官网explain-output ，这里需要强调rows是核心指标，绝大部分rows小的语句执行一定很快（有例外，下面会讲到）。所以优化语句基本上都是在优化rows。 执行计划：让mysql预估执行操作(一般正确) all Explain进阶 Explain命令在解决数据库性能上是第一推荐使用命令，大部分的性能问题可以通过此命令来简单的解决，Explain可以用来查看SQL语句的执行效 果，可以帮助选择更好的索引和优化查询语句，写出更好的优化语句。 Explain语法：explain select … from … [where …] 例如：explain select * from news; 输出： +--+-----------+-----+----+-------------+---+-------+---+----+-----+ |id|select_type|table|type|possible_keys|key|key_len|ref|rows|Extra| +--+-----------+-----+----+-------------+---+-------+---+----+-----+ 下面对各个属性进行了解： 1、id：这是SELECT的查询序列号 2、select_type：select_type就是select的类型，可以有以下几种： SIMPLE：简单SELECT(不使用UNION或子查询等) PRIMARY：最外面的SELECT UNION：UNION中的第二个或后面的SELECT语句 DEPENDENT UNION：UNION中的第二个或后面的SELECT语句，取决于外面的查询 UNION RESULT：UNION的结果。 SUBQUERY：子查询中的第一个SELECT DEPENDENT SUBQUERY：子查询中的第一个SELECT，取决于外面的查询 DERIVED：导出表的SELECT(FROM子句的子查询) 3、table：显示这一行的数据是关于哪张表的 4、type：这列最重要，显示了连接使用了哪种类别,有无使用索引，是使用Explain命令分析性能瓶颈的关键项之一。 结果值从好到坏依次是： system > const > eq_ref > ref > fulltext > ref_or_null > index_merge > unique_subquery > index_subquery > range > index > ALL 一般来说，得保证查询至少达到range级别，最好能达到ref，否则就可能会出现性能问题。 5、possible_keys：列指出MySQL能使用哪个索引在该表中找到行 6、key：显示MySQL实际决定使用的键（索引）。如果没有选择索引，键是NULL 7、key_len：显示MySQL决定使用的键长度。如果键是NULL，则长度为NULL。使用的索引的长度。在不损失精确性的情况下，长度越短越好 8、ref：显示使用哪个列或常数与key一起从表中选择行。 9、rows：显示MySQL认为它执行查询时必须检查的行数。 10、Extra：包含MySQL解决查询的详细信息，也是关键参考项之一。 Distinct 一旦MYSQL找到了与行相联合匹配的行，就不再搜索了 Not exists MYSQL 优化了LEFT JOIN，一旦它找到了匹配LEFT JOIN标准的行， 就不再搜索了 Range checked for each Record（index map:#） 没有找到理想的索引，因此对于从前面表中来的每一 个行组合，MYSQL检查使用哪个索引，并用它来从表中返回行。这是使用索引的最慢的连接之一 Using filesort 看 到这个的时候，查询就需要优化了。MYSQL需要进行额外的步骤来发现如何对返回的行排序。它根据连接类型以及存储排序键值和匹配条件的全部行的行指针来 排序全部行 Using index 列数据是从仅仅使用了索引中的信息而没有读取实际的行动的表返回的，这发生在对表 的全部的请求列都是同一个索引的部分的时候 Using temporary 看到这个的时候，查询需要优化了。这 里，MYSQL需要创建一个临时表来存储结果，这通常发生在对不同的列集进行ORDER BY上，而不是GROUP BY上 Using where 使用了WHERE从句来限制哪些行将与下一张表匹配或者是返回给用户。如果不想返回表中的全部行，并且连接类型ALL或index， 这就会发生，或者是查询有问题 其他一些Tip：当type 显示为 “index” 时，并且Extra显示为“Using Index”， 表明使用了覆盖索引。 九、慢查询优化的基本步骤 先运行看看是否真的很慢，注意设置SQL_NO_CACHE where条件单表查，锁定最小返回记录表。这句话的意思是把查询语句的where都应用到表中返回的记录数最小的表开始查起，单表每个字段分别查询，看哪个字段的区分度最高 explain查看执行计划，是否与1预期一致（从锁定记录较少的表开始查询） order by limit 形式的sql语句让排序的表优先查 了解业务方使用场景 加索引时参照建索引的几大原则 观察结果，不符合预期继续从0分析 十、慢日志管理 慢日志 - 执行时间 > 10 - 未命中索引 - 日志文件路径 配置： - 内存 show variables like '%query%'; show variables like '%queries%'; set global 变量名 = 值 - 配置文件 mysqld --defaults-file='E:\\wupeiqi\\mysql-5.7.16-winx64\\mysql-5.7.16-winx64\\my-default.ini' my.conf内容： slow_query_log = ON slow_query_log_file = D:/.... 注意：修改配置文件之后，需要重启服务 MySQL日志管理 ======================================================== 错误日志: 记录 MySQL 服务器启动、关闭及运行错误等信息 二进制日志: 又称binlog日志，以二进制文件的方式记录数据库中除 SELECT 以外的操作 查询日志: 记录查询的信息 慢查询日志: 记录执行时间超过指定时间的操作 中继日志： 备库将主库的二进制日志复制到自己的中继日志中，从而在本地进行重放 通用日志： 审计哪个账号、在哪个时段、做了哪些事件 事务日志或称redo日志： 记录Innodb事务相关的如事务执行时间、检查点等 ======================================================== 一、bin-log 1. 启用 # vim /etc/my.cnf [mysqld] log-bin[=dir\\[filename]] # service mysqld restart 2. 暂停 //仅当前会话 SET SQL_LOG_BIN=0; SET SQL_LOG_BIN=1; 3. 查看 查看全部： # mysqlbinlog mysql.000002 按时间： # mysqlbinlog mysql.000002 --start-datetime=\"2012-12-05 10:02:56\" # mysqlbinlog mysql.000002 --stop-datetime=\"2012-12-05 11:02:54\" # mysqlbinlog mysql.000002 --start-datetime=\"2012-12-05 10:02:56\" --stop-datetime=\"2012-12-05 11:02:54\" 按字节数： # mysqlbinlog mysql.000002 --start-position=260 # mysqlbinlog mysql.000002 --stop-position=260 # mysqlbinlog mysql.000002 --start-position=260 --stop-position=930 4. 截断bin-log（产生新的bin-log文件） a. 重启mysql服务器 b. # mysql -uroot -p123 -e 'flush logs' 5. 删除bin-log文件 # mysql -uroot -p123 -e 'reset master' 二、查询日志 启用通用查询日志 # vim /etc/my.cnf [mysqld] log[=dir\\[filename]] # service mysqld restart 三、慢查询日志 启用慢查询日志 # vim /etc/my.cnf [mysqld] log-slow-queries[=dir\\[filename]] long_query_time=n # service mysqld restart MySQL 5.6: slow-query-log=1 slow-query-log-file=slow.log long_query_time=3 单位为秒 查看慢查询日志 测试:BENCHMARK(count,expr) SELECT BENCHMARK(50000000,2*3); "},"架构/0.架构演进.html":{"url":"架构/0.架构演进.html","title":"1.架构演进","keywords":"","body":"1、概述2、基本概念3、架构演进3.2第一次演进：Tomcat与数据库分开部署3.3 第二次演进：引入本地缓存和分布式缓存3.4 第三次演进：引入反向代理实现负载均衡3.5 第四次演进：数据库读写分离3.6 第五次演进：数据库按业务分库3.7 第六次演进：把大表拆分为小表3.8 第七次演进：使用LVS或F5来使多个Nginx负载均衡3.9 第八次演进：通过DNS轮询实现机房间的负载均衡3.10 第九次演进：引入NoSQL数据库和搜索引擎等技术3.11 第十次演进：大应用拆分为小应用3.12 第十一次演进：复用的功能抽离成微服务3.13 第十二次演进：引入企业服务总线ESB屏蔽服务接口的访问差异3.14 第十三次演进：引入容器化技术实现运行环境隔离与动态服务管理3.15 第十四次演进：以云平台承载系统4、架构设计总结1、概述 本文以淘宝作为例子，介绍从一百个并发到千万级并发情况下服务端的架构的演进过程，同时列举出每个演进阶段会遇到的相关技术，让大家对架构的演进有一个整体的认知，文章最后汇总了一些架构设计的原则。 2、基本概念 在介绍架构之前，为了避免部分读者对架构设计中的一些概念不了解，下面对几个最基础的概念进行介绍。 1）什么是分布式？ 系统中的多个模块在不同服务器上部署，即可称为分布式系统，如Tomcat和数据库分别部署在不同的服务器上，或两个相同功能的Tomcat分别部署在不同服务器上。 2）什么是高可用？ 系统中部分节点失效时，其他节点能够接替它继续提供服务，则可认为系统具有高可用性。 3）什么是集群？ 一个特定领域的软件部署在多台服务器上并作为一个整体提供一类服务，这个整体称为集群。 如Zookeeper中的Master和Slave分别部署在多台服务器上，共同组成一个整体提供集中配置服务。 在常见的集群中，客户端往往能够连接任意一个节点获得服务，并且当集群中一个节点掉线时，其他节点往往能够自动的接替它继续提供服务，这时候说明集群具有高可用性。 4）什么是负载均衡？ 请求发送到系统时，通过某些方式把请求均匀分发到多个节点上，使系统中每个节点能够均匀的处理请求负载，则可认为系统是负载均衡的。 5）什么是正向代理和反向代理？ 系统内部要访问外部网络时，统一通过一个代理服务器把请求转发出去，在外部网络看来就是代理服务器发起的访问，此时代理服务器实现的是正向代理； 当外部请求进入系统时，代理服务器把该请求转发到系统中的某台服务器上，对外部请求来说，与之交互的只有代理服务器，此时代理服务器实现的是反向代理。 简单来说，正向代理是代理服务器代替系统内部来访问外部网络的过程，反向代理是外部请求访问系统时通过代理服务器转发到内部服务器的过程。 3、架构演进 3.1 单机架构 以淘宝作为例子：在网站最初时，应用数量与用户数都较少，可以把Tomcat和数据库部署在同一台服务器上。 浏览器往www.taobao.com发起请求时，首先经过DNS服务器（域名系统）把域名转换为实际IP地址10.102.4.1，浏览器转而访问该IP对应的Tomcat。 架构瓶颈：随着用户数的增长，Tomcat和数据库之间竞争资源，单机性能不足以支撑业务。 3.2第一次演进：Tomcat与数据库分开部署 image Tomcat和数据库分别独占服务器资源，显著提高两者各自性能。 架构瓶颈：随着用户数的增长，并发读写数据库成为瓶颈。 Tips：欢迎关注微信公众号：Java后端，获取更多技术博文推送。 3.3 第二次演进：引入本地缓存和分布式缓存 581/format/webp) image 在Tomcat同服务器上或同JVM中增加本地缓存，并在外部增加分布式缓存，缓存热门商品信息或热门商品的html页面等。通过缓存能把绝大多数请求在读写数据库前拦截掉，大大降低数据库压力。 其中涉及的技术包括：使用memcached作为本地缓存，使用Redis作为分布式缓存，还会涉及缓存一致性、缓存穿透/击穿、缓存雪崩、热点数据集中失效等问题。 架构瓶颈：缓存抗住了大部分的访问请求，随着用户数的增长，并发压力主要落在单机的Tomcat上，响应逐渐变慢。 3.4 第三次演进：引入反向代理实现负载均衡 ormat/webp) image 在多台服务器上分别部署Tomcat，使用反向代理软件（Nginx）把请求均匀分发到每个Tomcat中。 此处假设Tomcat最多支持100个并发，Nginx最多支持50000个并发，那么理论上Nginx把请求分发到500个Tomcat上，就能抗住50000个并发。 其中涉及的技术包括：Nginx、HAProxy，两者都是工作在网络第七层的反向代理软件，主要支持http协议，还会涉及session共享、文件上传下载的问题。 架构瓶颈：反向代理使应用服务器可支持的并发量大大增加，但并发量的增长也意味着更多请求穿透到数据库，单机的数据库最终成为瓶颈。 3.5 第四次演进：数据库读写分离 image 把数据库划分为读库和写库，读库可以有多个，通过同步机制把写库的数据同步到读库，对于需要查询最新写入数据场景，可通过在缓存中多写一份，通过缓存获得最新数据。 其中涉及的技术包括：Mycat，它是数据库中间件，可通过它来组织数据库的分离读写和分库分表，客户端通过它来访问下层数据库，还会涉及数据同步，数据一致性的问题。 架构瓶颈：业务逐渐变多，不同业务之间的访问量差距较大，不同业务直接竞争数据库，相互影响性能。 3.6 第五次演进：数据库按业务分库 image 把不同业务的数据保存到不同的数据库中，使业务之间的资源竞争降低，对于访问量大的业务，可以部署更多的服务器来支撑。 这样同时导致跨业务的表无法直接做关联分析，需要通过其他途径来解决，但这不是本文讨论的重点，有兴趣的可以自行搜索解决方案。 架构瓶颈：随着用户数的增长，单机的写库会逐渐会达到性能瓶颈。 3.7 第六次演进：把大表拆分为小表 image 比如针对评论数据，可按照商品ID进行hash，路由到对应的表中存储； 针对支付记录，可按照小时创建表，每个小时表继续拆分为小表，使用用户ID或记录编号来路由数据。 只要实时操作的表数据量足够小，请求能够足够均匀的分发到多台服务器上的小表，那数据库就能通过水平扩展的方式来提高性能。其中前面提到的Mycat也支持在大表拆分为小表情况下的访问控制。 这种做法显著的增加了数据库运维的难度，对DBA的要求较高。数据库设计到这种结构时，已经可以称为分布式数据库 但这只是一个逻辑的数据库整体，数据库里不同的组成部分是由不同的组件单独来实现的 如分库分表的管理和请求分发，由Mycat实现，SQL的解析由单机的数据库实现，读写分离可能由网关和消息队列来实现，查询结果的汇总可能由数据库接口层来实现等等 这种架构其实是MPP（大规模并行处理）架构的一类实现。 目前开源和商用都已经有不少MPP数据库，开源中比较流行的有Greenplum、TiDB、Postgresql XC、HAWQ等，商用的如南大通用的GBase、睿帆科技的雪球DB、华为的LibrA等等 不同的MPP数据库的侧重点也不一样，如TiDB更侧重于分布式OLTP场景，Greenplum更侧重于分布式OLAP场景 这些MPP数据库基本都提供了类似Postgresql、Oracle、MySQL那样的SQL标准支持能力，能把一个查询解析为分布式的执行计划分发到每台机器上并行执行，最终由数据库本身汇总数据进行返回 也提供了诸如权限管理、分库分表、事务、数据副本等能力，并且大多能够支持100个节点以上的集群，大大降低了数据库运维的成本，并且使数据库也能够实现水平扩展。 架构瓶颈：数据库和Tomcat都能够水平扩展，可支撑的并发大幅提高，随着用户数的增长，最终单机的Nginx会成为瓶颈。 3.8 第七次演进：使用LVS或F5来使多个Nginx负载均衡 image 由于瓶颈在Nginx，因此无法通过两层的Nginx来实现多个Nginx的负载均衡。 图中的LVS和F5是工作在网络第四层的负载均衡解决方案，其中LVS是软件，运行在操作系统内核态，可对TCP请求或更高层级的网络协议进行转发，因此支持的协议更丰富，并且性能也远高于Nginx，可假设单机的LVS可支持几十万个并发的请求转发； F5是一种负载均衡硬件，与LVS提供的能力类似，性能比LVS更高，但价格昂贵。 由于LVS是单机版的软件，若LVS所在服务器宕机则会导致整个后端系统都无法访问，因此需要有备用节点。 可使用keepalived软件模拟出虚拟IP，然后把虚拟IP绑定到多台LVS服务器上，浏览器访问虚拟IP时，会被路由器重定向到真实的LVS服务器 当主LVS服务器宕机时，keepalived软件会自动更新路由器中的路由表，把虚拟IP重定向到另外一台正常的LVS服务器，从而达到LVS服务器高可用的效果。 此处需要注意的是，上图中从Nginx层到Tomcat层这样画并不代表全部Nginx都转发请求到全部的Tomcat 在实际使用时，可能会是几个Nginx下面接一部分的Tomcat，这些Nginx之间通过keepalived实现高可用，其他的Nginx接另外的Tomcat，这样可接入的Tomcat数量就能成倍的增加。 架构瓶颈：由于LVS也是单机的，随着并发数增长到几十万时，LVS服务器最终会达到瓶颈，此时用户数达到千万甚至上亿级别，用户分布在不同的地区，与服务器机房距离不同，导致了访问的延迟会明显不同。 3.9 第八次演进：通过DNS轮询实现机房间的负载均衡 image 在DNS服务器中可配置一个域名对应多个IP地址，每个IP地址对应到不同的机房里的虚拟IP。 当用户访问www.taobao.com时，DNS服务器会使用轮询策略或其他策略，来选择某个IP供用户访问。此方式能实现机房间的负载均衡 至此，系统可做到机房级别的水平扩展，千万级到亿级的并发量都可通过增加机房来解决，系统入口处的请求并发量不再是问题。 架构瓶颈：随着数据的丰富程度和业务的发展，检索、分析等需求越来越丰富，单单依靠数据库无法解决如此丰富的需求。 3.10 第九次演进：引入NoSQL数据库和搜索引擎等技术 image 当数据库中的数据多到一定规模时，数据库就不适用于复杂的查询了，往往只能满足普通查询的场景。 对于统计报表场景，在数据量大时不一定能跑出结果，而且在跑复杂查询时会导致其他查询变慢 对于全文检索、可变数据结构等场景，数据库天生不适用。因此需要针对特定的场景，引入合适的解决方案。 如对于海量文件存储，可通过分布式文件系统HDFS解决，对于key value类型的数据，可通过HBase和Redis等方案解决 对于全文检索场景，可通过搜索引擎如ElasticSearch解决，对于多维分析场景，可通过Kylin或Druid等方案解决。 当然，引入更多组件同时会提高系统的复杂度，不同的组件保存的数据需要同步，需要考虑一致性的问题，需要有更多的运维手段来管理这些组件等。 架构瓶颈：引入更多组件解决了丰富的需求，业务维度能够极大扩充，随之而来的是一个应用中包含了太多的业务代码，业务的升级迭代变得困难。 3.11 第十次演进：大应用拆分为小应用 image 按照业务板块来划分应用代码，使单个应用的职责更清晰，相互之间可以做到独立升级迭代。这时候应用之间可能会涉及到一些公共配置，可以通过分布式配置中心Zookeeper来解决。 架构瓶颈：不同应用之间存在共用的模块，由应用单独管理会导致相同代码存在多份，导致公共功能升级时全部应用代码都要跟着升级。 3.12 第十一次演进：复用的功能抽离成微服务 image 如用户管理、订单、支付、鉴权等功能在多个应用中都存在，那么可以把这些功能的代码单独抽取出来形成一个单独的服务来管理 这样的服务就是所谓的微服务，应用和服务之间通过HTTP、TCP或RPC请求等多种方式来访问公共服务，每个单独的服务都可以由单独的团队来管理。 此外，可以通过Dubbo、SpringCloud等框架实现服务治理、限流、熔断、降级等功能，提高服务的稳定性和可用性。 架构瓶颈：不同服务的接口访问方式不同，应用代码需要适配多种访问方式才能使用服务，此外，应用访问服务，服务之间也可能相互访问，调用链将会变得非常复杂，逻辑变得混乱。 3.13 第十二次演进：引入企业服务总线ESB屏蔽服务接口的访问差异 image 通过ESB统一进行访问协议转换，应用统一通过ESB来访问后端服务，服务与服务之间也通过ESB来相互调用，以此降低系统的耦合程度。 这种单个应用拆分为多个应用，公共服务单独抽取出来来管理，并使用企业消息总线来解除服务之间耦合问题的架构，就是所谓的SOA（面向服务）架构，这种架构与微服务架构容易混淆，因为表现形式十分相似。 个人理解，微服务架构更多是指把系统里的公共服务抽取出来单独运维管理的思想，而SOA架构则是指一种拆分服务并使服务接口访问变得统一的架构思想，SOA架构中包含了微服务的思想。 架构瓶颈：业务不断发展，应用和服务都会不断变多，应用和服务的部署变得复杂，同一台服务器上部署多个服务还要解决运行环境冲突的问题 此外，对于如大促这类需要动态扩缩容的场景，需要水平扩展服务的性能，就需要在新增的服务上准备运行环境，部署服务等，运维将变得十分困难。 3.14 第十三次演进：引入容器化技术实现运行环境隔离与动态服务管理 image 目前最流行的容器化技术是Docker，最流行的容器管理服务是Kubernetes(K8S)，应用/服务可以打包为Docker镜像，通过K8S来动态分发和部署镜像。 Docker镜像可理解为一个能运行你的应用/服务的最小的操作系统，里面放着应用/服务的运行代码，运行环境根据实际的需要设置好。 把整个“操作系统”打包为一个镜像后，就可以分发到需要部署相关服务的机器上，直接启动Docker镜像就可以把服务起起来，使服务的部署和运维变得简单。 在大促的之前，可以在现有的机器集群上划分出服务器来启动Docker镜像，增强服务的性能 大促过后就可以关闭镜像，对机器上的其他服务不造成影响（在第18节之前，服务运行在新增机器上需要修改系统配置来适配服务，这会导致机器上其他服务需要的运行环境被破坏）。 架构瓶颈：使用容器化技术后服务动态扩缩容问题得以解决，但是机器还是需要公司自身来管理，在非大促的时候，还是需要闲置着大量的机器资源来应对大促，机器自身成本和运维成本都极高，资源利用率低。 3.15 第十四次演进：以云平台承载系统 image 系统可部署到公有云上，利用公有云的海量机器资源，解决动态硬件资源的问题 在大促的时间段里，在云平台中临时申请更多的资源，结合Docker和K8S来快速部署服务，在大促结束后释放资源，真正做到按需付费，资源利用率大大提高，同时大大降低了运维成本。 所谓的云平台，就是把海量机器资源，通过统一的资源管理，抽象为一个资源整体 在云平台上可按需动态申请硬件资源（如CPU、内存、网络等），并且之上提供通用的操作系统，提供常用的技术组件（如Hadoop技术栈，MPP数据库等）供用户使用，甚至提供开发好的应用 用户不需要关心应用内部使用了什么技术，就能够解决需求（如音视频转码服务、邮件服务、个人博客等）。 在云平台中会涉及如下几个概念： IaaS：基础设施即服务。对应于上面所说的机器资源统一为资源整体，可动态申请硬件资源的层面； PaaS：平台即服务。对应于上面所说的提供常用的技术组件方便系统的开发和维护； SaaS：软件即服务。对应于上面所说的提供开发好的应用或服务，按功能或性能要求付费。 至此：以上所提到的从高并发访问问题，到服务的架构和系统实施的层面都有了各自的解决方案。 但同时也应该意识到，在上面的介绍中，其实是有意忽略了诸如跨机房数据同步、分布式事务实现等等的实际问题，这些问题以后有机会再拿出来单独讨论。 4、架构设计总结 1）架构的调整是否必须按照上述演变路径进行？\\ 不是的，以上所说的架构演变顺序只是针对某个侧面进行单独的改进 在实际场景中，可能同一时间会有几个问题需要解决，或者可能先达到瓶颈的是另外的方面，这时候就应该按照实际问题实际解决。 如在政府类的并发量可能不大，但业务可能很丰富的场景，高并发就不是重点解决的问题，此时优先需要的可能会是丰富需求的解决方案。 2）对于将要实施的系统，架构应该设计到什么程度？\\ 对于单次实施并且性能指标明确的系统，架构设计到能够支持系统的性能指标要求就足够了，但要留有扩展架构的接口以便不备之需。 对于不断发展的系统，如电商平台，应设计到能满足下一阶段用户量和性能指标要求的程度，并根据业务的增长不断的迭代升级架构，以支持更高的并发和更丰富的业务。 3）服务端架构和大数据架构有什么区别？\\ 所谓的“大数据”其实是海量数据采集清洗转换、数据存储、数据分析、数据服务等场景解决方案的一个统称，在每一个场景都包含了多种可选的技术 如数据采集有Flume、Sqoop、Kettle等，数据存储有分布式文件系统HDFS、FastDFS，NoSQL数据库HBase、MongoDB等，数据分析有Spark技术栈、机器学习算法等。 总的来说大数据架构就是根据业务的需求，整合各种大数据组件组合而成的架构，一般会提供分布式存储、分布式计算、多维分析、数据仓库、机器学习算法等能力。 而服务端架构更多指的是应用组织层面的架构，底层能力往往是由大数据架构来提供。 4）有没有一些架构设计的原则？\\ N+1设计：系统中的每个组件都应做到没有单点故障； 回滚设计：确保系统可以向前兼容，在系统升级时应能有办法回滚版本； 禁用设计：应该提供控制具体功能是否可用的配置，在系统出现故障时能够快速下线功能； 监控设计：在设计阶段就要考虑监控的手段； 多活数据中心设计：若系统需要极高的高可用，应考虑在多地实施数据中心进行多活，至少在一个机房断电的情况下系统依然可用； 采用成熟的技术：刚开发的或开源的技术往往存在很多隐藏的bug，出了问题没有商业支持可能会是一个灾难； 资源隔离设计：应避免单一业务占用全部资源； 架构应能水平扩展：系统只有做到能水平扩展，才能有效避免瓶颈问题； 非核心则购买：非核心功能若需要占用大量的研发资源才能解决，则考虑购买成熟的产品； 使用商用硬件：商用硬件能有效降低硬件故障的机率； 快速迭代：系统应该快速开发小功能模块，尽快上线进行验证，早日发现问题大大降低系统交付的风险； 无状态设计：服务接口应该做成无状态的，当前接口的访问不依赖于接口上次访问的状态。 "},"架构/0.微服务架构与soa架构.html":{"url":"架构/0.微服务架构与soa架构.html","title":"2.微服务架构与soa架构","keywords":"","body":"一、微服务二、SOA三、分布式四、集群五、补充：一、微服务 1、什么是微服务 微服务是一种架构风格，一个大型复杂软件应用由一个或多个微服务组成。系统中的各个微服务可被独立部署，各个微服务之间是松耦合的。每个微服务仅关注于完成一件任务并很好地完成该任务。在所有情况下，每个任务代表着一个小的业务能力。 微服务架构 = 80%的SOA服务架构思想 + 100%的组件化架构思想 + 80%的领域建模思想 2、通俗理解 由于业务间的逻辑越来越复杂，我们就不把这些业务全部杂糅在一起，每个业务都分开来做，这就是微服务，而微服务就是一种特殊的分布式。 3、优缺点 优点：上面的单体系统全部运行于一个进程之内，资源相互影响，添加功能可能会影响其它功能，导致维护麻烦。 而微服务一切分为不同的模块，运行于自身进程内，而且不同的服务可以使用不同的语言充分发挥优势。 缺点：引入了分布式的复杂性，如接口一致性。 不过很多问题强大的Spring Cloud都已经提供了解决方案！ 图1.1： 二、SOA 1、什么是SOA SOA（Service-Oriented Architecture），中文全称：面向服务的架构。 SOA是一种粗粒度、松耦合服务架构，服务之间通过简单、精确定义接口进行通讯，不涉及底层编程接口和通讯模型。SOA可以看作是B/S模型、XML（标准通用标记语言的子集）/Web Service技术之后的自然延伸。 2、通俗理解 SOA把系统分离成不同的服务，使用接口来进行数据交互，最终达到整合系统的目的。 3、优缺点 优点：敏捷性、一致性、业务流程的改进、灵活性、数据统一、运行监控、利用操作平台。 缺点：组织结构的改变、组织权力结构的改变、业务面临的新挑战、IT变得简单之前会越来越复杂、没有数据视图、监控复杂性、技术不匹配。 4、SOA的三大特征 1）独立的功能实体 2）大数据量低频访问 3）基于文本的消息传递 三、分布式 注：集群是个物理形态，分布式是个工作方式。 1、什么是分布式 分布式就是把一个大而复杂的业务计算分配到多个业务节点机器上，即多个节点机器构成一个完整的业务链。 一个业务分拆多个子业务，部署在不同的服务器上。 一个系统分为很多个子系统，这些子系统相互配合完成整个的业务逻辑叫做分布式，分布式中每一个节点都可以配置集群.。 (而集群指的是将几台服务器集中在一起，实现同一业务。) 集群: ​ 将一个大的系统划分为多个业务模块，业务模块分别部署到不同的机器上，各个业务模块之间通过接口进行数据交互。区别分布式的方式是根据不同机器不同业务。 　　上面：service A、B、C、D 分别是业务组件，通过API Geteway进行业务访问。 　　注：分布式需要做好事务管理。 例如： 2、单体应用和分布式的比较 四、集群 注：集群是个物理形态，分布式是个工作方式。 1、什么是集群 集群就是具有完整功能的系统群集，一个集群节点坏了，其它节点能照常运行，这是不是像一个并联电路？一个并联电路节点坏了，其它节点还是能够正常工作的。 集群的功能作用就是为了增加整个系统的负载和吞吐量，所以一般我们把集群和负载均衡拿到一起来讨论和研究，当然，集群的另一个显著功能就是做系统的容错，为了保证系统稳健、长久的运行下去，一个集群节点的故障不会导致整个系统完蛋，对于客户而言是透明的，只要能正常访问，他们都不知道您系统中或许某几个系统节点已经罢工了，这样就涉及到云计算中的弹性分配，我们能根据系统的高峰、低谷期动态增加或者减少集群节点。 2、通俗理解 同一个业务，部署在多个服务器上。（即：多台服务器部署相同应用构成一个集群） 3、集群的两大特性、能力、技术 特性：可扩展性、高可用性 能力：负载均衡、错误恢复 技术：集群地址、内部通信 注：内部通信是集群能正常运转的基础,它使得集群具有均衡负载和错误恢复的能力。 五、补充： 1）分布式和集群的区别： 集群：同一个业务，部署在多个服务器上 分布式：一个业务分拆多个子业务，部署在不同的服务器上 2）微服务和SOA的区别： SOA（Service Oriented Architecture）“面向服务的架构”:他是一种设计方法，其中包含多个服务， 服务之间通过相互依赖最终提供一系列的功能。一个服务 通常以独立的形式存在与操作系统进程中。各个服务之间 通过网络调用。 微服务架构:其实和 SOA 架构类似,微服务是在 SOA 上做的升华，微服务架构强调的一个重点是“业务需要彻底的组件化和服务化”，原有的单个业务系统会拆分为多个可以独立开发、设计、运行的小应用。这些小应用之间通过服务完成交互和集成。 3）微服务和分布式的区别及联系 1.分布式属于微服务 2.区别及联系 ​ 分布式：分散压力。 　　微服务：分散能力。 　　分布式： 　　不同模块部署在不同服务器上; 　　作用：分布式解决网站高并发带来问题; 　　集群：相同的服务; 　　多台服务器部署相同应用构成一个集群; 　　作用：通过负载均衡设备共同对外提供服务; 　　SOA［组装服务/ESB企业服务总线］： 　　业务系统分解为多个组件，让每个组件都独立提供离散，自治，可复用的服务能力; 　　通过服务的组合和编排来实现上层的业务流程; 　　作用：简化维护，降低整体风险，伸缩灵活; 　　微服务［找到服务/微服务网关open API］; 　　架构设计概念，各服务间隔离（分布式也是隔离），自治（分布式依赖整体组合）其它特性（单一职责，边界，异步通信，独立部署）是分布式概念的跟严格执行; 　　SOA到微服务架构的演进过程; 　　作用：各服务可独立应用，组合服务也可系统应用（巨石应用［monolith］的简化实现策略-平台思想）. 4）ESB和微服务API网关 1.ESB（企业服务总线），简单 来说 ESB 就是一根管道，用来连接各个服务节点。为了集 成不同系统，不同协议的服务，ESB 做了消息的转化解释和路由工作，让不同的服务互联互通； 2.API网关:API网关是一个服务器，是系统的唯一入口。从面向对象设计的角度看，它与外观模式类似。API网关封装了系统内部架构，为每个客户端提供一个定制的API。它可能还具有其它职责，如身份验证、监控、负载均衡、缓存、请求分片与管理、静态响应处理。API网关方式的核心要点是，所有的客户端和消费端都通过统一的网关接入微服务，在网关层处理所有的非业务功能。通常，网关也是提供REST/HTTP的访问API。服务端通过API-GW注册和管理服务。 "},"高可用架构/1.限流.html":{"url":"高可用架构/1.限流.html","title":"1.系统限流","keywords":"","body":"什么是限流限流方法计数器滑动窗口漏桶算法令牌桶算法工作中的使用spring cloud gatewaysentinel总结什么是限流 限流可以认为服务降级的一种，限流就是限制系统的输入和输出流量已达到保护系统的目的。一般来说系统的吞吐量是可以被测算的，为了保证系统的稳定运行，一旦达到的需要限制的阈值，就需要限制流量并采取一些措施以完成限制流量的目的。比如：延迟处理，拒绝处理，或者部分拒绝处理等等。 限流方法 计数器 实现方式 控制单位时间内的请求数量 import java.util.concurrent.atomic.AtomicInteger; public class Counter { /** * 最大访问数量 */ private final int limit = 10; /** * 访问时间差 */ private final long timeout = 1000; /** * 请求时间 */ private long time; /** * 当前计数器 */ private AtomicInteger reqCount = new AtomicInteger(0); public boolean limit() { long now = System.currentTimeMillis(); if (now 劣势 假设在 00:01 时发生一个请求,在 00:01-00:58 之间不在发送请求,在 00:59 时发送剩下的所有请求 n-1 (n 为限流请求数量),在下一分钟的 00:01 发送 n 个请求,这样在 2 秒钟内请求到达了 2n - 1 个. 设每分钟请求数量为 60 个,每秒可以处理 1 个请求,用户在 00:59 发送 60 个请求,在 01:00 发送 60 个请求 此时 2 秒钟有 120 个请求(每秒 60 个请求),远远大于了每秒钟处理数量的阈值 滑动窗口 实现方式 滑动窗口算法是将时间周期分为N个小周期，分别记录每个小周期内访问次数，并且根据时间滑动删除过期的小周期。 如下图，假设时间周期为1min，将1min再分为2个小周期，统计每个小周期的访问数量，则可以看到，第一个时间周期内，访问数量为75，第二个时间周期内，访问数量为100，超过100的访问则被限流掉了 由此可见，当滑动窗口的格子划分的越多，那么滑动窗口的滚动就越平滑，限流的统计就会越精确。 此算法可以很好的解决固定窗口算法的临界问题。 滑动窗口是对计数器方式的改进, 增加一个时间粒度的度量单位 把一分钟分成若干等分(6 份,每份 10 秒), 在每一份上设置独立计数器,在 00:00-00:09 之间发生请求计数器累加 1.当等分数量越大限流统计就越详细 package com.example.demo1.service; import java.util.Iterator; import java.util.Random; import java.util.concurrent.ConcurrentLinkedQueue; import java.util.stream.IntStream; public class TimeWindow { private ConcurrentLinkedQueue queue = new ConcurrentLinkedQueue(); /** * 间隔秒数 */ private int seconds; /** * 最大限流 */ private int max; public TimeWindow(int max, int seconds) { this.seconds = seconds; this.max = max; /** * 永续线程执行清理queue 任务 */ new Thread(() -> { while (true) { try { // 等待 间隔秒数-1 执行清理操作 Thread.sleep((seconds - 1) * 1000L); } catch (InterruptedException e) { e.printStackTrace(); } clean(); } }).start(); } public static void main(String[] args) throws Exception { final TimeWindow timeWindow = new TimeWindow(10, 1); // 测试3个线程 IntStream.range(0, 3).forEach((i) -> { new Thread(() -> { while (true) { try { Thread.sleep(new Random().nextInt(20) * 100); } catch (InterruptedException e) { e.printStackTrace(); } timeWindow.take(); } }).start(); }); } /** * 获取令牌，并且添加时间 */ public void take() { long start = System.currentTimeMillis(); try { int size = sizeOfValid(); if (size > max) { System.err.println(\"超限\"); } synchronized (queue) { if (sizeOfValid() > max) { System.err.println(\"超限\"); System.err.println(\"queue中有 \" + queue.size() + \" 最大数量 \" + max); } this.queue.offer(System.currentTimeMillis()); } System.out.println(\"queue中有 \" + queue.size() + \" 最大数量 \" + max); } } public int sizeOfValid() { Iterator it = queue.iterator(); Long ms = System.currentTimeMillis() - seconds * 1000; int count = 0; while (it.hasNext()) { long t = it.next(); if (t > ms) { // 在当前的统计时间范围内 count++; } } return count; } /** * 清理过期的时间 */ public void clean() { Long c = System.currentTimeMillis() - seconds * 1000; Long tl = null; while ((tl = queue.peek()) != null && tl 漏桶算法 漏桶算法简介 漏桶算法，又称leaky bucket。为了理解漏桶算法，我们看一下对于该算法的示意图： 从图中我们可以看到，整个算法其实十分简单。首先，我们有一个固定容量的桶，有水流进来，也有水流出去。对于流进来的水来说，我们无法预计一共有多少水会流进来，也无法预计水流的速度。但是对于流出去的水来说，这个桶可以固定水流出的速率。而且，当桶满了之后，多余的水将会溢出。 我们将算法中的水换成实际应用中的请求，我们可以看到漏桶算法天生就限制了请求的速度。当使用了漏桶算法，我们可以保证接口会以一个常速速率来处理请求。所以漏桶算法天生不会出现临界问题。 漏桶算法可以粗略的认为就是注水漏水过程，往桶中以一定速率流出水，以任意速率流入水，当水超过桶流量则丢弃，因为桶容量是不变的，保证了整体的速率。 二、漏桶算法小栗子 来看个小栗子： public class LeakyBucket { public long timeStamp = System.currentTimeMillis(); // 当前时间 public long capacity; // 桶的容量 public long rate; // 水漏出的速度 public long water; // 当前水量(当前累积请求数) public boolean grant() { long now = System.currentTimeMillis(); // 先执行漏水，计算剩余水量 water = Math.max(0, water - (now - timeStamp) * rate); timeStamp = now; if ((water + 1) 说明： （1）未满加水：通过代码 water +=1进行不停加水的动作。 （2）漏水：通过时间差来计算漏水量。 （3）剩余水量：总水量-漏水量。 令牌桶算法 实现方式 规定固定容量的桶, token 以固定速度往桶内填充, 当桶满时 token 不会被继续放入, 每过来一个请求把 token 从桶中移除, 如果桶中没有 token 不能请求 public class TokenBucket { /** * 时间 */ private long time; /** * 总量 */ private Double total; /** * token 放入速度 */ private Double rate; /** * 当前总量 */ private Double nowSize; public boolean limit() { long now = System.currentTimeMillis(); nowSize = Math.min(total, nowSize + (now - time) * rate); time = now; if (nowSize 工作中的使用 spring cloud gateway spring cloud gateway 默认使用 redis 进行限流, 笔者一般只是修改修改参数属于拿来即用. 并没有去从头实现上述那些算法. org.springframework.cloud spring-cloud-starter-gateway org.springframework.boot spring-boot-starter-data-redis-reactive spring: cloud: gateway: routes: - id: requestratelimiter_route uri: lb://pigx-upms order: 10000 predicates: - Path=/admin/** filters: - name: RequestRateLimiter args: redis-rate-limiter.replenishRate: 1 # 令牌桶的容积 redis-rate-limiter.burstCapacity: 3 # 流速 每秒 key-resolver: \"#{@remoteAddrKeyResolver}\" #SPEL表达式去的对应的bean - StripPrefix=1 @Bean KeyResolver remoteAddrKeyResolver() { return exchange -> Mono.just(exchange.getRequest().getRemoteAddress().getHostName()); } sentinel 通过配置来控制每个 url 的流量 com.alibaba.cloud spring-cloud-starter-alibaba-sentinel spring: cloud: nacos: discovery: server-addr: localhost:8848 sentinel: transport: dashboard: localhost:8080 port: 8720 datasource: ds: nacos: server-addr: localhost:8848 dataId: spring-cloud-sentinel-nacos groupId: DEFAULT_GROUP rule-type: flow namespace: xxxxxxxx 配置内容在 nacos 上进行编辑 [ { \"resource\": \"/hello\", \"limitApp\": \"default\", \"grade\": 1, \"count\": 1, \"strategy\": 0, \"controlBehavior\": 0, \"clusterMode\": false } ] resource：资源名，即限流规则的作用对象。 limitApp：流控针对的调用来源，若为 default 则不区分调用来源。 grade：限流阈值类型，QPS 或线程数模式，0 代表根据并发数量来限流，1 代表根据 QPS 来进行流量控制。 count：限流阈值 strategy：判断的根据是资源自身，还是根据其它关联资源 (refResource)，还是根据链路入口 controlBehavior：流控效果（直接拒绝 / 排队等待 / 慢启动模式） clusterMode：是否为集群模式 总结 sentinel 和 spring cloud gateway 两个框架都是很好的限流框架, 但是在我使用中还没有将spring-cloud-alibaba接入到项目中进行使用, 所以我会选择spring cloud gateway, 当接入完整的或者接入 Nacos 项目使用 setinel 会有更加好的体验. "},"高可用系统/2.服务熔断.html":{"url":"高可用系统/2.服务熔断.html","title":"2.服务熔断","keywords":"","body":"一 微服务雪崩效应二 服务熔断一 微服务雪崩效应 在介绍熔断机制之前，我们需要了解微服务的雪崩效应。在微服务架构中，微服务是完成一个单一的业务功能，这样做的好处是可以做到解耦，每个微服务可以独立演进。但是，一个应用可能会有多个微服务组成，微服务之间的数据交互通过远程过程调用完成。这就带来一个问题，假设微服务A调用微服务B和微服务C，微服务B和微服务C又调用其它的微服务，这就是所谓的“扇出”。如果扇出的链路上某个微服务的调用响应时间过长或者不可用，对微服务A的调用就会占用越来越多的系统资源，进而引起系统崩溃，所谓的“雪崩效应”。 二 服务熔断 熔断机制是应对雪崩效应的一种微服务链路保护机制。我们在各种场景下都会接触到熔断这两个字。高压电路中，如果某个地方的电压过高，熔断器就会熔断，对电路进行保护。股票交易中，如果股票指数过高，也会采用熔断机制，暂停股票的交易。同样，在微服务架构中，熔断机制也是起着类似的作用。当扇出链路的某个微服务不可用或者响应时间太长时，会进行服务的降级，进而熔断该节点微服务的调用，快速返回错误的响应信息。当检测到该节点微服务调用响应正常后，恢复调用链路。 在Spring Cloud框架里，熔断机制通过Hystri或者sentinel实现。Hystrix会监控微服务间调用的状况，当失败的调用到一定阈值，缺省是5秒内20次调用失败，就会启动熔断机制。 具体的实践会在后续微服务篇中说明 熔段解决如下几个问题： 当所依赖的对象不稳定时，能够起到快速失败的目的 快速失败后，能够根据一定的算法动态试探所依赖对象是否恢复 "},"高可用系统/3.服务降级.html":{"url":"高可用系统/3.服务降级.html","title":"3.服务降级","keywords":"","body":"1.简介2.使用场景3.核心设计4.高级特性4.1 分级降级1.简介 什么是服务降级？当服务器压力剧增的情况下，根据实际业务情况及流量，对一些服务和页面有策略的不处理或换种简单的方式处理，从而释放服务器资源以保证核心交易正常运作或高效运作。 如果还是不理解，那么可以举个例子：假如目前有很多人想要给我付钱，但我的服务器除了正在运行支付的服务之外，还有一些其它的服务在运行，比如搜索、定时任务和详情等等。然而这些不重要的服务就占用了JVM的不少内存与CPU资源，为了能把钱都收下来（钱才是目标），我设计了一个动态开关，把这些不重要的服务直接在最外层拒掉，这样处理后的后端处理收钱的服务就有更多的资源来收钱了（收钱速度更快了），这就是一个简单的服务降级的使用场景。 2.使用场景 服务降级主要用于什么场景呢？当整个微服务架构整体的负载超出了预设的上限阈值或即将到来的流量预计将会超过预设的阈值时，为了保证重要或基本的服务能正常运行，我们可以将一些 不重要 或 不紧急 的服务或任务进行服务的 延迟使用 或 暂停使用。 3.核心设计 3.1 分布式开关 根据上述需求，我们可以设置一个分布式开关，用于实现服务的降级，然后集中式管理开关配置信息即可。具体方案如下： 服务降级-分布式开关 3.2 自动降级 超时降级 —— 主要配置好超时时间和超时重试次数和机制，并使用异步机制探测恢复情况 失败次数降级 —— 主要是一些不稳定的API，当失败调用次数达到一定阀值自动降级，同样要使用异步机制探测回复情况 故障降级 —— 如要调用的远程服务挂掉了（网络故障、DNS故障、HTTP服务返回错误的状态码和RPC服务抛出异常），则可以直接降级 限流降级 —— 当触发了限流超额时，可以使用暂时屏蔽的方式来进行短暂的屏蔽 当我们去秒杀或者抢购一些限购商品时，此时可能会因为访问量太大而导致系统崩溃，此时开发者会使用限流来进行限制访问量，当达到限流阀值，后续请求会被降级；降级后的处理方案可以是：排队页面（将用户导流到排队页面等一会重试）、无货（直接告知用户没货了）、错误页（如活动太火爆了，稍后重试）。 3.3 配置中心 微服务降级的配置信息是集中式的管理，然后通过可视化界面进行友好型的操作。配置中心和应用之间需要网络通信，因此可能会因网络闪断或网络重启等因素，导致配置推送信息丢失、重启或网络恢复后不能再接受、变更不及时等等情况，因此服务降级的配置中心需要实现以下几点特性，从而尽可能的保证配置变更即使达到： 服务降级-配置中心 启动主动拉取配置 —— 用于初始化配置（减少第一次定时拉取周期） 发布订阅配置 —— 用于实现配置及时变更（可以解决90%左右的配置变更） 定时拉取配置 —— 用于解决发布订阅失效或消失丢失的情况（可以解决9%左右的发布订阅失效的消息变更） 离线文件缓存配置 —— 用于临时解决重启后连接不上配置中心的问题 可编辑式配置文档 —— 用于直接编辑文档的方式来实现配置的定义 提供Telnet命令变更配置 —— 用于解决配置中心失效而不能变更配置的常见 3.4 处理策略 当触发服务降级后，新的交易再次到达时，我们该如何来处理这些请求呢？从微服务架构全局的视角来看，我们通常有以下是几种常用的降级处理方案： 页面降级 —— 可视化界面禁用点击按钮、调整静态页面 延迟服务 —— 如定时任务延迟处理、消息入MQ后延迟处理 写降级 —— 直接禁止相关写操作的服务请求 读降级 —— 直接禁止相关度的服务请求 缓存降级 —— 使用缓存方式来降级部分读频繁的服务接口 针对后端代码层面的降级处理策略，则我们通常使用以下几种处理措施进行降级处理： 抛异常 返回NULL 调用Mock数据 调用Fallback处理逻辑 4.高级特性 假设我们已经为每个服务都做好了一个降级开关，也已经在线上验证通过了，感觉完全没问题了。 场景一：某一天，运营搞了一次活动，突然跑过来说，现在流量已经快涨到上限了，有没有批量降级所有不重要服务的方式？开发一脸懵逼的看着，这又不是操作DB，哪里有批量操作呀。 场景二：某一天，运营又搞事了，说我们等下要搞一个活动，让我们赶紧提前把不重要的服务都降级了，开发又是一脸懵逼，我怎么知道要降级哪些服务呀。 反思：服务降级的功能虽然是实现了，可是没有考虑实施时的体验。服务太多，不知道该降级哪些服务，单个操作降级速度太慢…… 4.1 分级降级 当微服务架构发生不同程度的情况时，我们可以根据服务的对比而进行选择式舍弃（即丢车保帅的原则），从而进一步保障核心的服务的正常运作。 如果等线上服务即将发生故障时，才去逐个选择哪些服务该降级、哪些服务不能降级，然而线上有成百上千个服务，则肯定是来不及降级就会被拖垮。同时，在大促或秒杀等活动前才去梳理，也是会有不少的工作量，因此建议在开发期就需要架构师或核心开发人员来提前梳理好，是否能降级的初始评估值，即是否能降级的默认值。 为了便于批量操作微服务架构中服务的降级，我们可以从全局的角度来建立服务重要程度的评估模型，如果有条件的话，建议可以使用 层次分析法（The analytic hierarchy process，简称AHP） 的数学建模模型（或其它模型）来进行定性和定量的评估（肯定比架构师直接拍脑袋决定是否降级好很多倍，当然难度和复杂度也会高许多，即你需要一个会数学建模人才），而层次分析法的基本思路是人对一个复杂的决策问题的思维和判断过程大体上是一样的。 以下是个人给出的最终评价模型，可作为服务降级的评价参考模型进行设计： 我们利用数学建模的方式或架构师直接拍脑袋的方式，结合服务能否降级的优先原则，并根据台风预警（都属于风暴预警）的等级进行参考设计，可将微服务架构的所有服务进行故障风暴等级划分为以下四种： 评估模型： 蓝色风暴 —— 表示需要小规模降级非核心服务 黄色风暴 —— 表示需要中等规模降级非核心服务 橙色风暴 —— 表示需要大规模降级非核心服务 红色风暴 —— 表示必须降级所有非核心服务 设计说明： 故障严重程度为：蓝色＜黄色＜橙色＜红色 建议根据二八原则可以将服务划分为：80%的非核心服务+20%的核心服务 以上模型只是整体微服务架构的服务降级评估模型，具体大促或秒杀活动时，建议以具体主题为中心进行建立（不同主题的活动，因其依赖的服务不同，而使用不同的进行降级更为合理）。当然模型可以使用同一个，但其数据需要有所差异。最好能建立一套模型库，然后实施时只需要输入相关服务即可输出最终降级方案，即输出本次大促或秒杀时，当发生蓝色风暴时需要降级的服务清单、当发生黄色风暴时需要降级的服务清单…… "}}