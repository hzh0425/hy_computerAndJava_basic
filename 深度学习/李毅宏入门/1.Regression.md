## 一 应用

![image-20201221114719833](https://gitee.com/zisuu/picture/raw/master/img/20201221114720.png)

![image-20201221114746030](https://gitee.com/zisuu/picture/raw/master/img/20201221114746.png)

## 二 步骤

**1.定义model**

机器学习的本质是定义函数

x就成为feature(特征),找到函数,根据特征,即可输出结果

![image-20201221115007951](https://gitee.com/zisuu/picture/raw/master/img/20201221115008.png)

**2.收集trainingData**

![image-20201221115149299](https://gitee.com/zisuu/picture/raw/master/img/20201221115149.png)

![image-20201221115350244](https://gitee.com/zisuu/picture/raw/master/img/20201221115350.png)

**3.定义loss函数,得到最优解**

用于定义上述模型的好坏

![image-20201221115454518](https://gitee.com/zisuu/picture/raw/master/img/20201221115454.png)

![image-20201221115630371](https://gitee.com/zisuu/picture/raw/master/img/20201221115633.png)

蓝色区域的function可使loss最低,从而得到最优function

![image-20201221115729025](https://gitee.com/zisuu/picture/raw/master/img/20201221115729.png)

## 三 梯度下降

### 1.Gradient步骤

现在已定义模型y=wx+b和一个loss函数

我们可以利用梯度下降法求解参数w

也即将w对loss函数进行微分(dl/dw0,也即图中的切线位置):

![image-20201221120226107](https://gitee.com/zisuu/picture/raw/master/img/20201221120226.png)

我们的目的是为了得到能让Loss尽量低的w的值

所以,当切线过于陡峭时,可以适当移动w0以改变loss的值

![image-20201221120406184](https://gitee.com/zisuu/picture/raw/master/img/20201221120406.png)

那么问题是,移动的步伐大小是多少呢?这个大小就叫learningRate

那么更新后的w1:

![image-20201221120552311](https://gitee.com/zisuu/picture/raw/master/img/20201221120552.png)

![image-20201221120602680](https://gitee.com/zisuu/picture/raw/master/img/20201221120602.png)

接下去就是重复这个步骤,假设经过T次更新,达到Local Optimal,也即参数无法再更新

![image-20201221120720635](https://gitee.com/zisuu/picture/raw/master/img/20201221120720.png)



### 2.多参数

其实步骤和单参数一样

![image-20201221120911831](https://gitee.com/zisuu/picture/raw/master/img/20201221120911.png)



### 3.具体微分值

![image-20201221121332347](https://gitee.com/zisuu/picture/raw/master/img/20201221121332.png)

















