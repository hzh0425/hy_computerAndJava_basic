# 分布式系统设计理念

分布式系统架构的第一原则是不要分布！这句话看似矛盾实则揭露了分布式系统的很多特征。

> ## 分布式系统的目标与要素

分布式系统的目标是提升系统的整体性能和吞吐量另外还要尽量保证分布式系统的容错性（假如增加10台服务器才达到单机运行效果2倍左右的性能，那么这个分布式系统就根本没有存在的意义）。

即使采用了分布式系统，我们也要尽力运用并发编程、高性能网络框架等等手段提升单机上的程序性能。

> ## 分布式系统设计两大思路：中心化和去中心化

![分布式系统设计两大思路：中心化和去中心化](https://gitee.com/zisuu/picture/raw/master/img/20210108000422.jpeg)

### 1）中心化设计：

- **两个角色：** 中心化的设计思想很简单，分布式集群中的节点机器按照角色分工，大体上氛围两种角色： **“领导”** 和 **“干活的”**
- **角色职责：** “领导”通常负责分发任务并监督“干活的”，发现谁太闲了，就想发设法地给其安排新任务，确保没有一个“干活的”能够偷懒，如果“领导”发现某个“干活的”因为劳累过度而病倒了，则是不会考虑先尝试“医治”他的，而是一脚踢出去，然后把他的任务分给其他人。其中微服务架构 **Kubernetes** 就恰好采用了这一设计思路。
- 中心化设计的问题：
  1. 中心化的设计存在的最大问题是“领导”的安危问题，如果“领导”出了问题，则群龙无首，整个集群就奔溃了。但我们难以同时安排两个“领导”以避免单点问题。
  2. 中心化设计还存在另外一个潜在的问题，既“领导”的能力问题：可以领导10个人高效工作并不意味着可以领导100个人高效工作，所以如果系统设计和实现得不好，问题就会卡在“领导”身上。
- **领导安危问题的解决办法：** 大多数中心化系统都采用了主备两个“领导”的设计方案，可以是热备或者冷备，也可以是自动切换或者手动切换，而且越来越多的新系统都开始具备自动选举切换“领导”的能力，以提升系统的可用性。

### 2)   去中心化设计

- **终生地位平等：** 在去中心化的设计里，通常没有“领导”和“干活的”这两种角色的区分，大家的角色都是一样的，地位是平等的，全球互联网就是一个典型的去中心化的分布式系统，联网的任意节点设备宕机，都只会影响很小范围的功能。

- **“去中心化”不是不要中心，而是由节点来自由选择中心。** （集群的成员会自发的举行“会议”选举新的“领导”主持工作。最典型的案例就是ZooKeeper及Go语言实现的Etcd）

- **去中心化设计的问题：** 去中心化设计里最难解决的一个问题是 **“脑裂”问题** ，这种情况的发声概率很低，但影响很大。脑裂指一个集群由于网络的故障，被分为至少两个彼此无法通信的单独集群，此时如果两个集群都各自工作，则可能会产生眼中的数据冲突何错误。一般的设计思路是，当集群半段发声了脑裂问题是，规模较小的集群就“自杀”或者拒绝服务。

  > 在zookeeper集群中，有一个leader和多个follower（observer不参与选举，可以忽略），leader通过周期性向follower发送心跳的方式维持自己的存在感。当follower没有收到心跳超过一定时间后，就认为leader已经宕机，开始重新选举。但是这个时候，leader有可能没有宕机，而是假死，比如发生网络分区的情况。假设集群中总共有6个节点，发生网络分区后，变成以下情况（原本只有机房1的leader，分区后机房2也要选举leader）：
  >
  > ![img](https://gitee.com/zisuu/picture/raw/master/img/20210108002950.png)
  >
  > 如果没有过半机制，就会导致，机房2开始重新选举时，同意的节点数>=6/2=3，结果leader2成功选举。一个集群有两个leader会导致混乱，不同的请求分别到达两个leader进行处理，最终会导致两个机房节点不一致。zookeeper的解决方法就是过半机制，即每次选举leader时，同意选举的节点数需要多于集群一半节点数才能当选，等于一半都不行。使用过半机制后，即使像上面那样分成两个集群，机房2由于同意选举的结点数最多只有3个，等于总节点数一般，但是未超过一半，因此无法选出leader，也无法提供服务。机房1的leader通过心跳机制发现自己管理的节点数未过半，会自动退位，至此整个集群无leader，停止服务。但是避免了2个leader分别执行写操作导致不一致的情况。
  >
  > 在此之上，zookeeper还有奇数个节点的机制，也就是说使集群总节点数是奇数，这样的好处是分区后必然有一个机房的节点数是过半的，能够选出leader，因此集群仍然可以对外服务。zookeeper集群就有了这样的一个特性：集群中只要有过半的机器是正常工作的，那么整个集群对外就是可用的。

> ### 分布式与集群的区别是什么？

- **分布式：** 一个业务分拆多个子业务，部署在不同的服务器上
- **集群：** 同一个业务，部署在多个服务器上。比如之前做电商网站搭的redis集群以及solr集群都是属于将redis服务器提供的缓存服务以及solr服务器提供的搜索服务部署在多个服务器上以提高系统性能、并发量解决海量存储问题。

# CAP定理

![CAP定理](https://gitee.com/zisuu/picture/raw/master/img/20210108000431.jpeg)
在理论计算机科学中，CAP定理（CAP theorem），又被称作布鲁尔定理（Brewer’s theorem），它指出对于一个分布式计算系统来说，不可能同时满足以下三点：
\- **一致性（Consistence）** :所有节点访问同一份最新的数据副本
\- **可用性（Availability）**:每次请求都能获取到非错的响应——但是不保证获取的数据为最新数据
\- **分区容错性（Partition tolerance）** : 分布式系统在遇到某节点或网络分区故障的时候，仍然能够对外提供满足一致性和可用性的服务。

CAP仅适用于原子读写的NOSQL场景中，并不适合数据库系统。

**注意：不是所谓的3选2（不要被网上大多数文章误导了）:**

现实生活中，大部分人解释这一定律时，常常简单的表述为：“一致性、可用性、分区容忍性三者你只能同时达到其中两个，不可能同时达到”。实际上这是一个非常具有误导性质的说法，而且在CAP理论诞生12年之后，CAP之父也在2012年重写了之前的论文。

**当发生网络分区的时候，如果我们要继续服务，那么强一致性和可用性只能2选1。也就是说当网络分区之后P是前提，决定了P之后才有C和A的选择。也就是说分区容错性（Partition tolerance）我们是必须要实现的。**

我在网上找了很多文章想看一下有没有文章提到这个不是所谓的3选2，用百度半天没找到了一篇，用谷歌搜索找到一篇比较不错的，如果想深入学习一下CAP就看这篇文章把，我这里就不多BB了：**《分布式系统之CAP理论》 ：** http://www.cnblogs.com/hxsyl/p/4381980.html

# BASE理论

BASE理论由eBay架构师Dan Pritchett提出，在2008年上被分表为论文，并且eBay给出了他们在实践中总结的基于BASE理论的一套新的分布式事务解决方案。

**BASE** 是 **Basically Available（基本可用）** 、**Soft-state（软状态）** 和 **Eventually Consistent（最终一致性）** 三个短语的缩写。BASE理论是对CAP中一致性和可用性权衡的结果，其来源于对大规模互联网系统分布式实践的总结，是基于CAP定理逐步演化而来的，它大大降低了我们对系统的要求。

> ## BASE理论的核心思想

即使无法做到强一致性，但每个应用都可以根据自身业务特点，采用适当的方式来使系统达到最终一致性。也就是牺牲数据的一致性来满足系统的高可用性，系统中一部分数据不可用或者不一致时，仍需要保持系统整体“主要可用”。

针对数据库领域，BASE思想的主要实现是对业务数据进行拆分，让不同的数据分布在不同的机器上，以提升系统的可用性，当前主要有以下两种做法：

- 按功能划分数据库
- 分片（如开源的Mycat、Amoeba等）。

由于拆分后会涉及分布式事务问题，所以eBay在该BASE论文中提到了如何用最终一致性的思路来实现高性能的分布式事务。

> ## BASE理论三要素

![BASE理论三要素](https://gitee.com/zisuu/picture/raw/master/img/20210108003001.jpeg)

### 1. **基本可用**

基本可用是指分布式系统在出现不可预知故障的时候，允许损失部分可用性。但是，这绝不等价于系统不可用。

比如：

- **响应时间上的损失**:正常情况下，一个在线搜索引擎需要在0.5秒之内返回给用户相应的查询结果，但由于出现故障，查询结果的响应时间增加了1~2秒
- **系统功能上的损失**：正常情况下，在一个电子商务网站上进行购物的时候，消费者几乎能够顺利完成每一笔订单，但是在一些节日大促购物高峰的时候，由于消费者的购物行为激增，为了保护购物系统的稳定性，部分消费者可能会被引导到一个降级页面

### 2. **软状态**

软状态指允许系统中的数据存在中间状态，并认为该中间状态的存在不会影响系统的整体可用性，即允许系统在不同节点的数据副本之间进行数据同步的过程存在延时

### 3. **最终一致性**

最终一致性强调的是系统中所有的数据副本，在经过一段时间的同步后，最终能够达到一个一致的状态。因此，最终一致性的本质是需要系统保证最终数据能够达到一致，而不需要实时保证系统数据的强一致性。

# 总结

本文主要是简单的介绍了三个常见的概念： **分布式系统设计理念** 、 **CAP定理** 、 **BASE理论** ，关于分布式系统的还有很多很多东西。
![分布式系统的经典基础理论总结](https://gitee.com/zisuu/picture/raw/master/img/20210108003005.jpeg)



# HDFS原理

**一、什么是HDFS**

HDFS即Hadoop分布式文件系统（Hadoop Distributed Filesystem），以流式数据访问模式来存储超大文件，它和现有的分布式文件系统有很多共同点。但同时，它和其他的分布式文件系统的区别也是很明显的。HDFS是一个高度容错性的系统，适合部署在廉价的机器上。HDFS能提供高吞吐量的数据访问，非常适合大规模数据集上的应用。HDFS放宽了一部分POSIX约束，来实现流式读取文件系统数据的目的。HDFS在最开始是作为Apache Nutch搜索引擎项目的基础架构而开发的。

**二、HDFS架构**

 个HDFS集群包含一个单独的NameNode和多个DataNode。HDFS采用一种称为rack-aware的策略。Rack1 和Rack2

![img](https://gitee.com/zisuu/picture/raw/master/img/20210108003011.png)

备份数据的存放：备份数据的存放是HDFS可靠性和性能的关键。HDFS采用一种称为rack-aware的策略来决定备份数据的存放。通过一个称为Rack Awareness的过程，NameNode决定每个DataNode所属rack id。缺省情况下，一个block块会有三个备份，一个在NameNode指定的DataNode上，一个在指定DataNode非同一rack的DataNode上，一个在指定DataNode同一rack的DataNode上。这种策略综合考虑了同一rack失效、以及不同rack之间数据复制性能问题。

副本的选择：为了降低整体的带宽消耗和读取延时，HDFS会尽量读取最近的副本。如果在同一个rack上有一个副本，那么就读该副本。如果一个HDFS集群跨越多个数据中心，那么将首先尝试读本地数据中心的副本。安全模式：系统启动后先进入安全模式，此时系统中的内容不允许修改和删除，直到安全模式结束。安全模式主要是为了启动检查各个DataNode上数据块的安全性

 

**三、HDFS核心组件详解**

*********************************************************NameNode**************************************************

![img](https://gitee.com/zisuu/picture/raw/master/img/20210108003016.png)

1、NameNode是HDFS的核心模块，也是HDFS架构的master。NomeNode一点宕机则整个HDFS服务不可用。
2、NameNode仅仅存储HDFS的元数据：文件系统中的文件目录结构，并且能跟踪整个集群中的文件。
3、NameNode不存储实际的文件数据，实际数据是存储在DataNode中，他存储的是文件分块的基础数据；能通过文件获取文件的快列表及其分布在哪些dataNode上。
4、NameNode并不会将文件的分块数据持久化存储，这些信息会在HDFS启动时由各个dataNode上报过来。他把这些数据存入内存中。并且会定时对内存中的数据进行快照。所以对于NameNode节点的机器内存应该大一些。

5、NameNode在hadoop 2.0版本之前是单点的，Hadoop 2.0版本才提出了高可用 (High Availability, HA) 解决方案，并且经过多个版本的迭代更新，已经广泛应用于生产环境。

6、[NameNode也可以将数据持久化](https://blog.csdn.net/lvtula/article/details/82354989) 

 

*********************************************************DataNode**************************************************

1、DataNode：HDFS的Slave节点，存储文件实际的数据，负责将数据落入磁盘。所以DataNode节点需要较大的磁盘。
2、DataNode在启动时会将自己发布到NameNode上，并上报自己持有的数据块表。定期向NameNode发送心跳，如果NameNode长时间没有接受到DataNode发送的心跳，NameNode就会认为该DataNode以及失效，将其剔除集群。心跳参数dfs.heartbeat.interval=3（默认3秒发送一次心跳）
3、当某个DateNode宕机后，不会影响数据和集群的可用性。NameNode会安排其他DataNode进行副本复制接管他的工作。
4、DataNode会定时上报自己负责的数据块列表。

*********************************************************Secondary NameNode**************************************************

SecondaryNameNode有两个作用，一是镜像备份，二是日志与镜像的定期合并。两个过程同时进行，称为checkpoint. 镜像备份的作用:备份fsimage(fsimage是元数据发送检查点时写入文件);日志与镜像的定期合并的作用:将Namenode中edits日志和fsimage合并,防止如果Namenode节点故障，namenode下次启动的时候，会把fsimage加载到内存中，应用edit log,edit log往往很大，导致操作往往很耗时。

**四、HDFS操作流程**

**************************************************文件上传流程******************************************

1、客户端发起写文件请求 hadoop fs -put
2、nameNode检查上传文件的命名空间是否存在（就是检测文件目录结构是否存在），创建者是否有权限进行操作。然后返回状态高数客户端你可以上传数据
3、客户端按照分块配置大小将文件分块。然后请求NameNode我要上传blk1，副本数是三个，这个文件一共分割了5块。
4、NameNode检测自己管理下的DateNodes是否满足要求，然后返回给客户端三台DateNode节点信息（存储策略是机架模式）。
5、Client端根据返回的DataNode信息选择一个离自己最近的一个DataNode节点，创建pipeLine（数据传输管道），DataNode1->DataNode2创建pipeLine,DataNode2->DataNode3创建pipeLine;DataNode3通过这一串管道传递给client数据传输管道已经建立完毕。
6、client端创建Stream流（以packet为单位传输数据 64kb）上传数据。
7、DataNode1接受并保持源源不断的packet，然后把packet源源不断的传递给DataNode2，DataNode2也做相应的操作。
8、DataNode也通过pipeLine发送ACK认证数据是否接收完毕。
9、第一个数据块上传完毕后client端开始上传第二个数据块

 *********************************************************文件的获取流程****************************************

1、client 发起 hadoop fs -get请求
2、NomeNode检查该文件的信息，文件的分块信息和每个分块所对应哪个DateNode，，以及备份信息和备份信息所在哪个DataNode。把这些信息返回给client端。（返回原则也是机架原则，根据网络拓扑将距离最近的DataNode排在前边返回）
3、根据NameNode的信息，请求各个文件块对应的DataNode节点获取文件数据。
4、合并下载过来的数据块，形成一个完整的文件。



# spark原理

## spark运行架构

spark基础运行架构如下所示：

 

spark结合yarn集群背后的运行流程如下所示：

 

spark 运行流程：

Spark架构采用了分布式计算中的Master-Slave模型。Master是对应集群中的含有Master进程的节点，Slave是集群中含有Worker进程的节点。

- Master作为整个集群的控制器，负责整个集群的正常运行；
- Worker相当于计算节点，接收主节点命令与进行状态汇报；
- Executor负责任务的执行；
- Client作为用户的客户端负责提交应用；
- Driver负责控制一个应用的执行。

Spark集群部署后，需要在主节点和从节点分别启动Master进程和Worker进程，对整个集群进行控制。在一个Spark应用的执行过程中，Driver和Worker是两个重要角色。Driver 程序是应用逻辑执行的起点，负责作业的调度，即Task任务的分发，而多个Worker用来管理计算节点和创建Executor并行处理任务。在执行阶段，Driver会将Task和Task所依赖的file和jar序列化后传递给对应的Worker机器，同时Executor对相应数据分区的任务进行处理。

1. Excecutor /Task 每个程序自有，不同程序互相隔离，task多线程并行
2. 集群对Spark透明，Spark只要能获取相关节点和进程
3. Driver 与Executor保持通信，协作处理

三种集群模式：

1.Standalone 独立集群

2.Mesos, apache mesos

3.Yarn, hadoop yarn

基本概念：

- Application =>Spark的应用程序，包含一个Driver program和若干Executor
- SparkContext => Spark应用程序的入口，负责调度各个运算资源，协调各个Worker Node上的Executor
- Driver Program => 运行Application的main()函数并且创建SparkContext
- Executor => 是为Application运行在Worker node上的一个进程，该进程负责运行Task，并且负责将数据存在内存或者磁盘上。每个Application都会申请各自的Executor来处理任务
- Cluster Manager =>在集群上获取资源的外部服务 (例如：Standalone、Mesos、Yarn)
- Worker Node => 集群中任何可以运行Application代码的节点，运行一个或多个Executor进程
- Task => 运行在Executor上的工作单元
- Job => SparkContext提交的具体Action操作，常和Action对应
- Stage => 每个Job会被拆分很多组task，每组任务被称为Stage，也称TaskSet
- RDD => 是Resilient distributed datasets的简称，中文为弹性分布式数据集;是Spark最核心的模块和类
- DAGScheduler => 根据Job构建基于Stage的DAG，并提交Stage给TaskScheduler
- TaskScheduler => 将Taskset提交给Worker node集群运行并返回结果
- Transformations => 是Spark API的一种类型，Transformation返回值还是一个RDD，所有的Transformation采用的都是懒策略，如果只是将Transformation提交是不会执行计算的
- Action => 是Spark API的一种类型，Action返回值不是一个RDD，而是一个scala集合；计算只有在Action被提交的时候计算才被触发。

## Spark核心概念之RDD

 

## Spark核心概念之Transformations / Actions

 

Transformation返回值还是一个RDD。它使用了链式调用的设计模式，对一个RDD进行计算后，变换成另外一个RDD，然后这个RDD又可以进行另外一次转换。这个过程是分布式的。 Action返回值不是一个RDD。它要么是一个Scala的普通集合，要么是一个值，要么是空，最终或返回到Driver程序，或把RDD写入到文件系统中。

Action是返回值返回给driver或者存储到文件，是RDD到result的变换，Transformation是RDD到RDD的变换。

只有action执行时，rdd才会被计算生成，这是rdd懒惰执行的根本所在。

## Spark核心概念之Jobs / Stage

Job => 包含多个task的并行计算，一个action触发一个job

stage => 一个job会被拆为多组task，每组任务称为一个stage，以shuffle进行划分

 

## Spark核心概念之Shuffle

以reduceByKey为例解释shuffle过程。

 

在没有task的文件分片合并下的shuffle过程如下：（`spark.shuffle.consolidateFiles=false`）

 

fetch 来的数据存放到哪里？

刚 fetch 来的 FileSegment 存放在 softBuffer 缓冲区，经过处理后的数据放在内存 + 磁盘上。这里我们主要讨论处理后的数据，可以灵活设置这些数据是“只用内存”还是“内存＋磁盘”。如果spark.shuffle.spill = false就只用内存。由于不要求数据有序，shuffle write 的任务很简单：将数据 partition 好，并持久化。之所以要持久化，一方面是要减少内存存储空间压力，另一方面也是为了 fault-tolerance。

shuffle之所以需要把中间结果放到磁盘文件中，是因为虽然上一批task结束了，下一批task还需要使用内存。如果全部放在内存中，内存会不够。另外一方面为了容错，防止任务挂掉。

存在问题如下：

1. 产生的 FileSegment 过多。每个 ShuffleMapTask 产生 R（reducer 个数）个 FileSegment，M 个 ShuffleMapTask 就会产生 M * R 个文件。一般 Spark job 的 M 和 R 都很大，因此磁盘上会存在大量的数据文件。
2. 缓冲区占用内存空间大。每个 ShuffleMapTask 需要开 R 个 bucket，M 个 ShuffleMapTask 就会产生 MR 个 bucket。虽然一个 ShuffleMapTask 结束后，对应的缓冲区可以被回收，但一个 worker node 上同时存在的 bucket 个数可以达到 cores R 个（一般 worker 同时可以运行 cores 个 ShuffleMapTask），占用的内存空间也就达到了cores× R × 32 KB。对于 8 核 1000 个 reducer 来说，占用内存就是 256MB。

为了解决上述问题，我们可以使用文件合并的功能。

在进行task的文件分片合并下的shuffle过程如下：（`spark.shuffle.consolidateFiles=true`）

 

可以明显看出，在一个 core 上连续执行的 ShuffleMapTasks 可以共用一个输出文件 ShuffleFile。先执行完的 ShuffleMapTask 形成 ShuffleBlock i，后执行的 ShuffleMapTask 可以将输出数据直接追加到 ShuffleBlock i 后面，形成 ShuffleBlock i'，每个 ShuffleBlock 被称为 FileSegment。下一个 stage 的 reducer 只需要 fetch 整个 ShuffleFile 就行了。这样，每个 worker 持有的文件数降为 cores× R。FileConsolidation 功能可以通过`spark.shuffle.consolidateFiles=true`来开启。

## Spark核心概念之Cache

```
val rdd1 = ... // 读取hdfs数据，加载成RDD



rdd1.cache



 



val rdd2 = rdd1.map(...)



val rdd3 = rdd1.filter(...)



 



rdd2.take(10).foreach(println)



rdd3.take(10).foreach(println)



 



rdd1.unpersist
```

cache和unpersisit两个操作比较特殊，他们既不是action也不是transformation。cache会将标记需要缓存的rdd，真正缓存是在第一次被相关action调用后才缓存；unpersisit是抹掉该标记，并且立刻释放内存。只有action执行时，rdd1才会开始创建并进行后续的rdd变换计算。

cache其实也是调用的persist持久化函数，只是选择的持久化级别为`MEMORY_ONLY`。

persist支持的RDD持久化级别如下：

 

需要注意的问题：

Cache或shuffle场景序列化时， spark序列化不支持protobuf message，需要java 可以serializable的对象。一旦在序列化用到不支持java serializable的对象就会出现上述错误。

Spark只要写磁盘，就会用到序列化。除了shuffle阶段和persist会序列化，其他时候RDD处理都在内存中，不会用到序列化。

 

 

[Spark Streaming实时计算框架介绍](https://www.cnblogs.com/Leo_wl/p/3530464.html)



阅读目录

- [Spark Streaming实时计算框架介绍](https://www.cnblogs.com/Leo_wl/p/3530464.html#_label0)

[回到目录](https://www.cnblogs.com/Leo_wl/p/3530464.html#_labelTop)

# [Spark Streaming实时计算框架介绍](http://www.cnblogs.com/shenh062326/p/3530092.html)

 

随着大数据的发展，人们对大数据的处理要求也越来越高，原有的批处理框架MapReduce适合离线计算，却无法满足实时性要求较高的业务，如实时推荐、用户行为分析等。 Spark Streaming是建立在Spark上的实时计算框架，通过它提供的丰富的API、基于内存的高速执行引擎，用户可以结合流式、批处理和交互试查询应用。本文将详细介绍Spark Streaming实时计算框架的原理与特点、适用场景。

 

## Spark Streaming实时计算框架

 

  Spark是一个类似于MapReduce的分布式计算框架，其核心是弹性分布式数据集，提供了比MapReduce更丰富的模型，可以在快速在内存中对数据集进行多次迭代，以支持复杂的数据挖掘算法和图形计算算法。Spark Streaming是一种构建在Spark上的实时计算框架，它扩展了Spark处理大规模流式数据的能力。

Spark Streaming的优势在于：

- 能运行在100+的结点上，并达到秒级延迟。
- 使用基于内存的Spark作为执行引擎，具有高效和容错的特性。
- 能集成Spark的批处理和交互查询。
- 为实现复杂的算法提供和批处理类似的简单接口。

基于云梯Spark on Yarn的Spark Streaming总体架构如图1所示。其中Spark on Yarn的启动流程我的另外一篇文章（《程序员》2013年11月期刊《深入剖析阿里巴巴云梯Yarn集群》）有详细描述，这里不再赘述。Spark on Yarn启动后，由Spark AppMaster把Receiver作为一个Task提交给某一个Spark Executor；Receive启动后输入数据，生成数据块，然后通知Spark AppMaster；Spark AppMaster会根据数据块生成相应的Job，并把Job的Task提交给空闲Spark Executor 执行。图中蓝色的粗箭头显示被处理的数据流，输入数据流可以是磁盘、网络和HDFS等，输出可以是HDFS，数据库等。

 

图1 云梯Spark Streaming总体架构

Spark Streaming的基本原理是将输入数据流以时间片（秒级）为单位进行拆分，然后以类似批处理的方式处理每个时间片数据，其基本原理如图2所示。

 

图2 Spark Streaming基本原理图

首先，Spark Streaming把实时输入数据流以时间片Δt （如1秒）为单位切分成块。Spark Streaming会把每块数据作为一个RDD，并使用RDD操作处理每一小块数据。每个块都会生成一个Spark Job处理，最终结果也返回多块。

下面介绍Spark Streaming内部实现原理。

使用Spark Streaming编写的程序与编写Spark程序非常相似，在Spark程序中，主要通过操作RDD（Resilient Distributed Datasets弹性分布式数据集）提供的接口，如map、reduce、filter等，实现数据的批处理。而在Spark Streaming中，则通过操作DStream（表示数据流的RDD序列）提供的接口，这些接口和RDD提供的接口类似。图3和图4展示了由Spark Streaming程序到Spark jobs的转换图。

 

图3 Spark Streaming程序转换为DStream Graph

 

图4 DStream Graph转换为Spark jobs

在图3中，Spark Streaming把程序中对DStream的操作转换为DStream Graph，图4中，对于每个时间片，DStream Graph都会产生一个RDD Graph；针对每个输出操作（如print、foreach等），Spark Streaming都会创建一个Spark action；对于每个Spark action，Spark Streaming都会产生一个相应的Spark job，并交给JobManager。JobManager中维护着一个Jobs队列, Spark job存储在这个队列中，JobManager把Spark job提交给Spark Scheduler，Spark Scheduler负责调度Task到相应的Spark Executor上执行。

Spark Streaming的另一大优势在于其容错性，RDD会记住创建自己的操作，每一批输入数据都会在内存中备份，如果由于某个结点故障导致该结点上的数据丢失，这时可以通过备份的数据在其它结点上重算得到最终的结果。

正如Spark Streaming最初的目标一样，它通过丰富的API和基于内存的高速计算引擎让用户可以结合流式处理，批处理和交互查询等应用。因此Spark Streaming适合一些需要历史数据和实时数据结合分析的应用场合。当然，对于实时性要求不是特别高的应用也能完全胜任。另外通过RDD的数据重用机制可以得到更高效的容错处理。









# 问题汇总

**1.想做哪方面的研究?**

分布式计算,分布式存储

因为和我平常的学习内容有关,我之前学习大数据的时候,有了解郭相关内容

比如分布式计算,spark,以批调度的形式进行计算,并引入了流的概念

当然流的概念最好的实践应该是flink

分布式存储hdfs,

这两个都是中心化的分布式架构设计,都是master-slave架构,都有双节点以及备用主节点

我就很想去学习这种分布式的底层原理,但是奈何一直没有引导我得人,所以这也是我为什么联系您的原因

**2.未来规划**

冲刺保研,我之前成绩不好,但是我得专业课成绩很不错

我这学期的高分课程数据结构和离散等,我都有拿满级的信心

下学期开始,基本都是专业课,那就是我得优势,比如408的科目.所以我想要先冲刺保研

若无法保研,就考研

我想留在本校,当然可能还得shi情况而定,首选留本校

科研方向的化,就是大数据,分布式,数据挖掘相关的.我对于人工智能不怎么感兴趣

**3.项目经验**

我之前有跟着网课老师做个几个项目

比如基于实时流式计算的推荐系统

基于hive的数据仓库

还有几个web项目

我这次参加服创.准备报地铁大数据实时分析预测系统

**4.疑问**

1.我能跟着老师学到什么?

2.是实践的,还是理论的?我想要理论+实践,我有很好的自学能力

**5.对分布式了解多少?**

分布式系统设计原理

CAP

BASE

大数据几个框架

springcloud 微服务

